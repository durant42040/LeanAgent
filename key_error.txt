(raylet) [2024-08-10 13:23:56,233 E 4011078 4011108] (raylet) file_system_monitor.cc:111: /tmp/ray/sessio
n_2024-08-10_13-22-35_117947_4010085 is over 95% full, available space: 93580058624; capacity: 1887507697
664. Object creation will fail if spilling is required.
 47%|██████████████████████████████▊                                   | 1350/2890 [01:40<24:47,  1.04it/
s](raylet) [2024-08-10 13:24:06,238 E 4011078 4011108] (raylet) file_system_monitor.cc:111: /tmp/ray/sess
ion_2024-08-10_13-22-35_117947_4010085 is over 95% full, available space: 93580042240; capacity: 18875076
97664. Object creation will fail if spilling is required.
 53%|███████████████████████████████████▎                              | 1544/2890 [02:04<00:23, 56.59it/
s](raylet) [2024-08-10 13:24:26,247 E 4011078 4011108] (raylet) file_system_monitor.cc:111: /tmp/ray/sess
ion_2024-08-10_13-22-35_117947_4010085 is over 95% full, available space: 93580013568; capacity: 18875076
97664. Object creation will fail if spilling is required. [repeated 2x across cluster] (Ray deduplicates
logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/
ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)
 60%|███████████████████████████████████████▎                          | 1720/2890 [02:35<33:46,  1.73s/i
t](raylet) [2024-08-10 13:24:56,261 E 4011078 4011108] (raylet) file_system_monitor.cc:111: /tmp/ray/sess
ion_2024-08-10_13-22-35_117947_4010085 is over 95% full, available space: 93578973184; capacity: 18875076
97664. Object creation will fail if spilling is required. [repeated 3x across cluster]
 71%|██████████████████████████████████████████████▊                   | 2052/2890 [03:12<00:30, 27.75it/
s](raylet) [2024-08-10 13:25:26,276 E 4011078 4011108] (raylet) file_system_monitor.cc:111: /tmp/ray/sess
ion_2024-08-10_13-22-35_117947_4010085 is over 95% full, available space: 93578919936; capacity: 18875076
97664. Object creation will fail if spilling is required. [repeated 3x across cluster]
 80%|████████████████████████████████████████████████████▋             | 2308/2890 [03:58<32:25,  3.34s/i
t](raylet) [2024-08-10 13:26:06,294 E 4011078 4011108] (raylet) file_system_monitor.cc:111: /tmp/ray/sess
ion_2024-08-10_13-22-35_117947_4010085 is over 95% full, available space: 93577297920; capacity: 18875076
97664. Object creation will fail if spilling is required. [repeated 4x across cluster]
 85%|████████████████████████████████████████████████████████▏         | 2462/2890 [04:05<00:16, 26.06it/
s](raylet) [2024-08-10 13:26:46,315 E 4011078 4011108] (raylet) file_system_monitor.cc:111: /tmp/ray/sess
ion_2024-08-10_13-22-35_117947_4010085 is over 95% full, available space: 93576396800; capacity: 18875076
97664. Object creation will fail if spilling is required. [repeated 4x across cluster]
(raylet) [2024-08-10 13:26:56,319 E 4011078 4011108] (raylet) file_system_monitor.cc:111: /tmp/ray/sessio
n_2024-08-10_13-22-35_117947_4010085 is over 95% full, available space: 93576392704; capacity: 1887507697
664. Object creation will fail if spilling is required.
 85%|████████████████████████████████████████████████████████▏         | 2462/2890 [04:18<00:16, 26.06it/
s](raylet) [2024-08-10 13:27:06,324 E 4011078 4011108] (raylet) file_system_monitor.cc:111: /tmp/ray/sess
ion_2024-08-10_13-22-35_117947_4010085 is over 95% full, available space: 93576368128; capacity: 18875076
97664. Object creation will fail if spilling is required.
(raylet) [2024-08-10 13:27:16,330 E 4011078 4011108] (raylet) file_system_monitor.cc:111: /tmp/ray/sessio
n_2024-08-10_13-22-35_117947_4010085 is over 95% full, available space: 93576359936; capacity: 1887507697
664. Object creation will fail if spilling is required.
(raylet) [2024-08-10 13:27:26,336 E 4011078 4011108] (raylet) file_system_monitor.cc:111: /tmp/ray/sessio
n_2024-08-10_13-22-35_117947_4010085 is over 95% full, available space: 93576347648; capacity: 1887507697
664. Object creation will fail if spilling is required.
 85%|████████████████████████████████████████████████████████▎         | 2467/2890 [04:51<00:50,  8.45it/
s]
(raylet) [2024-08-10 13:27:36,207 E 4011078 4011078] (raylet) node_manager.cc:3064: 1 Workers (tasks / ac
tors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: ad96fa9e8e
36081c0268204da1a38bb6687bf9652c00b0dc9c3a9352, IP: 131.215.143.185) over the last time period. To see mo
re information about the Workers killed on this node, use `ray logs raylet.out -ip 131.215.143.185`
(raylet)
(raylet) Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/lat
est/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reduci
ng task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment v
ariable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment va
riable `RAY_memory_monitor_refresh_ms` to zero.
2024-08-10 13:27:42.313 | INFO     | generate_benchmark_lean4:main:346 - Failed to trace repo LeanGitRepo
(url='https://github.com/teorth/pfr', commit='6a5082ee465f9e44cea479c7b741b3163162bb7e') because of Task
was killed due to the node running low on memory.
Memory on the node (IP: 131.215.143.185, ID: ad96fa9e8e36081c0268204da1a38bb6687bf9652c00b0dc9c3a9352) wh
ere the task (actor ID: 3079fe725b49869d69639ad201000000, name=_TracedRepoHelper.__init__, pid=4018530, m
emory used=0.13GB) was running was 478.57GB / 503.73GB (0.950057), which exceeds the memory usage thresho
ld of 0.95. Ray killed this worker (ID: 42310a93c5adab4c99af10077426e401042b9349c61a723bf807f9d8) because
 it was the most recently scheduled task; to see more information about memory usage on this node, use `r
ay logs raylet.out -ip 131.215.143.185`. To see the logs of the worker, use `ray logs worker-42310a93c5ad
ab4c99af10077426e401042b9349c61a723bf807f9d8*out -ip 131.215.143.185. Top 10 memory users:
PID     MEM(GB) COMMAND
1312425 28.10   python /home/adarsh/ReProver/main.py
4010085 12.14   python /home/adarsh/ReProver/compute_server.py
3412529 6.34    /home/adarsh/.elan/toolchains/leanprover--lean4---v4.7.0/bin/lean --server /home/adarsh/P
utnamBench
1457158 4.30    python /home/adarsh/ReProver/compute_server.py
1192859 3.83    /home/adarsh/.vscode-server/cli/servers/Stable-89de5a8d4d6205e5b11647eb6a74844ca23d2573/s
erver/node ...
33227   2.97    /home/adarsh/.elan/toolchains/leanprover--lean4---v4.8.0-rc1/bin/lean --worker file:///ho
me/adarsh/L...
424107  2.75    /home/adarsh/.elan/toolchains/leanprover--lean4---v4.8.0-rc1/bin/lean --worker file:///ho
me/adarsh/L...
1346581 2.26    /home/adarsh/.elan/toolchains/leanprover--lean4---v4.8.0-rc1/bin/lean --worker file:///ho
me/adarsh/L...
521587  2.07    /home/adarsh/.elan/toolchains/leanprover--lean4---v4.8.0-rc1/bin/lean --worker file:///ho
me/adarsh/L...
1435800 1.58    python /home/adarsh/ReProver/compute_server.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-c
ore/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task p
arallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when t
he task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_
threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor
_refresh_ms` to zero.
2024-08-10 13:27:42.314 | INFO     | __main__:generate_dataset:246 - Finished generating benchmark at /ra
id/adarsh/datasets_test/pfr_6a5082ee465f9e44cea479c7b741b3163162bb7e
2024-08-10 13:27:42.314 | INFO     | __main__:generate_dataset:247 - Merging datasets
2024-08-10 13:27:42.314 | INFO     | __main__:merge_datasets:74 - Merging datasets for random
2024-08-10 13:27:42.314 | INFO     | __main__:merge_datasets:80 - Processing train split
2024-08-10 13:27:42.345 | INFO     | __main__:merge_datasets:104 - Finished processing train split
2024-08-10 13:27:42.345 | INFO     | __main__:merge_datasets:80 - Processing val split
2024-08-10 13:27:42.345 | INFO     | __main__:merge_datasets:104 - Finished processing val split
2024-08-10 13:27:42.345 | INFO     | __main__:merge_datasets:80 - Processing test split
2024-08-10 13:27:42.346 | INFO     | __main__:merge_datasets:104 - Finished processing test split
2024-08-10 13:27:42.346 | INFO     | __main__:merge_datasets:105 - Finished merging datasets for random
2024-08-10 13:27:42.346 | INFO     | __main__:merge_datasets:74 - Merging datasets for novel_premises
2024-08-10 13:27:42.346 | INFO     | __main__:merge_datasets:80 - Processing train split
2024-08-10 13:27:42.383 | INFO     | __main__:merge_datasets:104 - Finished processing train split
2024-08-10 13:27:42.383 | INFO     | __main__:merge_datasets:80 - Processing val split
2024-08-10 13:27:42.384 | INFO     | __main__:merge_datasets:104 - Finished processing val split
2024-08-10 13:27:42.384 | INFO     | __main__:merge_datasets:80 - Processing test split
2024-08-10 13:27:42.386 | INFO     | __main__:merge_datasets:104 - Finished processing test split
2024-08-10 13:27:42.386 | INFO     | __main__:merge_datasets:105 - Finished merging datasets for novel_pr
emises
2024-08-10 13:27:42.386 | INFO     | __main__:merge_datasets:107 - Merging corpus
2024-08-10 13:27:42.392 | INFO     | __main__:merge_datasets:130 - Finished merging corpus
2024-08-10 13:27:42.392 | INFO     | __main__:merge_datasets:132 - Adding metadata
2024-08-10 13:27:42.392 | WARNING  | __main__:merge_datasets:152 - No metadata file found
2024-08-10 13:27:42.392 | INFO     | __main__:merge_datasets:154 - Deleting individual datasets
2024-08-10 13:27:42.392 | INFO     | __main__:generate_dataset:249 - Finished merging datasets
2024-08-10 13:27:42.495 | INFO     | __main__:main:635 - Latest PL checkpoint found: AK123321/pl-leancopi
lot-2
2024-08-10 13:27:42.587 | INFO     | __main__:download_pl_checkpoint:274 - Checkpoint downloaded to: /hom
e/adarsh/.cache/huggingface/hub/models--AK123321--pl-leancopilot-2/snapshots/d4955ed1972ad84c03dce9189162
d03912f2d86f/model.ckpt
2024-08-10 13:27:42.587 | INFO     | __main__:main:638 - Checkpoint path: /home/adarsh/.cache/huggingface
/hub/models--AK123321--pl-leancopilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86f/model.ckpt
2024-08-10 13:27:42.587 | INFO     | __main__:train:464 - Training model with checkpoint: /home/adarsh/.c
ache/huggingface/hub/models--AK123321--pl-leancopilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86
f/model.ckpt
Seed set to 3407
Lightning automatically upgraded your loaded checkpoint from v0.0.0 to v2.4.0. To apply the upgrade to yo
ur files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../.cache/huggingface
/hub/models--AK123321--pl-leancopilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86f/model.ckpt`
2024-08-10 13:28:16.404 | INFO     | __main__:train:486 - Loaded premise retriever at /home/adarsh/.cache
/huggingface/hub/models--AK123321--pl-leancopilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86f/mo
del.ckpt
2024-08-10 13:28:16.405 | INFO     | __main__:train:490 - Data path: /raid/adarsh/datasets_test/merged/ra
ndom
/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/transformers/tokenization
_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True`
by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by def
ault. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
2024-08-10 13:28:16.508 | INFO     | common:__init__:199 - Building the corpus from /raid/adarsh/datasets
_test/merged/corpus.jsonl
2024-08-10 13:28:18.060 | INFO     | retrieval.datamodule:load_or_cache_data:52 - Loaded data from cache
/raid/adarsh/datasets_test/merged/random/cache_train/cached_data.pkl
Training dataset size: 353052
2024-08-10 13:28:18.093 | INFO     | retrieval.datamodule:load_or_cache_data:52 - Loaded data from cache
/raid/adarsh/datasets_test/merged/random/cache_val/cached_data.pkl
Validation dataset size: 5403
2024-08-10 13:28:18.127 | INFO     | retrieval.datamodule:load_or_cache_data:52 - Loaded data from cache
/raid/adarsh/datasets_test/merged/random/cache_pred/cached_data.pkl
Testing dataset size: 5321
2024-08-10 13:28:18.128 | INFO     | __main__:train:523 - Training dataset size after load: 353052
2024-08-10 13:28:18.128 | INFO     | __main__:train:524 - Validation dataset size after load: 5403
2024-08-10 13:28:18.128 | INFO     | __main__:train:525 - Testing dataset size after load: 5321
Using bfloat16 Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
2024-08-10 13:28:18.142 | INFO     | __main__:train:543 - Starting progressive training...
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 1 processes
----------------------------------------------------------------------------------------------------

2024-08-10 13:28:31.388 | INFO     | retrieval.datamodule:load_or_cache_data:52 - Loaded data from cache
/raid/adarsh/datasets_test/merged/random/cache_train/cached_data.pkl
Training dataset size: 353052
2024-08-10 13:28:31.622 | INFO     | retrieval.datamodule:load_or_cache_data:52 - Loaded data from cache
/raid/adarsh/datasets_test/merged/random/cache_val/cached_data.pkl
Validation dataset size: 5403
2024-08-10 13:28:31.662 | INFO     | retrieval.datamodule:load_or_cache_data:52 - Loaded data from cache
/raid/adarsh/datasets_test/merged/random/cache_pred/cached_data.pkl
Testing dataset size: 5321
/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning/callbac
ks/model_checkpoint.py:654: Checkpoint directory /raid/adarsh/checkpoints_test exists and is not empty.
Restoring states from the checkpoint path at /home/adarsh/.cache/huggingface/hub/models--AK123321--pl-lea
ncopilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86f/model.ckpt
Lightning automatically upgraded your loaded checkpoint from v0.0.0 to v2.4.0. To apply the upgrade to yo
ur files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../.cache/huggingface
/hub/models--AK123321--pl-leancopilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86f/model.ckpt`
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
2024-08-10 13:28:35.998 | INFO     | common:get_optimizers:432 - Optimizing with AdamW

  | Name    | Type           | Params | Mode
---------------------------------------------------
0 | encoder | T5EncoderModel | 217 M  | train
---------------------------------------------------
217 M     Trainable params
0         Non-trainable params
217 M     Total params
870.630   Total estimated model params size (MB)
235       Modules in train mode
0         Modules in eval mode
Restored all states from the checkpoint at /home/adarsh/.cache/huggingface/hub/models--AK123321--pl-leanc
opilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86f/model.ckpt
Epoch 0:   0%|                                                                  | 0/176526 [00:00<?, ?it/
s]2024-08-10 13:28:38.691 | INFO     | __main__:main:647 - An error occurred: Caught KeyError in DataLoad
er worker process 0.
Original Traceback (most recent call last):
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/torch/utils/data/
_utils/worker.py", line 309, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/torch/utils/data/
_utils/fetch.py", line 52, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/torch/utils/data/
_utils/fetch.py", line 52, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/adarsh/ReProver/retrieval/datamodule.py", line 123, in __getitem__
    for p in self.corpus.get_premises(ex["context"].path):
  File "/home/adarsh/ReProver/common.py", line 251, in get_premises
    return self._get_file(path).premises
  File "/home/adarsh/ReProver/common.py", line 226, in _get_file
    return self.transitive_dep_graph.nodes[path]["file"]
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/networkx/classes/
reportviews.py", line 194, in __getitem__
    return self._nodes[n]
KeyError: '.lake/packages/mathlib/Mathlib/Data/Ordmap/Ordset.lean'

Traceback (most recent call last):
  File "/home/adarsh/ReProver/compute_server.py", line 641, in main
    train(model_checkpoint_path, merged_data_path, next_suffix)
  File "/home/adarsh/ReProver/compute_server.py", line 545, in train
    trainer.fit(model, datamodule=data_module, ckpt_path=model_checkpoint_path)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/trainer/call.py", line 46, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/trainer/trainer.py", line 1025, in _run_stage
    self.fit_loop.run()
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/loops/training_epoch_loop.py", line 140, in run
    self.advance(data_fetcher)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/loops/training_epoch_loop.py", line 212, in advance
    batch, _, __ = next(data_fetcher)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/utilities/combined_loader.py", line 78, in __next__
    out[i] = next(self.iterators[i])
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/torch/utils/data/
dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/torch/utils/data/
dataloader.py", line 1344, in _next_data
    return self._process_data(data)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/torch/utils/data/
dataloader.py", line 1370, in _process_data
    data.reraise()
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/torch/_utils.py",
 line 706, in reraise
    raise exception
KeyError: Caught KeyError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/torch/utils/data/
_utils/worker.py", line 309, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/torch/utils/data/
_utils/fetch.py", line 52, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/torch/utils/data/
_utils/fetch.py", line 52, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/adarsh/ReProver/retrieval/datamodule.py", line 123, in __getitem__
    for p in self.corpus.get_premises(ex["context"].path):
  File "/home/adarsh/ReProver/common.py", line 251, in get_premises
    return self._get_file(path).premises
  File "/home/adarsh/ReProver/common.py", line 226, in _get_file
    return self.transitive_dep_graph.nodes[path]["file"]
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/networkx/classes/
reportviews.py", line 194, in __getitem__
    return self._nodes[n]
KeyError: '.lake/packages/mathlib/Mathlib/Data/Ordmap/Ordset.lean'

^C^Z
[5]+  Stopped                 bash run_compute_server.sh
(base) adarsh@tensorlab-DGX-Station-A100-920-23487-2531-000:~/ReProver$ bash run_compute_server.sh
Script executed from: /home/adarsh/ReProver
[2024-08-10 13:33:55,423] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda
 (auto detect)
 [WARNING]  async_io requires the dev libaio .so object and headers but these were not found.
 [WARNING]  async_io: please install the libaio-dev package with apt
 [WARNING]  If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS envi
ronment variables to where it can be found.
 [WARNING]  Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 [WARNING]  sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
 [WARNING]  using untested triton version (3.0.0), only 1.0.0 is known to be compatible
/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/deepspeed/runtime/zero/li
near.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.cust
om_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/deepspeed/runtime/zero/li
near.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.cust
om_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
2024-08-10 13:34:09.093 | INFO     | __main__:check_progress_file:586 - Checking contents of /home/adarsh
/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning/loops/progress.py
2024-08-10 13:34:09.093 | INFO     | __main__:check_progress_file:590 - Contents of progress.py:
2024-08-10 13:34:09.093 | INFO     | __main__:check_progress_file:591 - # Copyright The Lightning AI team
.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import asdict, dataclass, field
from typing import Type

from typing_extensions import override


@dataclass
class _BaseProgress:
    """Mixin that implements state-loading utilities for dataclasses."""

    def state_dict(self) -> dict:
        return asdict(self)

    def load_state_dict(self, state_dict: dict) -> None:
        if state_dict["completed"] == None:
            state_dict["completed"] = 0
        self.__dict__.update(state_dict)

    @classmethod
    def from_state_dict(cls, state_dict: dict) -> "_BaseProgress":
        obj = cls()
        obj.load_state_dict(state_dict)
        return obj

    def reset(self) -> None:
        """Reset the object's state."""
        raise NotImplementedError


@dataclass
class _ReadyCompletedTracker(_BaseProgress):
    """Track an event's progress.

    Args:
        ready: Intended to track the number of events ready to start.
        completed: Intended to be incremented after the event completes (e.g. after ``on_*_end`` runs).

    These attributes should be increased in order, that is, :attr:`ready` first and :attr:`completed` las
t.

    """

    ready: int = 0
    completed: int = 0

    @override
    def reset(self) -> None:
        """Reset the state."""
        self.ready = 0
        self.completed = 0

    def reset_on_restart(self) -> None:
        """Reset the progress on restart.

        If there is a failure before all attributes are increased, restore the attributes to the last ful
ly completed
        value.

        """
        self.ready = self.completed


@dataclass
class _StartedTracker(_ReadyCompletedTracker):
    """Track an event's progress.

    Args:
        ready: Intended to track the number of events ready to start.
        started: Intended to be incremented after the event is started (e.g. after ``on_*_start`` runs).
        completed: Intended to be incremented after the event completes (e.g. after ``on_*_end`` runs).

    These attributes should be increased in order, that is, :attr:`ready` first and :attr:`completed` las
t.

    """

    started: int = 0

    @override
    def reset(self) -> None:
        super().reset()
        self.started = 0

    @override
    def reset_on_restart(self) -> None:
        super().reset_on_restart()
        self.started = self.completed


@dataclass
class _ProcessedTracker(_StartedTracker):
    """Track an event's progress.

    Args:
        ready: Intended to track the number of events ready to start.
        started: Intended to be incremented after the event is started (e.g. after ``on_*_start`` runs).
        processed: Intended to be incremented after the event is processed.
        completed: Intended to be incremented after the event completes (e.g. after ``on_*_end`` runs).

    These attributes should be increased in order, that is, :attr:`ready` first and :attr:`completed` las
t.

    """

    processed: int = 0

    @override
    def reset(self) -> None:
        super().reset()
        self.processed = 0

    @override
    def reset_on_restart(self) -> None:
        super().reset_on_restart()
        self.processed = self.completed


@dataclass
class _Progress(_BaseProgress):
    """Track aggregated and current progress.

    Args:
        total: Intended to track the total progress of an event.
        current: Intended to track the current progress of an event.

    """

    total: _ReadyCompletedTracker = field(default_factory=_ProcessedTracker)
    current: _ReadyCompletedTracker = field(default_factory=_ProcessedTracker)

    def __post_init__(self) -> None:
        if self.total.__class__ is not self.current.__class__:
            raise ValueError("The `total` and `current` instances should be of the same class")

    def increment_ready(self) -> None:
        self.total.ready += 1
        self.current.ready += 1

    def increment_started(self) -> None:
        if not isinstance(self.total, _StartedTracker):
            raise TypeError(f"`{self.total.__class__.__name__}` doesn't have a `started` attribute")
        self.total.started += 1
        self.current.started += 1

    def increment_processed(self) -> None:
        if not isinstance(self.total, _ProcessedTracker):
            raise TypeError(f"`{self.total.__class__.__name__}` doesn't have a `processed` attribute")
        self.total.processed += 1
        self.current.processed += 1

    def increment_completed(self) -> None:
        self.total.completed += 1
        self.current.completed += 1

    @classmethod
    def from_defaults(cls, tracker_cls: Type[_ReadyCompletedTracker], **kwargs: int) -> "_Progress":
        """Utility function to easily create an instance from keyword arguments to both ``Tracker``s."""
        return cls(total=tracker_cls(**kwargs), current=tracker_cls(**kwargs))

    @override
    def reset(self) -> None:
        self.total.reset()
        self.current.reset()

    def reset_on_run(self) -> None:
        self.current.reset()

    def reset_on_restart(self) -> None:
        self.current.reset_on_restart()

    @override
    def load_state_dict(self, state_dict: dict) -> None:
        if state_dict["total"]["completed"] == None:
            state_dict["total"]["completed"] = 0
        self.total.load_state_dict(state_dict["total"])
        self.current.load_state_dict(state_dict["current"])


@dataclass
class _BatchProgress(_Progress):
    """Tracks batch progress.

    These counters are local to a trainer rank. By default, they are not globally synced across all ranks
.

    Args:
        total: Tracks the total batch progress.
        current: Tracks the current batch progress.
        is_last_batch: Whether the batch is the last one. This is useful for iterable datasets.

    """

    is_last_batch: bool = False

    @override
    def reset(self) -> None:
        super().reset()
        self.is_last_batch = False

    @override
    def reset_on_run(self) -> None:
        super().reset_on_run()
        self.is_last_batch = False

    @override
    def load_state_dict(self, state_dict: dict) -> None:
        if state_dict["total"]["completed"] == None:
            state_dict["total"]["completed"] = 0
        super().load_state_dict(state_dict)
        self.is_last_batch = state_dict["is_last_batch"]


@dataclass
class _SchedulerProgress(_Progress):
    """Tracks scheduler progress.

    These counters are local to a trainer rank. By default, they are not globally synced across all ranks
.

    Args:
        total: Tracks the total scheduler progress.
        current: Tracks the current scheduler progress.

    """

    total: _ReadyCompletedTracker = field(default_factory=_ReadyCompletedTracker)
    current: _ReadyCompletedTracker = field(default_factory=_ReadyCompletedTracker)


@dataclass
class _OptimizerProgress(_BaseProgress):
    """Track optimizer progress.

    Args:
        step: Tracks ``optimizer.step`` calls.
        zero_grad: Tracks ``optimizer.zero_grad`` calls.

    """

    step: _Progress = field(default_factory=lambda: _Progress.from_defaults(_ReadyCompletedTracker))
    zero_grad: _Progress = field(default_factory=lambda: _Progress.from_defaults(_StartedTracker))

    @override
    def reset(self) -> None:
        self.step.reset()
        self.zero_grad.reset()

    def reset_on_run(self) -> None:
        self.step.reset_on_run()
        self.zero_grad.reset_on_run()

    def reset_on_restart(self) -> None:
        self.step.reset_on_restart()
        self.zero_grad.reset_on_restart()

    @override
    def load_state_dict(self, state_dict: dict) -> None:
        if state_dict["step"]["total"]["completed"] == None:
            state_dict["step"]["total"]["completed"] = 0
        self.step.load_state_dict(state_dict["step"])
        self.zero_grad.load_state_dict(state_dict["zero_grad"])


@dataclass
class _OptimizationProgress(_BaseProgress):
    """Track optimization progress.

    Args:
        optimizer: Tracks optimizer progress.

    """

    optimizer: _OptimizerProgress = field(default_factory=_OptimizerProgress)

    @property
    def optimizer_steps(self) -> int:
        return self.optimizer.step.total.completed

    @override
    def reset(self) -> None:
        self.optimizer.reset()

    def reset_on_run(self) -> None:
        self.optimizer.reset_on_run()

    def reset_on_restart(self) -> None:
        self.optimizer.reset_on_restart()

    @override
    def load_state_dict(self, state_dict: dict) -> None:
        if state_dict["optimizer"]["step"]["total"]["completed"] == None:
            state_dict["optimizer"]["step"]["total"]["completed"] = 0
        self.optimizer.load_state_dict(state_dict["optimizer"])

2024-08-10 13:34:09.094 | INFO     | __main__:main:608 - Starting compute server...
2024-08-10 13:34:09.094 | INFO     | __main__:main:609 - Current working directory: /home/adarsh/ReProver
2024-08-10 13:34:09.094 | INFO     | __main__:main:614 - ROOT_DIR: /raid/adarsh
2024-08-10 13:34:09.094 | INFO     | __main__:main:615 - DATA_DIR: datasets_test
2024-08-10 13:34:09.094 | INFO     | __main__:main:619 - Configuring LeanDojo...
2024-08-10 13:34:09.097 | INFO     | generate_benchmark_lean4:configure_leandojo:294 - Current working di
rectory: /home/adarsh/ReProver
2024-08-10 13:34:09.097 | INFO     | __main__:main:621 - LeanDojo configured
2024-08-10 13:34:09.097 | INFO     | __main__:main:629 - Unique URLs: {'https://github.com/teorth/pfr.git
'}
2024-08-10 13:34:09.097 | INFO     | __main__:main:630 - About to generate datasets...
2024-08-10 13:34:09.097 | INFO     | __main__:generate_dataset:226 - Generating 1 datasets
2024-08-10 13:34:09.097 | INFO     | __main__:generate_dataset:231 - Processing https://github.com/teorth
/pfr.git
2024-08-10 13:34:09.330 | INFO     | __main__:get_compatible_commit:166 - Latest commit: 6a5082ee465f9e44
cea479c7b741b3163162bb7e
2024-08-10 13:34:09.331 | INFO     | __main__:get_compatible_commit:169 - Creating LeanGitRepo for https:
//github.com/teorth/pfr
2024-08-10 13:34:09.662 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the com
mit hash for lean4 v4.8.0-rc1
2024-08-10 13:34:10.296 | INFO     | __main__:get_compatible_commit:171 - Getting config for https://gith
ub.com/teorth/pfr.git
2024-08-10 13:34:10.296 | INFO     | __main__:get_compatible_commit:175 - Latest commit compatible for ur
l https://github.com/teorth/pfr.git
2024-08-10 13:34:10.296 | INFO     | __main__:generate_dataset:236 - Found compatible commit 6a5082ee465f
9e44cea479c7b741b3163162bb7e for https://github.com/teorth/pfr.git
2024-08-10 13:34:10.296 | INFO     | __main__:generate_dataset:237 - Lean version: v4.8.0-rc1
2024-08-10 13:34:10.296 | INFO     | __main__:generate_dataset:240 - Creating LeanGitRepo for https://git
hub.com/teorth/pfr
2024-08-10 13:34:10.296 | INFO     | __main__:generate_dataset:244 - Generating benchmark at /raid/adarsh
/datasets_test/pfr_6a5082ee465f9e44cea479c7b741b3163162bb7e
2024-08-10 13:34:10.296 | INFO     | generate_benchmark_lean4:main:301 - Generating dataset to go into /r
aid/adarsh/datasets_test/pfr_6a5082ee465f9e44cea479c7b741b3163162bb7e
2024-08-10 13:34:10.296 | INFO     | generate_benchmark_lean4:main:308 - lean toolchain version: {'conten
t': 'leanprover/lean4:v4.8.0-rc1\n'}
2024-08-10 13:34:10.296 | INFO     | generate_benchmark_lean4:main:310 - lean version v: v4.8.0-rc1
2024-08-10 13:34:10.297 | INFO     | generate_benchmark_lean4:main:311 - is supported: True
2024-08-10 13:34:10.297 | INFO     | generate_benchmark_lean4:main:321 - lean path1 /home/adarsh/.elan/to
olchains/leanprover--lean4---4.8.0-rc1
2024-08-10 13:34:10.297 | INFO     | generate_benchmark_lean4:main:322 - lean path2 /.elan/toolchains/lea
nprover--lean4---4.8.0-rc1
2024-08-10 13:34:10.297 | INFO     | generate_benchmark_lean4:main:323 - lean path3 ~/.elan/toolchains/le
anprover--lean4---4.8.0-rc1
2024-08-10 13:34:10.297 | INFO     | generate_benchmark_lean4:main:327 - Lean toolchain path 2 does not e
xist: /.elan/toolchains/leanprover--lean4---4.8.0-rc1
2024-08-10 13:34:10.297 | INFO     | generate_benchmark_lean4:main:329 - Lean toolchain path 3 does not e
xist: ~/.elan/toolchains/leanprover--lean4---4.8.0-rc1
2024-08-10 13:34:10.297 | INFO     | generate_benchmark_lean4:main:332 - Switched to Lean toolchain at: /
home/adarsh/.elan/toolchains/leanprover--lean4---4.8.0-rc1
2024-08-10 13:34:10.330 | INFO     | generate_benchmark_lean4:main:334 - lean --version: Lean (version 4.
8.0-rc1, x86_64-unknown-linux-gnu, commit dcccfb73cb24, Release)

2024-08-10 13:34:10.330 | INFO     | generate_benchmark_lean4:main:335 - repo: LeanGitRepo(url='https://g
ithub.com/teorth/pfr', commit='6a5082ee465f9e44cea479c7b741b3163162bb7e')
2024-08-10 13:34:10.330 | INFO     | generate_benchmark_lean4:main:337 - Configuring LeanDojo again...
2024-08-10 13:34:10.333 | INFO     | generate_benchmark_lean4:configure_leandojo:294 - Current working di
rectory: /home/adarsh/ReProver
2024-08-10 13:34:10.334 | INFO     | generate_benchmark_lean4:main:339 - LeanDojo configured
2024-08-10 13:34:10.334 | INFO     | generate_benchmark_lean4:main:342 - Tracing the repo...
2024-08-10 13:34:10.334 | DEBUG    | lean_dojo.data_extraction.trace:get_traced_repo_path:217 - The trace
d repo is available in the cache.
2024-08-10 13:34:10.334 | INFO     | lean_dojo.data_extraction.trace:trace:246 - Loading the traced repo
from /raid/adarsh/.cache/lean_dojo/teorth-pfr-6a5082ee465f9e44cea479c7b741b3163162bb7e/pfr
2024-08-10 13:34:10.336 | DEBUG    | lean_dojo.utils:execute:110 - git remote get-url origin
2024-08-10 13:34:10.339 | DEBUG    | lean_dojo.utils:execute:110 - git log -n 1
2024-08-10 13:34:10.435 | DEBUG    | lean_dojo.data_extraction.traced_data:load_from_disk:1169 - Loading
2890 traced XML files from /raid/adarsh/.cache/lean_dojo/teorth-pfr-6a5082ee465f9e44cea479c7b741b3163162b
b7e/pfr with 31 workers
2024-08-10 13:34:11,774 INFO worker.py:1772 -- Started a local Ray instance. View the dashboard at 127.0.
0.1:8267
  4%|██▉                                                                | 125/2890 [00:07<00:57, 47.87it/
s](raylet) [2024-08-10 13:34:21,664 E 4044142 4044172] (raylet) file_system_monitor.cc:111: /tmp/ray/sess
ion_2024-08-10_13-34-10_589132_4042934 is over 95% full, available space: 93571997696; capacity: 18875076
97664. Object creation will fail if spilling is required.
 13%|█████████                                                          | 390/2890 [00:16<00:49, 50.15it/
s](raylet) [2024-08-10 13:34:31,669 E 4044142 4044172] (raylet) file_system_monitor.cc:111: /tmp/ray/sess
ion_2024-08-10_13-34-10_589132_4042934 is over 95% full, available space: 93571969024; capacity: 18875076
97664. Object creation will fail if spilling is required.
 20%|█████████████▋                                                     | 589/2890 [00:27<04:32,  8.44it/
s](raylet) [2024-08-10 13:34:41,677 E 4044142 4044172] (raylet) file_system_monitor.cc:111: /tmp/ray/sess
ion_2024-08-10_13-34-10_589132_4042934 is over 95% full, available space: 93571944448; capacity: 18875076
97664. Object creation will fail if spilling is required.
 31%|████████████████████▌                                              | 887/2890 [00:36<00:36, 54.44it/
s](raylet) [2024-08-10 13:34:51,684 E 4044142 4044172] (raylet) file_system_monitor.cc:111: /tmp/ray/sess
ion_2024-08-10_13-34-10_589132_4042934 is over 95% full, available space: 93571923968; capacity: 18875076
97664. Object creation will fail if spilling is required.
 35%|███████████████████████▏                                           | 999/2890 [00:44<00:30, 62.72it/
s](raylet) [2024-08-10 13:35:01,690 E 4044142 4044172] (raylet) file_system_monitor.cc:111: /tmp/ray/sess
ion_2024-08-10_13-34-10_589132_4042934 is over 95% full, available space: 93571919872; capacity: 18875076
97664. Object creation will fail if spilling is required.
 38%|█████████████████████████▏                                        | 1102/2890 [01:05<20:43,  1.44it/
s](raylet) [2024-08-10 13:35:11,697 E 4044142 4044172] (raylet) file_system_monitor.cc:111: /tmp/ray/sess
ion_2024-08-10_13-34-10_589132_4042934 is over 95% full, available space: 93571862528; capacity: 18875076
97664. Object creation will fail if spilling is required.
 40%|██████████████████████████▌                                       | 1163/2890 [01:07<00:51, 33.86it/
s](raylet) [2024-08-10 13:35:21,705 E 4044142 4044172] (raylet) file_system_monitor.cc:111: /tmp/ray/sess
ion_2024-08-10_13-34-10_589132_4042934 is over 95% full, available space: 93571051520; capacity: 18875076
97664. Object creation will fail if spilling is required.
 42%|███████████████████████████▌                                      | 1208/2890 [01:21<00:39, 42.39it/
s](raylet) [2024-08-10 13:35:31,713 E 4044142 4044172] (raylet) file_system_monitor.cc:111: /tmp/ray/sess
ion_2024-08-10_13-34-10_589132_4042934 is over 95% full, available space: 93571043328; capacity: 18875076
97664. Object creation will fail if spilling is required.
 47%|██████████████████████████████▊                                   | 1347/2890 [01:41<00:41, 37.63it/
s](raylet) [2024-08-10 13:35:41,722 E 4044142 4044172] (raylet) file_system_monitor.cc:111: /tmp/ray/sess
ion_2024-08-10_13-34-10_589132_4042934 is over 95% full, available space: 93571014656; capacity: 18875076
97664. Object creation will fail if spilling is required.
 54%|███████████████████████████████████▊                              | 1566/2890 [02:06<22:59,  1.04s/i
t](raylet) [2024-08-10 13:36:01,739 E 4044142 4044172] (raylet) file_system_monitor.cc:111: /tmp/ray/sess
ion_2024-08-10_13-34-10_589132_4042934 is over 95% full, available space: 93570990080; capacity: 18875076
97664. Object creation will fail if spilling is required. [repeated 2x across cluster] (Ray deduplicates
logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/
ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)
 60%|███████████████████████████████████████▎                          | 1724/2890 [02:36<45:43,  2.35s/i
t](raylet) [2024-08-10 13:36:31,765 E 4044142 4044172] (raylet) file_system_monitor.cc:111: /tmp/ray/sess
ion_2024-08-10_13-34-10_589132_4042934 is over 95% full, available space: 93569863680; capacity: 18875076
97664. Object creation will fail if spilling is required. [repeated 3x across cluster]
 71%|███████████████████████████████████████████████                   | 2063/2890 [02:43<00:22, 37.19it/
 71%|███████████████████████████████████████████████                   | 2063/2890 [03:15<00:22, 37.19it/
 71%|███████████████████████████████████████████████▏                  | 2064/2890 [03:15<30:13,  2.20s/i
t](raylet) [2024-08-10 13:37:01,790 E 4044142 4044172] (raylet) file_system_monitor.cc:111: /tmp/ray/sess
ion_2024-08-10_13-34-10_589132_4042934 is over 95% full, available space: 93569810432; capacity: 18875076
97664. Object creation will fail if spilling is required. [repeated 3x across cluster]
 72%|███████████████████████████████████████████████▏                  | 2067/2890 [03:15<23:26,  1.71s/i
 72%|███████████████████████████████████████████████▎                  | 2071/2890 [03:15<16:24,  1.20s/i
 72%|███████████████████████████████████████████████▍                  | 2075/2890 [03:15<11:32,  1.18it/
 72%|███████████████████████████████████████████████▍                  | 2079/2890 [03:15<08:08,  1.66it/
 72%|███████████████████████████████████████████████▌                  | 2083/2890 [03:16<05:51,  2.30it/
 72%|███████████████████████████████████████████████▋                  | 2087/2890 [03:16<04:12,  3.18it/
 72%|███████████████████████████████████████████████▊                  | 2093/2890 [03:16<02:38,  5.02it/
 73%|███████████████████████████████████████████████▉                  | 2097/2890 [03:16<02:00,  6.56it/
 73%|████████████████████████████████████████████████                  | 2103/2890 [03:16<01:25,  9.21it/
 73%|████████████████████████████████████████████████                  | 2107/2890 [03:16<01:12, 10.85it/
 73%|████████████████████████████████████████████████▏                 | 2111/2890 [03:17<01:08, 11.44it/
 73%|████████████████████████████████████████████████▎                 | 2114/2890 [03:17<01:02, 12.48it/
 73%|████████████████████████████████████████████████▍                 | 2119/2890 [03:17<00:50, 15.31it/
 74%|████████████████████████████████████████████████▋                 | 2130/2890 [03:17<00:28, 27.05it/
 74%|████████████████████████████████████████████████▊                 | 2136/2890 [03:17<00:23, 32.14it/
 74%|████████████████████████████████████████████████▉                 | 2141/2890 [03:17<00:22, 32.73it/
 74%|█████████████████████████████████████████████████                 | 2146/2890 [03:17<00:23, 32.10it/
 74%|█████████████████████████████████████████████████▏                | 2152/2890 [03:18<00:19, 37.29it/
 75%|█████████████████████████████████████████████████▎                | 2157/2890 [03:18<00:24, 30.39it/
 75%|█████████████████████████████████████████████████▎                | 2162/2890 [03:18<00:23, 30.81it/
 75%|█████████████████████████████████████████████████▍                | 2166/2890 [03:18<00:22, 32.51it/
 75%|█████████████████████████████████████████████████▌                | 2170/2890 [03:18<00:24, 29.57it/
 75%|█████████████████████████████████████████████████▋                | 2174/2890 [03:18<00:23, 29.86it/
 75%|█████████████████████████████████████████████████▋                | 2178/2890 [03:19<00:25, 28.37it/
 76%|█████████████████████████████████████████████████▊                | 2182/2890 [03:19<00:27, 26.20it/
 76%|█████████████████████████████████████████████████▉                | 2188/2890 [03:19<00:21, 33.13it/
 76%|██████████████████████████████████████████████████▏               | 2195/2890 [03:19<00:17, 40.24it/
 76%|██████████████████████████████████████████████████▏               | 2200/2890 [03:19<00:20, 34.08it/
 76%|██████████████████████████████████████████████████▎               | 2204/2890 [03:19<00:19, 34.68it/
 76%|██████████████████████████████████████████████████▍               | 2208/2890 [03:19<00:21, 31.98it/
 77%|██████████████████████████████████████████████████▌               | 2213/2890 [03:20<00:19, 34.19it/
 77%|██████████████████████████████████████████████████▋               | 2217/2890 [03:20<00:20, 33.43it/
 77%|██████████████████████████████████████████████████▋               | 2222/2890 [03:20<00:17, 37.13it/
 77%|██████████████████████████████████████████████████▊               | 2227/2890 [03:20<00:17, 38.83it/
 77%|██████████████████████████████████████████████████▉               | 2232/2890 [03:20<00:19, 33.63it/
 77%|███████████████████████████████████████████████████               | 2236/2890 [03:20<00:21, 29.83it/
 78%|███████████████████████████████████████████████████▏              | 2242/2890 [03:20<00:17, 36.23it/
 78%|███████████████████████████████████████████████████▎              | 2247/2890 [03:20<00:17, 37.03it/
 78%|███████████████████████████████████████████████████▍              | 2252/2890 [03:21<00:16, 39.44it/
 78%|███████████████████████████████████████████████████▌              | 2258/2890 [03:21<00:15, 41.76it/
 78%|███████████████████████████████████████████████████▋              | 2263/2890 [03:21<00:16, 37.00it/
 78%|███████████████████████████████████████████████████▊              | 2267/2890 [03:21<00:18, 33.78it/
 79%|███████████████████████████████████████████████████▉              | 2272/2890 [03:21<00:16, 36.62it/
 79%|████████████████████████████████████████████████████              | 2277/2890 [03:21<00:15, 38.39it/
 79%|████████████████████████████████████████████████████              | 2281/2890 [03:22<00:22, 27.52it/
 79%|████████████████████████████████████████████████████▏             | 2285/2890 [03:22<00:26, 23.17it/
 79%|████████████████████████████████████████████████████▎             | 2290/2890 [03:22<00:22, 26.73it/
 79%|████████████████████████████████████████████████████▍             | 2295/2890 [03:22<00:23, 25.27it/
 80%|████████████████████████████████████████████████████▌             | 2299/2890 [03:22<00:23, 25.01it/
 80%|████████████████████████████████████████████████████▌             | 2302/2890 [03:22<00:24, 23.81it/
 80%|████████████████████████████████████████████████████▋             | 2306/2890 [03:23<00:22, 26.27it/
 80%|████████████████████████████████████████████████████▊             | 2310/2890 [03:23<00:20, 28.82it/
 80%|████████████████████████████████████████████████████▊             | 2314/2890 [03:23<00:26, 21.74it/
 80%|████████████████████████████████████████████████████▊             | 2314/2890 [04:02<00:26, 21.74it/
 80%|████████████████████████████████████████████████████▊             | 2315/2890 [04:02<36:22,  3.80s/i
t](raylet) [2024-08-10 13:37:41,825 E 4044142 4044172] (raylet) file_system_monitor.cc:111: /tmp/ray/sess
ion_2024-08-10_13-34-10_589132_4042934 is over 95% full, available space: 93568217088; capacity: 18875076
97664. Object creation will fail if spilling is required. [repeated 4x across cluster]
 80%|████████████████████████████████████████████████████▉             | 2318/2890 [04:02<25:43,  2.70s/i
 80%|█████████████████████████████████████████████████████             | 2321/2890 [04:02<18:09,  1.92s/i
 80%|█████████████████████████████████████████████████████             | 2324/2890 [04:02<12:51,  1.36s/i
 81%|█████████████████████████████████████████████████████▏            | 2328/2890 [04:02<08:15,  1.13it/
 81%|█████████████████████████████████████████████████████▎            | 2334/2890 [04:03<04:42,  1.97it/
 81%|█████████████████████████████████████████████████████▍            | 2338/2890 [04:03<03:23,  2.71it/
 81%|█████████████████████████████████████████████████████▍            | 2342/2890 [04:03<02:27,  3.72it/
 81%|█████████████████████████████████████████████████████▌            | 2348/2890 [04:03<01:36,  5.61it/
 81%|█████████████████████████████████████████████████████▋            | 2352/2890 [04:03<01:15,  7.15it/
 82%|█████████████████████████████████████████████████████▊            | 2357/2890 [04:03<00:54,  9.71it/
 82%|█████████████████████████████████████████████████████▉            | 2361/2890 [04:04<00:47, 11.07it/
 82%|█████████████████████████████████████████████████████▉            | 2364/2890 [04:04<00:42, 12.48it/
 82%|██████████████████████████████████████████████████████            | 2367/2890 [04:04<00:38, 13.48it/
 82%|██████████████████████████████████████████████████████            | 2370/2890 [04:04<00:39, 13.10it/
 82%|██████████████████████████████████████████████████████▏           | 2374/2890 [04:04<00:31, 16.28it/
 82%|██████████████████████████████████████████████████████▎           | 2378/2890 [04:04<00:25, 19.90it/
 82%|██████████████████████████████████████████████████████▍           | 2381/2890 [04:04<00:24, 20.38it/
 82%|██████████████████████████████████████████████████████▍           | 2384/2890 [04:05<00:30, 16.63it/
 83%|██████████████████████████████████████████████████████▌           | 2387/2890 [04:05<00:31, 15.89it/
 83%|██████████████████████████████████████████████████████▋           | 2392/2890 [04:05<00:23, 20.99it/
 83%|██████████████████████████████████████████████████████▋           | 2397/2890 [04:05<00:21, 22.93it/
 83%|██████████████████████████████████████████████████████▊           | 2402/2890 [04:05<00:19, 25.13it/
 83%|██████████████████████████████████████████████████████▉           | 2407/2890 [04:06<00:16, 29.46it/
 83%|███████████████████████████████████████████████████████           | 2411/2890 [04:06<00:20, 23.37it/
 84%|███████████████████████████████████████████████████████▏          | 2414/2890 [04:06<00:21, 21.89it/
 84%|███████████████████████████████████████████████████████▎          | 2420/2890 [04:06<00:16, 28.07it/
 84%|███████████████████████████████████████████████████████▎          | 2424/2890 [04:06<00:15, 29.28it/
 84%|███████████████████████████████████████████████████████▍          | 2429/2890 [04:06<00:13, 33.60it/
 84%|███████████████████████████████████████████████████████▌          | 2433/2890 [04:06<00:14, 31.81it/
 84%|███████████████████████████████████████████████████████▋          | 2437/2890 [04:07<00:17, 25.73it/
s](raylet) [2024-08-10 13:38:21,850 E 4044142 4044172] (raylet) file_system_monitor.cc:111: /tmp/ray/sess
ion_2024-08-10_13-34-10_589132_4042934 is over 95% full, available space: 93567262720; capacity: 18875076
97664. Object creation will fail if spilling is required. [repeated 4x across cluster]
 84%|███████████████████████████████████████████████████████▋          | 2440/2890 [04:07<00:23, 18.77it/
 85%|███████████████████████████████████████████████████████▊          | 2443/2890 [04:07<00:25, 17.22it/
 85%|███████████████████████████████████████████████████████▊          | 2446/2890 [04:07<00:27, 16.03it/
 85%|███████████████████████████████████████████████████████▉          | 2448/2890 [04:08<00:27, 15.87it/
 85%|███████████████████████████████████████████████████████▉          | 2450/2890 [04:08<00:33, 13.24it/
 85%|███████████████████████████████████████████████████████▉          | 2452/2890 [04:08<00:42, 10.39it/
 85%|████████████████████████████████████████████████████████          | 2456/2890 [04:08<00:29, 14.51it/
 85%|████████████████████████████████████████████████████████▏         | 2460/2890 [04:08<00:24, 17.77it/
 85%|████████████████████████████████████████████████████████▏         | 2463/2890 [04:08<00:21, 19.89it/
 85%|████████████████████████████████████████████████████████▎         | 2467/2890 [04:09<00:21, 19.98it/
 86%|████████████████████████████████████████████████████████▍         | 2473/2890 [04:09<00:15, 27.20it/
 86%|████████████████████████████████████████████████████████▌         | 2477/2890 [04:09<00:14, 29.08it/
 86%|████████████████████████████████████████████████████████▋         | 2483/2890 [04:09<00:11, 36.11it/
 86%|████████████████████████████████████████████████████████▉         | 2491/2890 [04:09<00:08, 46.64it/
 86%|█████████████████████████████████████████████████████████         | 2497/2890 [04:09<00:10, 37.69it/
 87%|█████████████████████████████████████████████████████████▏        | 2502/2890 [04:09<00:09, 38.89it/
 87%|█████████████████████████████████████████████████████████▎        | 2507/2890 [04:10<00:10, 37.19it/
 87%|█████████████████████████████████████████████████████████▎        | 2512/2890 [04:10<00:11, 33.15it/
 87%|█████████████████████████████████████████████████████████▍        | 2516/2890 [04:10<00:12, 30.48it/
 87%|█████████████████████████████████████████████████████████▌        | 2520/2890 [04:10<00:13, 28.08it/
 87%|█████████████████████████████████████████████████████████▋        | 2527/2890 [04:10<00:10, 34.62it/
 88%|█████████████████████████████████████████████████████████▊        | 2531/2890 [04:10<00:11, 32.45it/
 88%|█████████████████████████████████████████████████████████▉        | 2535/2890 [04:11<00:11, 31.34it/
 88%|█████████████████████████████████████████████████████████▉        | 2539/2890 [04:11<00:12, 28.85it/
 88%|██████████████████████████████████████████████████████████        | 2543/2890 [04:11<00:12, 28.03it/
 88%|██████████████████████████████████████████████████████████▎       | 2551/2890 [04:11<00:08, 38.62it/
 88%|██████████████████████████████████████████████████████████▎       | 2556/2890 [04:11<00:09, 35.04it/
 89%|██████████████████████████████████████████████████████████▍       | 2561/2890 [04:11<00:08, 38.26it/
 89%|██████████████████████████████████████████████████████████▌       | 2566/2890 [04:11<00:09, 32.54it/
 89%|██████████████████████████████████████████████████████████▉       | 2580/2890 [04:12<00:06, 50.26it/
 89%|██████████████████████████████████████████████████████████▉       | 2580/2890 [04:59<00:06, 50.26it/
 89%|██████████████████████████████████████████████████████████▉       | 2582/2890 [04:59<12:10,  2.37s/i
t](raylet) [2024-08-10 13:38:31,859 E 4044142 4044172] (raylet) file_system_monitor.cc:111: /tmp/ray/sess
ion_2024-08-10_13-34-10_589132_4042934 is over 95% full, available space: 93567246336; capacity: 18875076
97664. Object creation will fail if spilling is required.
(raylet) [2024-08-10 13:38:41,872 E 4044142 4044172] (raylet) file_system_monitor.cc:111: /tmp/ray/sessio
n_2024-08-10_13-34-10_589132_4042934 is over 95% full, available space: 93567234048; capacity: 1887507697
664. Object creation will fail if spilling is required.
 90%|███████████████████████████████████████████████████████████▏      | 2590/2890 [04:59<07:37,  1.52s/i
 90%|███████████████████████████████████████████████████████████▎      | 2598/2890 [04:59<04:54,  1.01s/i
 91%|███████████████████████████████████████████████████████████▊      | 2621/2890 [05:00<01:53,  2.36it/
 91%|████████████████████████████████████████████████████████████▏     | 2635/2890 [05:00<01:12,  3.53it/
 92%|████████████████████████████████████████████████████████████▌     | 2651/2890 [05:00<00:44,  5.40it/
 92%|████████████████████████████████████████████████████████████▊     | 2664/2890 [05:00<00:30,  7.44it/
 93%|█████████████████████████████████████████████████████████████▏    | 2682/2890 [05:00<00:18, 11.41it/
 93%|█████████████████████████████████████████████████████████████▌    | 2695/2890 [05:00<00:13, 14.92it/
 94%|█████████████████████████████████████████████████████████████▉    | 2710/2890 [05:00<00:08, 20.73it/
 94%|██████████████████████████████████████████████████████████████▎   | 2730/2890 [05:00<00:05, 30.91it/
 95%|██████████████████████████████████████████████████████████████▊   | 2749/2890 [05:01<00:03, 42.84it/
 96%|███████████████████████████████████████████████████████████████▏  | 2767/2890 [05:01<00:02, 56.17it/
 96%|███████████████████████████████████████████████████████████████▌  | 2784/2890 [05:01<00:01, 63.01it/
 97%|███████████████████████████████████████████████████████████████▉  | 2798/2890 [05:01<00:01, 62.84it/
 97%|████████████████████████████████████████████████████████████████▏ | 2810/2890 [05:01<00:01, 61.81it/
 98%|████████████████████████████████████████████████████████████████▍ | 2820/2890 [05:01<00:01, 64.01it/
 98%|████████████████████████████████████████████████████████████████▋ | 2830/2890 [05:02<00:00, 66.66it/
 98%|████████████████████████████████████████████████████████████████▉ | 2841/2890 [05:02<00:00, 66.57it/
 99%|█████████████████████████████████████████████████████████████████▍| 2863/2890 [05:02<00:00, 95.42it/
100%|████████████████████████████████████████████████████████████████▊| 2879/2890 [05:02<00:00, 102.33it/
100%|██████████████████████████████████████████████████████████████████| 2890/2890 [05:03<00:00,  9.53it/
s]
(raylet) [2024-08-10 13:39:11,899 E 4044142 4044172] (raylet) file_system_monitor.cc:111: /tmp/ray/sessio
n_2024-08-10_13-34-10_589132_4042934 is over 95% full, available space: 93567135744; capacity: 1887507697
664. Object creation will fail if spilling is required. [repeated 3x across cluster]
2024-08-10 13:39:19.925 | DEBUG    | lean_dojo.data_extraction.lean:get_dependencies:489 - Querying the d
ependencies of LeanGitRepo(url='https://github.com/teorth/pfr', commit='6a5082ee465f9e44cea479c7b741b3163
162bb7e')
2024-08-10 13:39:21.312 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the com
mit hash for lean4 v4.6.0-rc1
2024-08-10 13:39:23.508 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the com
mit hash for lean4 v4.7.0
2024-08-10 13:39:24.521 | DEBUG    | lean_dojo.data_extraction.lean:url_to_repo:68 - url_to_repo("https:/
/github.com/leanprover-community/import-graph.git") failed. Retrying...
2024-08-10 13:39:25.641 | DEBUG    | lean_dojo.data_extraction.lean:url_to_repo:68 - url_to_repo("https:/
/github.com/leanprover-community/import-graph.git") failed. Retrying...
2024-08-10 13:39:28.801 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the com
mit hash for lean4 v4.10.0
2024-08-10 13:39:29.857 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the com
mit hash for doc-gen4 main
2024-08-10 13:39:30.501 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the com
mit hash for lean4 v4.11.0-rc1
2024-08-10 13:39:31.094 | DEBUG    | lean_dojo.data_extraction.lean:get_dependencies:489 - Querying the d
ependencies of LeanGitRepo(url='https://github.com/YaelDillies/LeanAPAP', commit='f47447e44d8e82ab214ed8c
1b199329141fc5b1f')
2024-08-10 13:39:31.814 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the com
mit hash for lean4 v4.9.0-rc1
2024-08-10 13:39:32.871 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the com
mit hash for lean4 v4.10.0-rc1
2024-08-10 13:39:33.646 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the com
mit hash for lean4 v4.9.0
2024-08-10 13:39:35.089 | DEBUG    | lean_dojo.data_extraction.lean:url_to_repo:68 - url_to_repo("https:/
/github.com/leanprover-community/mathlib4.git") failed. Retrying...
2024-08-10 13:39:36.223 | DEBUG    | lean_dojo.data_extraction.lean:url_to_repo:68 - url_to_repo("https:/
/github.com/leanprover-community/mathlib4.git") failed. Retrying...
2024-08-10 13:39:39.487 | DEBUG    | lean_dojo.data_extraction.lean:get_dependencies:489 - Querying the d
ependencies of LeanGitRepo(url='https://github.com/leanprover-community/mathlib4', commit='0077925cbead60
622b55b430eb5109f7365954c8')
2024-08-10 13:39:40.592 | DEBUG    | lean_dojo.data_extraction.lean:get_dependencies:489 - Querying the d
ependencies of LeanGitRepo(url='https://github.com/leanprover/doc-gen4', commit='6d8e3118ab526f8dfcabcbdf
9f05dc34e5c423a8')
2024-08-10 13:39:41.575 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the com
mit hash for lean4 v4.10.0-rc2
Following Github server redirection from /repos/mhuisi/lean4-cli to /repositories/341363356
2024-08-10 13:39:43.359 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the com
mit hash for lean4 nightly-2024-06-05
2024-08-10 13:39:44.605 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the com
mit hash for md4lean main
2024-08-10 13:39:44.783 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the com
mit hash for BibtexQuery master
2024-08-10 13:39:45.182 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the com
mit hash for lean4-unicode-basic main
2024-08-10 13:39:45.788 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the com
mit hash for lean4-cli nightly
2024-08-10 13:39:46.048 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the com
mit hash for lean4 nightly-2024-06-05
2024-08-10 13:39:47.291 | INFO     | generate_benchmark_lean4:main:346 - Failed to trace repo LeanGitRepo
(url='https://github.com/teorth/pfr', commit='6a5082ee465f9e44cea479c7b741b3163162bb7e') because of Inval
id tag or branch: `nightly-2024-06-05` for Repository(full_name="leanprover/lean4")
2024-08-10 13:39:57.072 | INFO     | __main__:generate_dataset:246 - Finished generating benchmark at /ra
id/adarsh/datasets_test/pfr_6a5082ee465f9e44cea479c7b741b3163162bb7e
2024-08-10 13:39:57.075 | INFO     | __main__:generate_dataset:247 - Merging datasets
2024-08-10 13:39:57.075 | INFO     | __main__:merge_datasets:74 - Merging datasets for random
2024-08-10 13:39:57.075 | INFO     | __main__:merge_datasets:80 - Processing train split
2024-08-10 13:39:57.075 | INFO     | __main__:merge_datasets:104 - Finished processing train split
2024-08-10 13:39:57.075 | INFO     | __main__:merge_datasets:80 - Processing val split
2024-08-10 13:39:57.075 | INFO     | __main__:merge_datasets:104 - Finished processing val split
2024-08-10 13:39:57.075 | INFO     | __main__:merge_datasets:80 - Processing test split
2024-08-10 13:39:57.076 | INFO     | __main__:merge_datasets:104 - Finished processing test split
2024-08-10 13:39:57.076 | INFO     | __main__:merge_datasets:105 - Finished merging datasets for random
2024-08-10 13:39:57.076 | INFO     | __main__:merge_datasets:74 - Merging datasets for novel_premises
2024-08-10 13:39:57.076 | INFO     | __main__:merge_datasets:80 - Processing train split
2024-08-10 13:39:57.076 | INFO     | __main__:merge_datasets:104 - Finished processing train split
2024-08-10 13:39:57.076 | INFO     | __main__:merge_datasets:80 - Processing val split
2024-08-10 13:39:57.076 | INFO     | __main__:merge_datasets:104 - Finished processing val split
2024-08-10 13:39:57.076 | INFO     | __main__:merge_datasets:80 - Processing test split
2024-08-10 13:39:57.076 | INFO     | __main__:merge_datasets:104 - Finished processing test split
2024-08-10 13:39:57.076 | INFO     | __main__:merge_datasets:105 - Finished merging datasets for novel_pr
emises
2024-08-10 13:39:57.076 | INFO     | __main__:merge_datasets:107 - Merging corpus
2024-08-10 13:39:57.077 | INFO     | __main__:merge_datasets:130 - Finished merging corpus
2024-08-10 13:39:57.077 | INFO     | __main__:merge_datasets:132 - Adding metadata
2024-08-10 13:39:57.077 | WARNING  | __main__:merge_datasets:152 - No metadata file found
2024-08-10 13:39:57.077 | INFO     | __main__:merge_datasets:154 - Deleting individual datasets
2024-08-10 13:39:57.077 | INFO     | __main__:generate_dataset:249 - Finished merging datasets
2024-08-10 13:39:57.204 | INFO     | __main__:main:636 - Latest PL checkpoint found: AK123321/pl-leancopi
lot-2
2024-08-10 13:39:57.296 | INFO     | __main__:download_pl_checkpoint:274 - Checkpoint downloaded to: /hom
e/adarsh/.cache/huggingface/hub/models--AK123321--pl-leancopilot-2/snapshots/d4955ed1972ad84c03dce9189162
d03912f2d86f/model.ckpt
2024-08-10 13:39:57.296 | INFO     | __main__:main:639 - Checkpoint path: /home/adarsh/.cache/huggingface
/hub/models--AK123321--pl-leancopilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86f/model.ckpt
2024-08-10 13:39:57.296 | INFO     | __main__:train:464 - Training model with checkpoint: /home/adarsh/.c
ache/huggingface/hub/models--AK123321--pl-leancopilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86
f/model.ckpt
Seed set to 3407
Lightning automatically upgraded your loaded checkpoint from v0.0.0 to v2.4.0. To apply the upgrade to yo
ur files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../.cache/huggingface
/hub/models--AK123321--pl-leancopilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86f/model.ckpt`
2024-08-10 13:40:17.726 | INFO     | __main__:train:486 - Loaded premise retriever at /home/adarsh/.cache
/huggingface/hub/models--AK123321--pl-leancopilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86f/mo
del.ckpt
2024-08-10 13:40:17.727 | INFO     | __main__:train:490 - Data path: /raid/adarsh/datasets_test/merged/ra
ndom
/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/transformers/tokenization
_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True`
by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by def
ault. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
2024-08-10 13:40:17.839 | INFO     | common:__init__:199 - Building the corpus from /raid/adarsh/datasets
_test/merged/corpus.jsonl
2024-08-10 13:40:19.346 | INFO     | retrieval.datamodule:load_or_cache_data:52 - Loaded data from cache
/raid/adarsh/datasets_test/merged/random/cache_train/cached_data.pkl
Training dataset size: 353052
2024-08-10 13:40:19.379 | INFO     | retrieval.datamodule:load_or_cache_data:52 - Loaded data from cache
/raid/adarsh/datasets_test/merged/random/cache_val/cached_data.pkl
Validation dataset size: 5403
2024-08-10 13:40:19.412 | INFO     | retrieval.datamodule:load_or_cache_data:52 - Loaded data from cache
/raid/adarsh/datasets_test/merged/random/cache_pred/cached_data.pkl
Testing dataset size: 5321
2024-08-10 13:40:19.414 | INFO     | __main__:train:523 - Training dataset size after load: 353052
2024-08-10 13:40:19.414 | INFO     | __main__:train:524 - Validation dataset size after load: 5403
2024-08-10 13:40:19.414 | INFO     | __main__:train:525 - Testing dataset size after load: 5321
Using bfloat16 Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
2024-08-10 13:40:19.424 | INFO     | __main__:train:543 - Starting progressive training...
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 1 processes
----------------------------------------------------------------------------------------------------

2024-08-10 13:40:28.553 | INFO     | retrieval.datamodule:load_or_cache_data:52 - Loaded data from cache
/raid/adarsh/datasets_test/merged/random/cache_train/cached_data.pkl
Training dataset size: 353052
2024-08-10 13:40:28.787 | INFO     | retrieval.datamodule:load_or_cache_data:52 - Loaded data from cache
/raid/adarsh/datasets_test/merged/random/cache_val/cached_data.pkl
Validation dataset size: 5403
2024-08-10 13:40:28.828 | INFO     | retrieval.datamodule:load_or_cache_data:52 - Loaded data from cache
/raid/adarsh/datasets_test/merged/random/cache_pred/cached_data.pkl
Testing dataset size: 5321
/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning/callbac
ks/model_checkpoint.py:654: Checkpoint directory /raid/adarsh/checkpoints_test exists and is not empty.
Restoring states from the checkpoint path at /home/adarsh/.cache/huggingface/hub/models--AK123321--pl-lea
ncopilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86f/model.ckpt
Lightning automatically upgraded your loaded checkpoint from v0.0.0 to v2.4.0. To apply the upgrade to yo
ur files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../.cache/huggingface
/hub/models--AK123321--pl-leancopilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86f/model.ckpt`
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
2024-08-10 13:40:30.187 | INFO     | common:get_optimizers:432 - Optimizing with AdamW

  | Name    | Type           | Params | Mode
---------------------------------------------------
0 | encoder | T5EncoderModel | 217 M  | train
---------------------------------------------------
217 M     Trainable params
0         Non-trainable params
217 M     Total params
870.630   Total estimated model params size (MB)
235       Modules in train mode
0         Modules in eval mode
Restored all states from the checkpoint at /home/adarsh/.cache/huggingface/hub/models--AK123321--pl-leanc
opilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86f/model.ckpt
Epoch 0:   0%|                                                                | 0/176526 [00:00<?, ?it/s]
2024-08-10 13:40:35.120 | INFO     | __main__:main:648 - An error occurred: Caught KeyError in DataLoader
 worker process 0.
Original Traceback (most recent call last):
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/torch/utils/data/
_utils/worker.py", line 309, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/torch/utils/data/
_utils/fetch.py", line 52, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/torch/utils/data/
_utils/fetch.py", line 52, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/adarsh/ReProver/retrieval/datamodule.py", line 123, in __getitem__
    for p in self.corpus.get_premises(ex["context"].path):
  File "/home/adarsh/ReProver/common.py", line 251, in get_premises
    return self._get_file(path).premises
  File "/home/adarsh/ReProver/common.py", line 226, in _get_file
    return self.transitive_dep_graph.nodes[path]["file"]
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/networkx/classes/
reportviews.py", line 194, in __getitem__
    return self._nodes[n]
KeyError: '.lake/packages/mathlib/Mathlib/Data/Ordmap/Ordset.lean'

Traceback (most recent call last):
  File "/home/adarsh/ReProver/compute_server.py", line 642, in main
    train(model_checkpoint_path, merged_data_path, next_suffix)
  File "/home/adarsh/ReProver/compute_server.py", line 545, in train
    trainer.fit(model, datamodule=data_module, ckpt_path=model_checkpoint_path)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/trainer/call.py", line 46, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/trainer/trainer.py", line 1025, in _run_stage
    self.fit_loop.run()
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/loops/training_epoch_loop.py", line 140, in run
    self.advance(data_fetcher)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/loops/training_epoch_loop.py", line 212, in advance
    batch, _, __ = next(data_fetcher)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/utilities/combined_loader.py", line 78, in __next__
    out[i] = next(self.iterators[i])
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/torch/utils/data/
dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/torch/utils/data/
dataloader.py", line 1344, in _next_data
    return self._process_data(data)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/torch/utils/data/
dataloader.py", line 1370, in _process_data
    data.reraise()
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/torch/_utils.py",
 line 706, in reraise
    raise exception
KeyError: Caught KeyError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/torch/utils/data/
_utils/worker.py", line 309, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/torch/utils/data/
_utils/fetch.py", line 52, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/torch/utils/data/
_utils/fetch.py", line 52, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/adarsh/ReProver/retrieval/datamodule.py", line 123, in __getitem__
    for p in self.corpus.get_premises(ex["context"].path):
  File "/home/adarsh/ReProver/common.py", line 251, in get_premises
    return self._get_file(path).premises
  File "/home/adarsh/ReProver/common.py", line 226, in _get_file
    return self.transitive_dep_graph.nodes[path]["file"]
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/networkx/classes/
reportviews.py", line 194, in __getitem__
    return self._nodes[n]
KeyError: '.lake/packages/mathlib/Mathlib/Data/Ordmap/Ordset.lean'

Epoch 0:   0%|                                                                | 0/176526 [00:13<?, ?it/s]
(base) adarsh@tensorlab-DGX-Station-A100-920-23487-2531-000:~/ReProver(base) adarsh@tensorlab-DGX-Station
-A(base) adarsh@tensorlab-DGX-Station-A100-920-23487-2531-000:~/ReProver$ bash run_compute_server.sh
Script executed from: /home/adarsh/ReProver
[2024-08-10 13:46:12,736] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda
 (auto detect)
 [WARNING]  async_io requires the dev libaio .so object and headers but these were not found.
 [WARNING]  async_io: please install the libaio-dev package with apt
 [WARNING]  If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS envi
ronment variables to where it can be found.
 [WARNING]  Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 [WARNING]  sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
 [WARNING]  using untested triton version (3.0.0), only 1.0.0 is known to be compatible
/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/deepspeed/runtime/zero/li
near.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.cust
om_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/deepspeed/runtime/zero/li
near.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.cust
om_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
2024-08-10 13:46:25.694 | INFO     | __main__:check_progress_file:586 - Checking contents of /home/adarsh
/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning/loops/progress.py
2024-08-10 13:46:25.694 | INFO     | __main__:check_progress_file:590 - Contents of progress.py:
2024-08-10 13:46:25.694 | INFO     | __main__:check_progress_file:591 - # Copyright The Lightning AI team
.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import asdict, dataclass, field
from typing import Type

from typing_extensions import override


@dataclass
class _BaseProgress:
    """Mixin that implements state-loading utilities for dataclasses."""

    def state_dict(self) -> dict:
        return asdict(self)

    def load_state_dict(self, state_dict: dict) -> None:
        if state_dict["completed"] == None:
            state_dict["completed"] = 0
        self.__dict__.update(state_dict)

    @classmethod
    def from_state_dict(cls, state_dict: dict) -> "_BaseProgress":
        obj = cls()
        obj.load_state_dict(state_dict)
        return obj

    def reset(self) -> None:
        """Reset the object's state."""
        raise NotImplementedError


@dataclass
class _ReadyCompletedTracker(_BaseProgress):
    """Track an event's progress.

    Args:
        ready: Intended to track the number of events ready to start.
        completed: Intended to be incremented after the event completes (e.g. after ``on_*_end`` runs).

    These attributes should be increased in order, that is, :attr:`ready` first and :attr:`completed` las
t.

    """

    ready: int = 0
    completed: int = 0

    @override
    def reset(self) -> None:
        """Reset the state."""
        self.ready = 0
        self.completed = 0

    def reset_on_restart(self) -> None:
        """Reset the progress on restart.

        If there is a failure before all attributes are increased, restore the attributes to the last ful
ly completed
        value.

        """
        self.ready = self.completed


@dataclass
class _StartedTracker(_ReadyCompletedTracker):
    """Track an event's progress.

    Args:
        ready: Intended to track the number of events ready to start.
        started: Intended to be incremented after the event is started (e.g. after ``on_*_start`` runs).
        completed: Intended to be incremented after the event completes (e.g. after ``on_*_end`` runs).

    These attributes should be increased in order, that is, :attr:`ready` first and :attr:`completed` las
t.

    """

    started: int = 0

    @override
    def reset(self) -> None:
        super().reset()
        self.started = 0

    @override
    def reset_on_restart(self) -> None:
        super().reset_on_restart()
        self.started = self.completed


@dataclass
class _ProcessedTracker(_StartedTracker):
    """Track an event's progress.

    Args:
        ready: Intended to track the number of events ready to start.
        started: Intended to be incremented after the event is started (e.g. after ``on_*_start`` runs).
        processed: Intended to be incremented after the event is processed.
        completed: Intended to be incremented after the event completes (e.g. after ``on_*_end`` runs).

    These attributes should be increased in order, that is, :attr:`ready` first and :attr:`completed` las
t.

    """

    processed: int = 0

    @override
    def reset(self) -> None:
        super().reset()
        self.processed = 0

    @override
    def reset_on_restart(self) -> None:
        super().reset_on_restart()
        self.processed = self.completed


@dataclass
class _Progress(_BaseProgress):
    """Track aggregated and current progress.

    Args:
        total: Intended to track the total progress of an event.
        current: Intended to track the current progress of an event.

    """

    total: _ReadyCompletedTracker = field(default_factory=_ProcessedTracker)
    current: _ReadyCompletedTracker = field(default_factory=_ProcessedTracker)

    def __post_init__(self) -> None:
        if self.total.__class__ is not self.current.__class__:
            raise ValueError("The `total` and `current` instances should be of the same class")

    def increment_ready(self) -> None:
        self.total.ready += 1
        self.current.ready += 1

    def increment_started(self) -> None:
        if not isinstance(self.total, _StartedTracker):
            raise TypeError(f"`{self.total.__class__.__name__}` doesn't have a `started` attribute")
        self.total.started += 1
        self.current.started += 1

    def increment_processed(self) -> None:
        if not isinstance(self.total, _ProcessedTracker):
            raise TypeError(f"`{self.total.__class__.__name__}` doesn't have a `processed` attribute")
        self.total.processed += 1
        self.current.processed += 1

    def increment_completed(self) -> None:
        self.total.completed += 1
        self.current.completed += 1

    @classmethod
    def from_defaults(cls, tracker_cls: Type[_ReadyCompletedTracker], **kwargs: int) -> "_Progress":
        """Utility function to easily create an instance from keyword arguments to both ``Tracker``s."""
        return cls(total=tracker_cls(**kwargs), current=tracker_cls(**kwargs))

    @override
    def reset(self) -> None:
        self.total.reset()
        self.current.reset()

    def reset_on_run(self) -> None:
        self.current.reset()

    def reset_on_restart(self) -> None:
        self.current.reset_on_restart()

    @override
    def load_state_dict(self, state_dict: dict) -> None:
        if state_dict["total"]["completed"] == None:
            state_dict["total"]["completed"] = 0
        self.total.load_state_dict(state_dict["total"])
        self.current.load_state_dict(state_dict["current"])


@dataclass
class _BatchProgress(_Progress):
    """Tracks batch progress.

    These counters are local to a trainer rank. By default, they are not globally synced across all ranks
.

    Args:
        total: Tracks the total batch progress.
        current: Tracks the current batch progress.
        is_last_batch: Whether the batch is the last one. This is useful for iterable datasets.

    """

    is_last_batch: bool = False

    @override
    def reset(self) -> None:
        super().reset()
        self.is_last_batch = False

    @override
    def reset_on_run(self) -> None:
        super().reset_on_run()
        self.is_last_batch = False

    @override
    def load_state_dict(self, state_dict: dict) -> None:
        if state_dict["total"]["completed"] == None:
            state_dict["total"]["completed"] = 0
        super().load_state_dict(state_dict)
        self.is_last_batch = state_dict["is_last_batch"]


@dataclass
class _SchedulerProgress(_Progress):
    """Tracks scheduler progress.

    These counters are local to a trainer rank. By default, they are not globally synced across all ranks
.

    Args:
        total: Tracks the total scheduler progress.
        current: Tracks the current scheduler progress.

    """

    total: _ReadyCompletedTracker = field(default_factory=_ReadyCompletedTracker)
    current: _ReadyCompletedTracker = field(default_factory=_ReadyCompletedTracker)


@dataclass
class _OptimizerProgress(_BaseProgress):
    """Track optimizer progress.

    Args:
        step: Tracks ``optimizer.step`` calls.
        zero_grad: Tracks ``optimizer.zero_grad`` calls.

    """

    step: _Progress = field(default_factory=lambda: _Progress.from_defaults(_ReadyCompletedTracker))
    zero_grad: _Progress = field(default_factory=lambda: _Progress.from_defaults(_StartedTracker))

    @override
    def reset(self) -> None:
        self.step.reset()
        self.zero_grad.reset()

    def reset_on_run(self) -> None:
        self.step.reset_on_run()
        self.zero_grad.reset_on_run()

    def reset_on_restart(self) -> None:
        self.step.reset_on_restart()
        self.zero_grad.reset_on_restart()

    @override
    def load_state_dict(self, state_dict: dict) -> None:
        if state_dict["step"]["total"]["completed"] == None:
            state_dict["step"]["total"]["completed"] = 0
        self.step.load_state_dict(state_dict["step"])
        self.zero_grad.load_state_dict(state_dict["zero_grad"])


@dataclass
class _OptimizationProgress(_BaseProgress):
    """Track optimization progress.

    Args:
        optimizer: Tracks optimizer progress.

    """

    optimizer: _OptimizerProgress = field(default_factory=_OptimizerProgress)

    @property
    def optimizer_steps(self) -> int:
        return self.optimizer.step.total.completed

    @override
    def reset(self) -> None:
        self.optimizer.reset()

    def reset_on_run(self) -> None:
        self.optimizer.reset_on_run()

    def reset_on_restart(self) -> None:
        self.optimizer.reset_on_restart()

    @override
    def load_state_dict(self, state_dict: dict) -> None:
        if state_dict["optimizer"]["step"]["total"]["completed"] == None:
            state_dict["optimizer"]["step"]["total"]["completed"] = 0
        self.optimizer.load_state_dict(state_dict["optimizer"])

2024-08-10 13:46:25.695 | INFO     | __main__:main:608 - Starting compute server...
2024-08-10 13:46:25.695 | INFO     | __main__:main:609 - Current working directory: /home/adarsh/ReProver
2024-08-10 13:46:25.695 | INFO     | __main__:main:617 - ROOT_DIR: /raid/adarsh
2024-08-10 13:46:25.695 | INFO     | __main__:main:618 - DATA_DIR: datasets_test
2024-08-10 13:46:25.695 | INFO     | __main__:main:622 - Configuring LeanDojo...
2024-08-10 13:46:25.698 | INFO     | generate_benchmark_lean4:configure_leandojo:294 - Current working di
rectory: /home/adarsh/ReProver
2024-08-10 13:46:25.698 | INFO     | __main__:main:624 - LeanDojo configured
2024-08-10 13:46:25.698 | INFO     | __main__:main:632 - Unique URLs: {'https://github.com/teorth/pfr.git
'}
2024-08-10 13:46:25.698 | INFO     | __main__:main:633 - About to generate datasets...
2024-08-10 13:46:25.698 | INFO     | __main__:generate_dataset:226 - Generating 1 datasets
2024-08-10 13:46:25.698 | INFO     | __main__:generate_dataset:231 - Processing https://github.com/teorth
/pfr.git
2024-08-10 13:46:25.915 | INFO     | __main__:get_compatible_commit:166 - Latest commit: 6a5082ee465f9e44
cea479c7b741b3163162bb7e
2024-08-10 13:46:25.916 | INFO     | __main__:get_compatible_commit:169 - Creating LeanGitRepo for https:
//github.com/teorth/pfr
2024-08-10 13:46:26.294 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the com
mit hash for lean4 v4.8.0-rc1
2024-08-10 13:46:26.918 | INFO     | __main__:get_compatible_commit:171 - Getting config for https://gith
ub.com/teorth/pfr.git
2024-08-10 13:46:26.918 | INFO     | __main__:get_compatible_commit:175 - Latest commit compatible for ur
l https://github.com/teorth/pfr.git
2024-08-10 13:46:26.918 | INFO     | __main__:generate_dataset:236 - Found compatible commit 6a5082ee465f
9e44cea479c7b741b3163162bb7e for https://github.com/teorth/pfr.git
2024-08-10 13:46:26.918 | INFO     | __main__:generate_dataset:237 - Lean version: v4.8.0-rc1
2024-08-10 13:46:26.918 | INFO     | __main__:generate_dataset:240 - Creating LeanGitRepo for https://git
hub.com/teorth/pfr
2024-08-10 13:46:26.918 | INFO     | __main__:generate_dataset:244 - Generating benchmark at /raid/adarsh
/datasets_test/pfr_6a5082ee465f9e44cea479c7b741b3163162bb7e
2024-08-10 13:46:26.918 | INFO     | generate_benchmark_lean4:main:301 - Generating dataset to go into /r
aid/adarsh/datasets_test/pfr_6a5082ee465f9e44cea479c7b741b3163162bb7e
2024-08-10 13:46:26.918 | INFO     | generate_benchmark_lean4:main:308 - lean toolchain version: {'conten
t': 'leanprover/lean4:v4.8.0-rc1\n'}
2024-08-10 13:46:26.918 | INFO     | generate_benchmark_lean4:main:310 - lean version v: v4.8.0-rc1
2024-08-10 13:46:26.919 | INFO     | generate_benchmark_lean4:main:311 - is supported: True
2024-08-10 13:46:26.919 | INFO     | generate_benchmark_lean4:main:321 - lean path1 /home/adarsh/.elan/to
olchains/leanprover--lean4---4.8.0-rc1
2024-08-10 13:46:26.919 | INFO     | generate_benchmark_lean4:main:322 - lean path2 /.elan/toolchains/lea
nprover--lean4---4.8.0-rc1
2024-08-10 13:46:26.919 | INFO     | generate_benchmark_lean4:main:323 - lean path3 ~/.elan/toolchains/le
anprover--lean4---4.8.0-rc1
2024-08-10 13:46:26.919 | INFO     | generate_benchmark_lean4:main:327 - Lean toolchain path 2 does not e
xist: /.elan/toolchains/leanprover--lean4---4.8.0-rc1
2024-08-10 13:46:26.919 | INFO     | generate_benchmark_lean4:main:329 - Lean toolchain path 3 does not e
xist: ~/.elan/toolchains/leanprover--lean4---4.8.0-rc1
2024-08-10 13:46:26.919 | INFO     | generate_benchmark_lean4:main:332 - Switched to Lean toolchain at: /
home/adarsh/.elan/toolchains/leanprover--lean4---4.8.0-rc1
2024-08-10 13:46:26.951 | INFO     | generate_benchmark_lean4:main:334 - lean --version: Lean (version 4.
8.0-rc1, x86_64-unknown-linux-gnu, commit dcccfb73cb24, Release)

2024-08-10 13:46:26.951 | INFO     | generate_benchmark_lean4:main:335 - repo: LeanGitRepo(url='https://g
ithub.com/teorth/pfr', commit='6a5082ee465f9e44cea479c7b741b3163162bb7e')
2024-08-10 13:46:26.952 | INFO     | generate_benchmark_lean4:main:337 - Configuring LeanDojo again...
2024-08-10 13:46:26.955 | INFO     | generate_benchmark_lean4:configure_leandojo:294 - Current working di
rectory: /home/adarsh/ReProver
2024-08-10 13:46:26.955 | INFO     | generate_benchmark_lean4:main:339 - LeanDojo configured
2024-08-10 13:46:26.955 | INFO     | generate_benchmark_lean4:main:342 - Tracing the repo...
2024-08-10 13:46:27.085 | INFO     | lean_dojo.data_extraction.trace:get_traced_repo_path:209 - Tracing L
eanGitRepo(url='https://github.com/teorth/pfr', commit='6a5082ee465f9e44cea479c7b741b3163162bb7e')
2024-08-10 13:46:27.085 | DEBUG    | lean_dojo.data_extraction.trace:get_traced_repo_path:211 - Working i
n the temporary directory /tmp/tmpinjlk62b
2024-08-10 13:46:27.673 | DEBUG    | lean_dojo.data_extraction.lean:clone_and_checkout:469 - Cloning Lean
GitRepo(url='https://github.com/teorth/pfr', commit='6a5082ee465f9e44cea479c7b741b3163162bb7e')
2024-08-10 13:46:27.674 | DEBUG    | lean_dojo.utils:execute:110 - git clone -n --recursive https://githu
b.com/teorth/pfr
2024-08-10 13:46:28.409 | DEBUG    | lean_dojo.utils:execute:110 - git checkout 6a5082ee465f9e44cea479c7b
741b3163162bb7e && git submodule update --recursive
2024-08-10 13:46:28.448 | DEBUG    | lean_dojo.data_extraction.trace:_trace:131 - Tracing LeanGitRepo(url
='https://github.com/teorth/pfr', commit='6a5082ee465f9e44cea479c7b741b3163162bb7e')
2024-08-10 13:46:28.448 | DEBUG    | lean_dojo.utils:execute:110 - lake build
info: LeanAPAP: cloning https://github.com/YaelDillies/LeanAPAP.git to '././.lake/packages/LeanAPAP'
info: mathlib: cloning https://github.com/leanprover-community/mathlib4.git to '././.lake/packages/mathli
b'
info: batteries: cloning https://github.com/leanprover-community/batteries to '././.lake/packages/batteri
es'
info: Qq: cloning https://github.com/leanprover-community/quote4 to '././.lake/packages/Qq'
info: aesop: cloning https://github.com/leanprover-community/aesop to '././.lake/packages/aesop'
info: proofwidgets: cloning https://github.com/leanprover-community/ProofWidgets4 to '././.lake/packages/
proofwidgets'
info: Cli: cloning https://github.com/leanprover/lean4-cli to '././.lake/packages/Cli'
info: importGraph: cloning https://github.com/leanprover-community/import-graph.git to '././.lake/package
s/importGraph'
[1905/1983] Building PFR.Mathlib.Probability.Independence.Kernel
info: ././././PFR/Mathlib/Probability/Independence/Kernel.lean:39:0-39:6: ProbabilityTheory.kernel.iIndep
Fun.comp.{u_9, u_8, u_7, u_6, u_5} {α : Type u_9} {Ω : Type u_8} {ι : Type u_7}
  {_mα : MeasurableSpace α} {_mΩ : MeasurableSpace Ω} {κ : ↥(kernel α Ω)} {μ : Measure α} {β : ι → Type u
_5}
  {γ : ι → Type u_6} {m : (i : ι) → MeasurableSpace (β i)} {mγ : (i : ι) → MeasurableSpace (γ i)}
  {f : (i : ι) → Ω → β i} (h : iIndepFun m f κ μ) (g : (i : ι) → β i → γ i) (hg : ∀ (i : ι), Measurable (
g i)) :
  iIndepFun mγ (fun i ↦ g i ∘ f i) κ μ
warning: ././././PFR/Mathlib/Probability/Independence/Kernel.lean:45:6-45:23: declaration uses 'sorry'
[1955/1983] Building LeanAPAP.Mathlib.Tactic.Positivity
warning: ././.lake/packages/LeanAPAP/././LeanAPAP/Mathlib/Tactic/Positivity.lean:18:6-18:15: declaration
uses 'sorry'
warning: ././.lake/packages/LeanAPAP/././LeanAPAP/Mathlib/Tactic/Positivity.lean:19:6-19:15: declaration
uses 'sorry'
warning: ././.lake/packages/LeanAPAP/././LeanAPAP/Mathlib/Tactic/Positivity.lean:20:6-20:15: declaration
uses 'sorry'
warning: ././.lake/packages/LeanAPAP/././LeanAPAP/Mathlib/Tactic/Positivity.lean:22:6-22:22: declaration
uses 'sorry'
warning: ././.lake/packages/LeanAPAP/././LeanAPAP/Mathlib/Tactic/Positivity.lean:23:6-23:22: declaration
uses 'sorry'
warning: ././.lake/packages/LeanAPAP/././LeanAPAP/Mathlib/Tactic/Positivity.lean:24:6-24:22: declaration
uses 'sorry'
warning: ././.lake/packages/LeanAPAP/././LeanAPAP/Mathlib/Tactic/Positivity.lean:25:6-25:22: declaration
uses 'sorry'
warning: ././.lake/packages/LeanAPAP/././LeanAPAP/Mathlib/Tactic/Positivity.lean:27:6-27:26: declaration
uses 'sorry'
warning: ././.lake/packages/LeanAPAP/././LeanAPAP/Mathlib/Tactic/Positivity.lean:28:6-28:26: declaration
uses 'sorry'
warning: ././.lake/packages/LeanAPAP/././LeanAPAP/Mathlib/Tactic/Positivity.lean:29:6-29:26: declaration
uses 'sorry'
warning: ././.lake/packages/LeanAPAP/././LeanAPAP/Mathlib/Tactic/Positivity.lean:30:6-30:26: declaration
uses 'sorry'
warning: ././.lake/packages/LeanAPAP/././LeanAPAP/Mathlib/Tactic/Positivity.lean:31:6-31:26: declaration
uses 'sorry'
warning: ././.lake/packages/LeanAPAP/././LeanAPAP/Mathlib/Tactic/Positivity.lean:32:6-32:26: declaration
uses 'sorry'
warning: ././.lake/packages/LeanAPAP/././LeanAPAP/Mathlib/Tactic/Positivity.lean:33:6-33:26: declaration
uses 'sorry'
[1974/1983] Building PFR.MoreRuzsaDist
warning: ././././PFR/MoreRuzsaDist.lean:549:6-549:30: declaration uses 'sorry'
warning: ././././PFR/MoreRuzsaDist.lean:761:6-761:21: declaration uses 'sorry'
warning: ././././PFR/MoreRuzsaDist.lean:766:6-766:22: declaration uses 'sorry'
warning: ././././PFR/MoreRuzsaDist.lean:770:6-770:23: declaration uses 'sorry'
warning: ././././PFR/MoreRuzsaDist.lean:777:6-777:23: declaration uses 'sorry'
warning: ././././PFR/MoreRuzsaDist.lean:782:6-782:24: declaration uses 'sorry'
warning: ././././PFR/MoreRuzsaDist.lean:788:6-788:25: declaration uses 'sorry'
warning: ././././PFR/MoreRuzsaDist.lean:793:6-793:24: declaration uses 'sorry'
warning: ././././PFR/MoreRuzsaDist.lean:798:6-798:23: declaration uses 'sorry'
warning: ././././PFR/MoreRuzsaDist.lean:827:6-827:25: declaration uses 'sorry'
warning: ././././PFR/MoreRuzsaDist.lean:835:6-835:30: declaration uses 'sorry'
warning: ././././PFR/MoreRuzsaDist.lean:844:6-844:30: declaration uses 'sorry'
warning: ././././PFR/MoreRuzsaDist.lean:848:6-848:31: declaration uses 'sorry'
warning: ././././PFR/MoreRuzsaDist.lean:857:6-857:29: declaration uses 'sorry'
[1975/1983] Building PFR.MultiTauFunctional
warning: ././././PFR/MultiTauFunctional.lean:52:6-52:25: declaration uses 'sorry'
warning: ././././PFR/MultiTauFunctional.lean:56:6-56:25: declaration uses 'sorry'
warning: ././././PFR/MultiTauFunctional.lean:59:6-59:25: declaration uses 'sorry'
warning: ././././PFR/MultiTauFunctional.lean:65:6-65:26: declaration uses 'sorry'
warning: ././././PFR/MultiTauFunctional.lean:69:6-69:30: declaration uses 'sorry'
warning: ././././PFR/MultiTauFunctional.lean:76:6-76:31: declaration uses 'sorry'
[1978/1983] Building PFR.Mathlib.Analysis.SpecialFunctions.NegMulLog
warning: ././././PFR/Mathlib/Analysis/SpecialFunctions/NegMulLog.lean:50:6-50:25: declaration uses 'sorry
'
warning: ././././PFR/Mathlib/Analysis/SpecialFunctions/NegMulLog.lean:54:6-54:28: declaration uses 'sorry
'
[1979/1983] Building PFR.kullback
warning: ././././PFR/kullback.lean:40:6-40:24: declaration uses 'sorry'
warning: ././././PFR/kullback.lean:44:6-44:19: declaration uses 'sorry'
warning: ././././PFR/kullback.lean:47:6-47:37: declaration uses 'sorry'
warning: ././././PFR/kullback.lean:52:6-52:22: declaration uses 'sorry'
warning: ././././PFR/kullback.lean:55:6-55:24: declaration uses 'sorry'
warning: ././././PFR/kullback.lean:59:6-59:35: declaration uses 'sorry'
warning: ././././PFR/kullback.lean:72:6-72:19: declaration uses 'sorry'
warning: ././././PFR/kullback.lean:76:6-76:23: declaration uses 'sorry'
[1980/1983] Building PFR.BoundingMutual
warning: ././././PFR/BoundingMutual.lean:28:6-28:27: declaration uses 'sorry'
[1981/1983] Building PFR.TorsionEndgame
warning: ././././PFR/TorsionEndgame.lean:37:6-37:22: declaration uses 'sorry'
warning: ././././PFR/TorsionEndgame.lean:41:6-41:32: declaration uses 'sorry'
warning: ././././PFR/TorsionEndgame.lean:43:6-43:32: declaration uses 'sorry'
warning: ././././PFR/TorsionEndgame.lean:45:6-45:32: declaration uses 'sorry'
warning: ././././PFR/TorsionEndgame.lean:49:6-49:21: declaration uses 'sorry'
warning: ././././PFR/TorsionEndgame.lean:52:6-52:25: declaration uses 'sorry'
warning: ././././PFR/TorsionEndgame.lean:55:6-55:26: declaration uses 'sorry'
warning: ././././PFR/TorsionEndgame.lean:58:6-58:36: declaration uses 'sorry'
warning: ././././PFR/TorsionEndgame.lean:70:6-70:22: declaration uses 'sorry'
warning: ././././PFR/TorsionEndgame.lean:73:6-73:15: declaration uses 'sorry'
warning: ././././PFR/TorsionEndgame.lean:80:6-80:22: declaration uses 'sorry'
warning: ././././PFR/TorsionEndgame.lean:92:6-92:32: declaration uses 'sorry'
warning: ././././PFR/TorsionEndgame.lean:101:8-101:19: declaration uses 'sorry'
[1982/1983] Building PFR.RhoFunctional
warning: ././././PFR/RhoFunctional.lean:34:6-34:22: declaration uses 'sorry'
warning: ././././PFR/RhoFunctional.lean:39:6-39:27: declaration uses 'sorry'
warning: ././././PFR/RhoFunctional.lean:43:6-43:26: declaration uses 'sorry'
warning: ././././PFR/RhoFunctional.lean:51:6-51:20: declaration uses 'sorry'
warning: ././././PFR/RhoFunctional.lean:56:6-56:21: declaration uses 'sorry'
warning: ././././PFR/RhoFunctional.lean:59:6-59:22: declaration uses 'sorry'
warning: ././././PFR/RhoFunctional.lean:63:6-63:20: declaration uses 'sorry'
warning: ././././PFR/RhoFunctional.lean:68:6-68:22: declaration uses 'sorry'
warning: ././././PFR/RhoFunctional.lean:73:6-73:21: declaration uses 'sorry'
warning: ././././PFR/RhoFunctional.lean:78:6-78:16: declaration uses 'sorry'
warning: ././././PFR/RhoFunctional.lean:90:6-90:26: declaration uses 'sorry'
warning: ././././PFR/RhoFunctional.lean:93:6-93:26: declaration uses 'sorry'
warning: ././././PFR/RhoFunctional.lean:96:6-96:22: declaration uses 'sorry'
warning: ././././PFR/RhoFunctional.lean:99:6-99:21: declaration uses 'sorry'
warning: ././././PFR/RhoFunctional.lean:102:6-102:16: declaration uses 'sorry'
warning: ././././PFR/RhoFunctional.lean:109:6-109:19: declaration uses 'sorry'
warning: ././././PFR/RhoFunctional.lean:113:6-113:23: declaration uses 'sorry'
warning: ././././PFR/RhoFunctional.lean:129:6-129:20: declaration uses 'sorry'
warning: ././././PFR/RhoFunctional.lean:138:6-138:14: declaration uses 'sorry'
warning: ././././PFR/RhoFunctional.lean:141:6-141:22: declaration uses 'sorry'
warning: ././././PFR/RhoFunctional.lean:144:6-144:14: declaration uses 'sorry'
warning: ././././PFR/RhoFunctional.lean:148:6-148:25: declaration uses 'sorry'
warning: ././././PFR/RhoFunctional.lean:153:6-153:26: declaration uses 'sorry'
warning: ././././PFR/RhoFunctional.lean:159:6-159:20: declaration uses 'sorry'
warning: ././././PFR/RhoFunctional.lean:167:6-167:21: declaration uses 'sorry'
warning: ././././PFR/RhoFunctional.lean:175:6-175:25: declaration uses 'sorry'
warning: ././././PFR/RhoFunctional.lean:181:8-181:26: declaration uses 'sorry'
warning: ././././PFR/RhoFunctional.lean:191:6-191:31: declaration uses 'sorry'
warning: ././././PFR/RhoFunctional.lean:197:6-197:27: declaration uses 'sorry'
2024-08-10 14:12:21.478 | DEBUG    | lean_dojo.utils:execute:110 - lean --print-prefix
2024-08-10 14:12:21.513 | DEBUG    | lean_dojo.utils:execute:110 - lean --version
2024-08-10 14:12:24.678 | DEBUG    | lean_dojo.data_extraction.trace:_trace:161 - lake env lean --threads
 32 --run ExtractData.lean
2024-08-10 14:12:24.679 | DEBUG    | lean_dojo.utils:execute:110 - lake env lean --threads 32 --run Extra
ctData.lean
100%|█████████████████████████████████████████████████████████████████▊| 2889/2896 [15:40<00:01,  6.29it/
s]2024-08-10 14:34:01.726 | DEBUG    | lean_dojo.utils:execute:110 - lake build Lean4Repl
2024-08-10 14:34:04.374 | DEBUG    | lean_dojo.utils:execute:110 - git remote get-url origin
2024-08-10 14:34:04.378 | DEBUG    | lean_dojo.utils:execute:110 - git log -n 1
2024-08-10 14:34:04.461 | DEBUG    | lean_dojo.data_extraction.traced_data:from_traced_files:1097 - Parsi
ng 2890 *.ast.json files in /tmp/tmpinjlk62b/pfr with 31 workers
2024-08-10 14:34:07,926 INFO worker.py:1772 -- Started a local Ray instance. View the dashboard at 127.0.
0.1:8267
  2%|█▍                                                                  | 59/2890 [00:04<03:04, 15.38it/
s](raylet) [2024-08-10 14:34:16,838 E 67183 67223] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_14-34-04_622526_4079540 is over 95% full, available space: 81096536064; capacity: 188750769766
4. Object creation will fail if spilling is required.
  6%|████▎                                                              | 185/2890 [00:14<04:49,  9.34it/
s](raylet) [2024-08-10 14:34:26,844 E 67183 67223] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_14-34-04_622526_4079540 is over 95% full, available space: 78500872192; capacity: 188750769766
4. Object creation will fail if spilling is required.
 12%|████████                                                           | 346/2890 [00:23<00:44, 57.23it/
s](raylet) [2024-08-10 14:34:36,937 E 67183 67223] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_14-34-04_622526_4079540 is over 95% full, available space: 78137798656; capacity: 188750769766
4. Object creation will fail if spilling is required.
 18%|███████████▊                                                       | 511/2890 [00:34<00:47, 49.89it/
s](raylet) [2024-08-10 14:34:46,944 E 67183 67223] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_14-34-04_622526_4079540 is over 95% full, available space: 77700857856; capacity: 188750769766
4. Object creation will fail if spilling is required.
 20%|█████████████▌                                                     | 585/2890 [00:42<00:55, 41.76it/
s](raylet) [2024-08-10 14:34:56,950 E 67183 67223] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_14-34-04_622526_4079540 is over 95% full, available space: 77700841472; capacity: 188750769766
4. Object creation will fail if spilling is required.
 24%|████████████████▏                                                  | 697/2890 [00:51<00:44, 48.74it/
s](raylet) [2024-08-10 14:35:06,957 E 67183 67223] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_14-34-04_622526_4079540 is over 95% full, available space: 77700804608; capacity: 188750769766
4. Object creation will fail if spilling is required.
 31%|████████████████████▋                                              | 895/2890 [01:16<14:13,  2.34it/
s](raylet) [2024-08-10 14:35:16,967 E 67183 67223] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_14-34-04_622526_4079540 is over 95% full, available space: 77700042752; capacity: 188750769766
4. Object creation will fail if spilling is required.
 37%|████████████████████████▋                                         | 1080/2890 [01:34<00:32, 55.22it/
s](raylet) [2024-08-10 14:35:36,981 E 67183 67223] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_14-34-04_622526_4079540 is over 95% full, available space: 77700005888; capacity: 188750769766
4. Object creation will fail if spilling is required. [repeated 2x across cluster] (Ray deduplicates logs
 by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-
observability/user-guides/configure-logging.html#log-deduplication for more options.)
 46%|██████████████████████████████▏                                   | 1323/2890 [01:56<18:32,  1.41it/
s](raylet) [2024-08-10 14:35:57,000 E 67183 67223] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_14-34-04_622526_4079540 is over 95% full, available space: 77699964928; capacity: 188750769766
4. Object creation will fail if spilling is required. [repeated 2x across cluster]
 55%|████████████████████████████████████▏                             | 1587/2890 [02:23<00:20, 64.09it/
s](raylet) [2024-08-10 14:36:17,018 E 67183 67223] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_14-34-04_622526_4079540 is over 95% full, available space: 77698818048; capacity: 188750769766
4. Object creation will fail if spilling is required. [repeated 2x across cluster]
 69%|█████████████████████████████████████████████▏                    | 1981/2890 [02:57<00:15, 60.31it/
s](raylet) [2024-08-10 14:36:47,043 E 67183 67223] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_14-34-04_622526_4079540 is over 95% full, available space: 77698789376; capacity: 188750769766
4. Object creation will fail if spilling is required. [repeated 3x across cluster]
 83%|██████████████████████████████████████████████████████▋           | 2393/2890 [03:37<10:34,  1.28s/i
t](raylet) [2024-08-10 14:37:17,069 E 67183 67223] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_14-34-04_622526_4079540 is over 95% full, available space: 77697101824; capacity: 188750769766
4. Object creation will fail if spilling is required. [repeated 3x across cluster]
 96%|███████████████████████████████████████████████████████████████▌  | 2782/2890 [03:44<00:01, 68.96it/
s](raylet) [2024-08-10 14:37:57,092 E 67183 67223] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_14-34-04_622526_4079540 is over 95% full, available space: 77697060864; capacity: 188750769766
4. Object creation will fail if spilling is required. [repeated 4x across cluster]
 98%|████████████████████████████████████████████████████████████████▊ | 2839/2890 [04:27<01:49,  2.15s/i
t](raylet) [2024-08-10 14:38:07,097 E 67183 67223] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_14-34-04_622526_4079540 is over 95% full, available space: 77697028096; capacity: 188750769766
4. Object creation will fail if spilling is required.
(raylet) [2024-08-10 14:38:17,100 E 67183 67223] (raylet) file_system_monitor.cc:111: /tmp/ray/session_20
24-08-10_14-34-04_622526_4079540 is over 95% full, available space: 77696233472; capacity: 1887507697664.
 Object creation will fail if spilling is required.
100%|██████████████████████████████████████████████████████████████████| 2890/2890 [04:31<00:00, 10.65it/
s]
(raylet) [2024-08-10 14:38:37,107 E 67183 67223] (raylet) file_system_monitor.cc:111: /tmp/ray/session_20
24-08-10_14-34-04_622526_4079540 is over 95% full, available space: 77696217088; capacity: 1887507697664.
 Object creation will fail if spilling is required. [repeated 2x across cluster]
2024-08-10 14:38:45.133 | DEBUG    | lean_dojo.data_extraction.lean:get_dependencies:489 - Querying the d
ependencies of LeanGitRepo(url='https://github.com/teorth/pfr', commit='6a5082ee465f9e44cea479c7b741b3163
162bb7e')
2024-08-10 14:38:46.585 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the com
mit hash for lean4 v4.6.0-rc1
2024-08-10 14:38:48.739 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the com
mit hash for lean4 v4.7.0
2024-08-10 14:38:49.852 | DEBUG    | lean_dojo.data_extraction.lean:url_to_repo:68 - url_to_repo("https:/
/github.com/leanprover-community/import-graph.git") failed. Retrying...
2024-08-10 14:38:50.989 | DEBUG    | lean_dojo.data_extraction.lean:url_to_repo:68 - url_to_repo("https:/
/github.com/leanprover-community/import-graph.git") failed. Retrying...
2024-08-10 14:38:54.134 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the com
mit hash for lean4 v4.10.0
2024-08-10 14:38:55.263 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the com
mit hash for doc-gen4 main
2024-08-10 14:38:55.903 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the com
mit hash for lean4 v4.11.0-rc1
2024-08-10 14:38:56.507 | DEBUG    | lean_dojo.data_extraction.lean:get_dependencies:489 - Querying the d
ependencies of LeanGitRepo(url='https://github.com/YaelDillies/LeanAPAP', commit='f47447e44d8e82ab214ed8c
1b199329141fc5b1f')
2024-08-10 14:38:57.064 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the com
mit hash for lean4 v4.9.0-rc1
2024-08-10 14:38:57.995 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the com
mit hash for lean4 v4.10.0-rc1
2024-08-10 14:38:58.694 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the com
mit hash for lean4 v4.9.0
2024-08-10 14:39:00.235 | DEBUG    | lean_dojo.data_extraction.lean:url_to_repo:68 - url_to_repo("https:/
/github.com/leanprover-community/mathlib4.git") failed. Retrying...
2024-08-10 14:39:01.355 | DEBUG    | lean_dojo.data_extraction.lean:url_to_repo:68 - url_to_repo("https:/
/github.com/leanprover-community/mathlib4.git") failed. Retrying...
2024-08-10 14:39:04.602 | DEBUG    | lean_dojo.data_extraction.lean:get_dependencies:489 - Querying the d
ependencies of LeanGitRepo(url='https://github.com/leanprover-community/mathlib4', commit='0077925cbead60
622b55b430eb5109f7365954c8')
2024-08-10 14:39:05.780 | DEBUG    | lean_dojo.data_extraction.lean:get_dependencies:489 - Querying the d
ependencies of LeanGitRepo(url='https://github.com/leanprover/doc-gen4', commit='6d8e3118ab526f8dfcabcbdf
9f05dc34e5c423a8')
2024-08-10 14:39:06.784 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the com
mit hash for lean4 v4.10.0-rc2
Following Github server redirection from /repos/mhuisi/lean4-cli to /repositories/341363356
2024-08-10 14:39:08.607 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the com
mit hash for lean4 nightly-2024-06-05
2024-08-10 14:39:09.896 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the com
mit hash for md4lean main
2024-08-10 14:39:10.052 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the com
mit hash for BibtexQuery master
2024-08-10 14:39:10.477 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the com
mit hash for lean4-unicode-basic main
2024-08-10 14:39:11.124 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the com
mit hash for lean4-cli nightly
2024-08-10 14:39:11.315 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the com
mit hash for lean4 nightly-2024-06-05
2024-08-10 14:39:13.593 | INFO     | generate_benchmark_lean4:main:346 - Failed to trace repo LeanGitRepo
(url='https://github.com/teorth/pfr', commit='6a5082ee465f9e44cea479c7b741b3163162bb7e') because of Inval
id tag or branch: `nightly-2024-06-05` for Repository(full_name="leanprover/lean4")
2024-08-10 14:39:22.378 | INFO     | __main__:generate_dataset:246 - Finished generating benchmark at /ra
id/adarsh/datasets_test/pfr_6a5082ee465f9e44cea479c7b741b3163162bb7e
2024-08-10 14:39:22.380 | INFO     | __main__:generate_dataset:247 - Merging datasets
2024-08-10 14:39:22.381 | INFO     | __main__:merge_datasets:74 - Merging datasets for random
2024-08-10 14:39:22.381 | INFO     | __main__:merge_datasets:80 - Processing train split
2024-08-10 14:39:22.381 | INFO     | __main__:merge_datasets:104 - Finished processing train split
2024-08-10 14:39:22.382 | INFO     | __main__:merge_datasets:80 - Processing val split
2024-08-10 14:39:22.382 | INFO     | __main__:merge_datasets:104 - Finished processing val split
2024-08-10 14:39:22.382 | INFO     | __main__:merge_datasets:80 - Processing test split
2024-08-10 14:39:22.382 | INFO     | __main__:merge_datasets:104 - Finished processing test split
2024-08-10 14:39:22.382 | INFO     | __main__:merge_datasets:105 - Finished merging datasets for random
2024-08-10 14:39:22.382 | INFO     | __main__:merge_datasets:74 - Merging datasets for novel_premises
2024-08-10 14:39:22.382 | INFO     | __main__:merge_datasets:80 - Processing train split
2024-08-10 14:39:22.383 | INFO     | __main__:merge_datasets:104 - Finished processing train split
2024-08-10 14:39:22.383 | INFO     | __main__:merge_datasets:80 - Processing val split
2024-08-10 14:39:22.383 | INFO     | __main__:merge_datasets:104 - Finished processing val split
2024-08-10 14:39:22.383 | INFO     | __main__:merge_datasets:80 - Processing test split
2024-08-10 14:39:22.383 | INFO     | __main__:merge_datasets:104 - Finished processing test split
2024-08-10 14:39:22.383 | INFO     | __main__:merge_datasets:105 - Finished merging datasets for novel_pr
emises
2024-08-10 14:39:22.383 | INFO     | __main__:merge_datasets:107 - Merging corpus
2024-08-10 14:39:22.383 | INFO     | __main__:merge_datasets:130 - Finished merging corpus
2024-08-10 14:39:22.383 | INFO     | __main__:merge_datasets:132 - Adding metadata
2024-08-10 14:39:22.383 | WARNING  | __main__:merge_datasets:152 - No metadata file found
2024-08-10 14:39:22.384 | INFO     | __main__:merge_datasets:154 - Deleting individual datasets
2024-08-10 14:39:22.384 | INFO     | __main__:generate_dataset:249 - Finished merging datasets
2024-08-10 14:39:22.562 | INFO     | __main__:main:639 - Latest PL checkpoint found: AK123321/pl-leancopi
lot-2
2024-08-10 14:39:22.666 | INFO     | __main__:download_pl_checkpoint:274 - Checkpoint downloaded to: /hom
e/adarsh/.cache/huggingface/hub/models--AK123321--pl-leancopilot-2/snapshots/d4955ed1972ad84c03dce9189162
d03912f2d86f/model.ckpt
2024-08-10 14:39:22.666 | INFO     | __main__:main:642 - Checkpoint path: /home/adarsh/.cache/huggingface
/hub/models--AK123321--pl-leancopilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86f/model.ckpt
2024-08-10 14:39:22.666 | INFO     | __main__:train:464 - Training model with checkpoint: /home/adarsh/.c
ache/huggingface/hub/models--AK123321--pl-leancopilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86
f/model.ckpt
Seed set to 3407
Lightning automatically upgraded your loaded checkpoint from v0.0.0 to v2.4.0. To apply the upgrade to yo
ur files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../.cache/huggingface
/hub/models--AK123321--pl-leancopilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86f/model.ckpt`
2024-08-10 14:39:46.313 | INFO     | __main__:train:486 - Loaded premise retriever at /home/adarsh/.cache
/huggingface/hub/models--AK123321--pl-leancopilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86f/mo
del.ckpt
2024-08-10 14:39:46.313 | INFO     | __main__:train:490 - Data path: /raid/adarsh/datasets_test/merged/ra
ndom
/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/transformers/tokenization
_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True`
by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by def
ault. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
2024-08-10 14:39:46.412 | INFO     | common:__init__:199 - Building the corpus from /raid/adarsh/datasets
_test/merged/corpus.jsonl
2024-08-10 14:39:47.881 | INFO     | retrieval.datamodule:load_or_cache_data:52 - Loaded data from cache
/raid/adarsh/datasets_test/merged/random/cache_train/cached_data.pkl
Training dataset size: 353052
2024-08-10 14:39:47.916 | INFO     | retrieval.datamodule:load_or_cache_data:52 - Loaded data from cache
/raid/adarsh/datasets_test/merged/random/cache_val/cached_data.pkl
Validation dataset size: 5403
2024-08-10 14:39:47.949 | INFO     | retrieval.datamodule:load_or_cache_data:52 - Loaded data from cache
/raid/adarsh/datasets_test/merged/random/cache_pred/cached_data.pkl
Testing dataset size: 5321
2024-08-10 14:39:47.951 | INFO     | __main__:train:523 - Training dataset size after load: 353052
2024-08-10 14:39:47.951 | INFO     | __main__:train:524 - Validation dataset size after load: 5403
2024-08-10 14:39:47.951 | INFO     | __main__:train:525 - Testing dataset size after load: 5321
Using bfloat16 Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
2024-08-10 14:39:47.962 | INFO     | __main__:train:543 - Starting progressive training...
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 1 processes
----------------------------------------------------------------------------------------------------

2024-08-10 14:40:04.894 | INFO     | retrieval.datamodule:load_or_cache_data:52 - Loaded data from cache
/raid/adarsh/datasets_test/merged/random/cache_train/cached_data.pkl
Training dataset size: 353052
2024-08-10 14:40:05.132 | INFO     | retrieval.datamodule:load_or_cache_data:52 - Loaded data from cache
/raid/adarsh/datasets_test/merged/random/cache_val/cached_data.pkl
Validation dataset size: 5403
2024-08-10 14:40:05.171 | INFO     | retrieval.datamodule:load_or_cache_data:52 - Loaded data from cache
/raid/adarsh/datasets_test/merged/random/cache_pred/cached_data.pkl
Testing dataset size: 5321
/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning/callbac
ks/model_checkpoint.py:654: Checkpoint directory /raid/adarsh/checkpoints_test exists and is not empty.
Restoring states from the checkpoint path at /home/adarsh/.cache/huggingface/hub/models--AK123321--pl-lea
ncopilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86f/model.ckpt
Lightning automatically upgraded your loaded checkpoint from v0.0.0 to v2.4.0. To apply the upgrade to yo
ur files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../.cache/huggingface
/hub/models--AK123321--pl-leancopilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86f/model.ckpt`
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
2024-08-10 14:40:06.069 | INFO     | common:get_optimizers:432 - Optimizing with AdamW

  | Name    | Type           | Params | Mode
---------------------------------------------------
0 | encoder | T5EncoderModel | 217 M  | train
---------------------------------------------------
217 M     Trainable params
0         Non-trainable params
217 M     Total params
870.630   Total estimated model params size (MB)
235       Modules in train mode
0         Modules in eval mode
Restored all states from the checkpoint at /home/adarsh/.cache/huggingface/hub/models--AK123321--pl-leanc
opilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86f/model.ckpt
Epoch 0:   0%|                                                                  | 0/176526 [00:00<?, ?it/
s]2024-08-10 14:40:09.683 | INFO     | __main__:main:651 - An error occurred: Caught KeyError in DataLoad
er worker process 0.
Original Traceback (most recent call last):
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/torch/utils/data/
_utils/worker.py", line 309, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/torch/utils/data/
_utils/fetch.py", line 52, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/torch/utils/data/
_utils/fetch.py", line 52, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/adarsh/ReProver/retrieval/datamodule.py", line 123, in __getitem__
    for p in self.corpus.get_premises(ex["context"].path):
  File "/home/adarsh/ReProver/common.py", line 251, in get_premises
    return self._get_file(path).premises
  File "/home/adarsh/ReProver/common.py", line 226, in _get_file
    return self.transitive_dep_graph.nodes[path]["file"]
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/networkx/classes/
reportviews.py", line 194, in __getitem__
    return self._nodes[n]
KeyError: '.lake/packages/mathlib/Mathlib/Data/Ordmap/Ordset.lean'

Traceback (most recent call last):
  File "/home/adarsh/ReProver/compute_server.py", line 645, in main
    train(model_checkpoint_path, merged_data_path, next_suffix)
  File "/home/adarsh/ReProver/compute_server.py", line 545, in train
    trainer.fit(model, datamodule=data_module, ckpt_path=model_checkpoint_path)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/trainer/call.py", line 46, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/trainer/trainer.py", line 1025, in _run_stage
    self.fit_loop.run()
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/loops/training_epoch_loop.py", line 140, in run
    self.advance(data_fetcher)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/loops/training_epoch_loop.py", line 212, in advance
    batch, _, __ = next(data_fetcher)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/utilities/combined_loader.py", line 78, in __next__
    out[i] = next(self.iterators[i])
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/torch/utils/data/
dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/torch/utils/data/
dataloader.py", line 1344, in _next_data
    return self._process_data(data)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/torch/utils/data/
dataloader.py", line 1370, in _process_data
    data.reraise()
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/torch/_utils.py",
 line 706, in reraise
    raise exception
KeyError: Caught KeyError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/torch/utils/data/
_utils/worker.py", line 309, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/torch/utils/data/
_utils/fetch.py", line 52, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/torch/utils/data/
_utils/fetch.py", line 52, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/adarsh/ReProver/retrieval/datamodule.py", line 123, in __getitem__
    for p in self.corpus.get_premises(ex["context"].path):
  File "/home/adarsh/ReProver/common.py", line 251, in get_premises
    return self._get_file(path).premises
  File "/home/adarsh/ReProver/common.py", line 226, in _get_file
    return self.transitive_dep_graph.nodes[path]["file"]
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/networkx/classes/
reportviews.py", line 194, in __getitem__
    return self._nodes[n]
KeyError: '.lake/packages/mathlib/Mathlib/Data/Ordmap/Ordset.lean'

Epoch 0:   0%|                                                                  | 0/176526 [00:05<?, ?it/
s]
(base) adarsh@tensorlab-DGX-Station-A100-920-23487-2531-000:~/ReProver$
