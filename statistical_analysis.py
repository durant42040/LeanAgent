# Data for Experiment 3
data_exp3 = {
    'Repository': [
        'Compfiles', 'Mathematics in Lean Source', 'PrimeNumberTheoremAnd', 'Math Workshop',
        'FLT', 'PFR', 'SciLean', 'Debate', 'Matrix Cookbook', 'Con-nf',
        'Foundation', 'Saturn', 'LeanEuclid', 'Lean4Lean'
    ],
    'Validation R@10 Exp3': [61.55, 67.99, 62.11, 64.44, 69.52, 63.13, 69.13, 70.35, 69.23, 58.69, 67.33, 77.12, 75.33, 74.85],
    'Average Test R@10 Exp3': [60.65, 63.41, 62.55, 63.13, 64.13, 64.18, 65.39, 65.93, 66.85, 66.79, 66.67, 66.85, 68.39, 69.18],
    'task1 Test R@10: Compfiles': [
        60.64706363237122, 61.32526532789667, 61.6708414949816, 61.08772811083543, 60.261503729020575, 60.30689658891723, 60.60123583745497, 
        60.153335213407054, 59.7709911492489, 60.00540189142794, 59.91546810620901,  59.88792078948343, 59.64851162308727, 59.420134063412014],
    'task2 Test R@10: Mathematics in Lean Source': [
        65.48591635672048, 65.75192142681436, 66.4970212474497, 66.49549432082041, 68.10014741519532, 67.93505866948891, 67.88206930569434, 
        67.97385677238191, 67.61899831570183, 67.69791601581457, 67.581090432529, 67.75405538565178, 67.55591710838813],
    'task3 Test R@10: PrimeNumberTheoremAnd': [
        60.24074464461104, 61.601678946721826, 61.53315591144687, 62.06460041347961, 61.59168435062894, 62.286496517728665, 63.17427181726865, 
        62.95367306725815, 62.84812634406436, 62.796309821659605, 62.573145480848204, 62.1576880065672],
    'task4 Test R@10: Math Workshop': [
        63.32418885891475, 65.32513361607977, 64.5102791379126, 67.1694072669325, 66.1893594713575, 67.5335852605448, 68.069787398159,
        68.17676215211705, 67.79894096460394, 68.11922176655432, 68.37256090708857],
    'task5 Test R@10: FLT': [
        67.05795410771319, 67.50657961559557, 68.07826197730941, 68.94304048302321, 69.25704440208798, 69.06493687974346, 69.13220145373448, 
        69.16048162503891, 69.47002065174642, 69.48301090016267],
    'task6 Test R@10: PFR': [
        62.58480312148852, 64.14710648979965, 64.12721536773596, 63.79502213074667, 63.97614939811086, 63.862150608153875, 63.66221697616451,
        64.11137631469667, 63.98214095462508],
    'task7 Test R@10: SciLean': [
        68.18567421512381, 68.94310521480176, 70.20595528030704, 70.11859835977681, 70.07787376617783, 70.21904098724055, 69.74478926670498,
        69.6667197138306],
    'task8 Test R@10: Debate': [68.91867342976552, 68.8826460829178, 69.18231380479203, 69.31982755616717 , 69.58427923681404, 69.13118545804842, 69.48585868047269],
    'task9 Test R@10: Matrix Cookbook': [71.04190539789398, 71.1094077126274, 71.08151370983799,  71.44692514637913, 71.19988426372417, 71.55403847771102],
    'task10 Test R@10: Con-nf': [65.80145354021758, 66.0667469234885, 64.94783306581058, 66.0404405207776, 65.51765650080257 ],
    'task11 Test R@10: Foundation': [65.21872588006309,  64.71692632303098 , 65.3519623141716, 65.13856379408705 ],
    'task12 Test R@10: Saturn': [70.44871794871794, 73.2396449704142, 71.61242603550296 ],
    'task13 Test R@10: LeanEuclid': [82.70537124802527, 83.11611374407583],
    'task14 Test R@10: Lean4Lean': [81.45833333333331],
}

# Data for Experiment 8
data_exp8 = {
    'Repository': [
        'Compfiles', 'Mathematics in Lean Source', 'PrimeNumberTheoremAnd', 'Math Workshop',
        'FLT', 'PFR', 'SciLean', 'Debate', 'Matrix Cookbook', 'Con-nf',
        'Foundation', 'Saturn', 'LeanEuclid', 'Lean4Lean'
    ],
    'Validation R@10 Exp8': [62.79, 68.67, 61.33, 66.67, 65.92, 64.63, 68.99, 68.7, 72.74, 60.18, 64.22, 85.74, 81.55, 76.62],
    'Average Test R@10 Exp8': [58.75, 63.04, 63.23, 63.54, 65.08, 64.35, 65.39, 66.12, 67.13, 67.23, 67.15, 68.99, 68.8, 69.46],
    'task1 Test R@10: Compfiles': [
        58.753674683076795, 59.266322324139544, 59.11501222901359, 58.65061069597731, 59.08351152440277, 58.254012190948366, 58.31714930746805, 
        58.16705172694482, 58.2052292761354, 57.81466274231215, 57.931586927587, 58.19557776263805, 57.83569946245733, 58.135263114918466
    ],
    'task2 Test R@10: Mathematics in Lean Source': [
        66.82241247037165, 68.22394378997019, 68.68426125028768, 68.46477136753248, 68.52621401540969, 69.0316174088683, 69.85560219203676,
        71.13058457926404, 71.08407817561478, 70.98203735928827, 71.3442495324248, 70.89091325285804, 70.812148413589
    ],
    'task3 Test R@10: PrimeNumberTheoremAnd': [
        62.37342972083976, 63.14389800117126, 63.57248915109329, 62.72338659851425, 63.358305722154476, 64.10691753468002, 63.84201634681258,
        64.27687840450473, 64.27556330718753, 64.32555466833091, 64.09342765687724, 64.9165197040928
    ],
    'task4 Test R@10: Math Workshop': [
        63.692126983903755, 64.88112322603578, 63.9572265456425, 64.24406215475595, 65.50233141990883, 65.34918568101253, 65.3123566360483,
        65.3153365514187, 65.78415399457909, 64.77471389023222, 65.22434587068398
    ],
    'task5 Test R@10: FLT': [
        69.4153115431529, 69.43774305709876, 70.20327098838466, 70.4426156383421, 71.33675063495751, 71.06544237274925, 70.90625206477227,
        71.0986096866722, 71.13206954581167, 70.97284724070164
    ],
    'task6 Test R@10: PFR': [
        63.22184229942324, 63.58756427174265, 63.68275102010593, 63.94294965538092, 63.77168689248713, 63.62856794668035, 63.94054979500917,
        63.14677779628911, 62.79802453366899
    ],
    'task7 Test R@10: SciLean': [
        68.99506046579262, 68.18453403141949, 68.60493009000173, 69.21270209449912, 68.94233134861383, 69.55466069440035, 69.0684458531042,
        69.1317642961663
    ],
    'task8 Test R@10: Debate': [
        68.90809008062388, 70.17778949140212, 69.6991246532138, 69.75589568301386, 69.9558657259099, 69.73137122548178, 69.76256041434732 
    ],
    'task9 Test R@10: Matrix Cookbook': [
        71.60183467326324, 71.1446252160538, 70.97734804877662, 71.58040636612066, 71.22861530004387, 71.39100846243703
    ],
    'task10 Test R@10: Con-nf': [
        68.94028103044498, 68.89075917252147, 69.10787470725997, 67.92715651834504, 68.53508001561279
    ],
    'task11 Test R@10: Foundation': [
        67.02251895209642, 67.00373960937341, 66.70270105481373, 67.33863788793366
    ],
    'task12 Test R@10: Saturn': [85.92970521541949, 84.79591836734693, 86.49659863945578],
    'task13 Test R@10: LeanEuclid': [73.07777777777778, 73.46666666666667],
    'task14 Test R@10: Lean4Lean': [73.47417840375586],
}

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats

# Prepare data
repos = data_exp3['Repository']
n_repos = len(repos)

# Helper function to calculate metrics
def calculate_metrics(data, exp_name):
    metrics = {}
    
    # 1. Final Average Test R@10
    metrics['Final_Avg_Test_R@10'] = data[f'Average Test R@10 {exp_name}'][-1]
    
    # 2. Area Under the Learning Curve (AULC)
    aulc = np.trapz(data[f'Average Test R@10 {exp_name}']) / n_repos
    metrics['AULC'] = aulc
    
    # # 3. Forgetting Measure
    # forgetting = []
    # for i in range(2, n_repos + 1):
    #     task_performance = [data[f'task{j} Test R@10: {repos[j-1]}'][i-j-1] for j in range(1, i)]
    #     forgetting.append(np.max(task_performance) - data[f'task{i-1} Test R@10: {repos[i-2]}'][-1])
    # metrics['Avg_Forgetting'] = np.mean(forgetting)
    
    # 4. Forward Transfer
    forward_transfer = []
    for i in range(2, n_repos + 1):
        forward_transfer.append(data[f'task{i} Test R@10: {repos[i-1]}'][0] - data[f'task1 Test R@10: {repos[0]}'][0])
    metrics['Avg_Forward_Transfer'] = np.mean(forward_transfer)
    
    # 5. Backward Transfer
    backward_transfer = []
    for i in range(1, n_repos):
        backward_transfer.append(data[f'task{i} Test R@10: {repos[i-1]}'][-1] - data[f'task{i} Test R@10: {repos[i-1]}'][0])
    metrics['Avg_Backward_Transfer'] = np.mean(backward_transfer)
    
    # 6. Stability (using Average Test R@10)
    stability = np.mean(np.diff(data[f'Average Test R@10 {exp_name}']))
    metrics['Stability'] = stability
    
    # 7. Plasticity (using Validation R@10)
    plasticity = np.mean(np.diff(data[f'Validation R@10 {exp_name}']))
    metrics['Plasticity'] = plasticity
    
    return metrics

# Calculate metrics for both experiments
metrics_exp3 = calculate_metrics(data_exp3, 'Exp3')
metrics_exp8 = calculate_metrics(data_exp8, 'Exp8')

# Compare metrics
comparison = pd.DataFrame({
    'Metric': metrics_exp3.keys(),
    'Experiment 3': metrics_exp3.values(),
    'Experiment 8': metrics_exp8.values(),
    'Difference (Exp3 - Exp8)': [metrics_exp3[k] - metrics_exp8[k] for k in metrics_exp3.keys()]
})

print(comparison)

# Visualize learning curves
plt.figure(figsize=(12, 6))
plt.plot(data_exp3['Average Test R@10 Exp3'], label='Experiment 3')
plt.plot(data_exp8['Average Test R@10 Exp8'], label='Experiment 8')
plt.xlabel('Repository')
plt.ylabel('Average Test R@10')
plt.title('Learning Curves: Experiment 3 vs Experiment 8')
plt.legend()
plt.grid(True)
plt.show()

# Visualize validation R@10
plt.figure(figsize=(12, 6))
plt.plot(data_exp3['Validation R@10 Exp3'], label='Experiment 3')
plt.plot(data_exp8['Validation R@10 Exp8'], label='Experiment 8')
plt.xlabel('Repository')
plt.ylabel('Validation R@10')
plt.title('Validation R@10: Experiment 3 vs Experiment 8')
plt.legend()
plt.grid(True)
plt.show()

def calculate_additional_metrics(data, exp_name):
    metrics = {}
    
    # 1. Average validation R@10 performance
    metrics['Avg_Validation_R@10'] = np.mean(data[f'Validation R@10 {exp_name}'])
    
    # 2. Average Accuracy (AA)
    metrics['AA'] = np.mean(data[f'Average Test R@10 {exp_name}'])
    
    # 3. Forgetting Measure (FM)
    fm_values = []
    for i in range(2, len(data['Repository']) + 1):
        task_performances = [data[f'task{j} Test R@10: {data["Repository"][j-1]}'][i-j-1] for j in range(1, i)]
        fm_values.append(np.max(task_performances) - data[f'task{i-1} Test R@10: {data["Repository"][i-2]}'][-1])
    metrics['FM'] = np.mean(fm_values)
    
    # 4. Incremental Plasticity (IP)
    ip_values = np.diff(data[f'Validation R@10 {exp_name}'])
    metrics['IP'] = np.mean(ip_values)
    
    # 5. Stability to Plasticity Ratio (SPR)
    metrics['SPR'] = np.mean(data[f'Average Test R@10 {exp_name}']) / np.mean(data[f'Validation R@10 {exp_name}'])
    
    # 6. Rate of Change in Test R@10 (RCT)
    rct_values = np.diff(data[f'Average Test R@10 {exp_name}'])
    metrics['RCT'] = np.mean(rct_values)
    
    # 7. Cumulative Progress (CP)
    metrics['CP'] = np.sum(np.diff(data[f'Average Test R@10 {exp_name}']))
    
    # 8. Average Incremental Accuracy (AIA)
    aia_values = [np.mean(data[f'Average Test R@10 {exp_name}'][:i+1]) for i in range(len(data['Repository']))]
    metrics['AIA'] = np.mean(aia_values)
    
    # 9. Remembering (REM) and Positive Backward Transfer (BWT+)
    bwt_values = []
    for i in range(1, len(data['Repository'])):
        bwt_values.append(data[f'task{i} Test R@10: {data["Repository"][i-1]}'][-1] - data[f'task{i} Test R@10: {data["Repository"][i-1]}'][0])
    metrics['REM'] = 1 - abs(min(0, np.mean(bwt_values)))
    metrics['BWT+'] = max(0, np.mean(bwt_values))
    
    # 10. Time-Weighted Cumulative Performance (TWCP)
    weights = np.arange(1, len(data['Repository']) + 1)
    metrics['TWCP'] = np.sum(weights * data[f'Average Test R@10 {exp_name}']) / np.sum(weights)
    
    # 11. Stability-Plasticity Score (SPS)
    stability = 1 - (np.std(data[f'Average Test R@10 {exp_name}']) / np.mean(data[f'Average Test R@10 {exp_name}']))
    plasticity = np.mean(np.diff(data[f'Validation R@10 {exp_name}']))
    beta = 0.5  # Assuming equal weight to stability and plasticity
    metrics['SPS'] = beta * stability + (1 - beta) * plasticity
    
    # 12. Catastrophic Forgetting Resilience (CFR)
    metrics['CFR'] = np.min(data[f'Average Test R@10 {exp_name}']) / np.max(data[f'Average Test R@10 {exp_name}'])
    
    # # 13. Progressive Learning Index (PLI)
    # pli_values = np.maximum(0, np.diff(data[f'Average Test R@10 {exp_name}']))
    # metrics['PLI'] = np.sum(pli_values) / len(pli_values)
    
    # return metrics

# Calculate additional metrics for both experiments
additional_metrics_exp3 = calculate_additional_metrics(data_exp3, 'Exp3')
additional_metrics_exp8 = calculate_additional_metrics(data_exp8, 'Exp8')

# Compare additional metrics
comparison = pd.DataFrame({
    'Metric': additional_metrics_exp3.keys(),
    'Experiment 3': additional_metrics_exp3.values(),
    'Experiment 8': additional_metrics_exp8.values(),
    'Difference (Exp3 - Exp8)': [additional_metrics_exp3[k] - additional_metrics_exp8[k] for k in additional_metrics_exp3.keys()]
})

print(comparison)

# import numpy as np
# import pandas as pd
# from scipy import stats

# # Create DataFrames
# df3 = pd.DataFrame(data_exp3)
# df8 = pd.DataFrame(data_exp8)

# # Combine the dataframes
# df = pd.merge(df3, df8, on='Repository', suffixes=('_exp3', '_exp8'))

# def calculate_metrics(df):
#     metrics = {}
    
#     # 1. Average Performance (Higher is better)
#     metrics['avg_val_performance_exp3'] = df['Validation R@10 Exp3'].mean()
#     metrics['avg_val_performance_exp8'] = df['Validation R@10 Exp8'].mean()
#     metrics['avg_test_performance_exp3'] = df['Average Test R@10 Exp3'].mean()
#     metrics['avg_test_performance_exp8'] = df['Average Test R@10 Exp8'].mean()
    
#     # 2. Forgetting Measure (Lower is better)
#     # Assuming the order of repositories represents the learning sequence
#     metrics['forgetting_exp3'] = df['Average Test R@10 Exp3'].iloc[0] - df['Average Test R@10 Exp3'].iloc[-1]
#     metrics['forgetting_exp8'] = df['Average Test R@10 Exp8'].iloc[0] - df['Average Test R@10 Exp8'].iloc[-1]

#     def calculate_forgetting(series):
#         max_performance = np.maximum.accumulate(series)
#         forgetting = np.mean(max_performance - series)
#         return forgetting
    
#     metrics['new_forgetting_exp3'] = calculate_forgetting(df['Average Test R@10 Exp3'])
#     metrics['new_forgetting_exp8'] = calculate_forgetting(df['Average Test R@10 Exp8'])
    
#     # 3. Learning Curve Area (Higher is better)
#     metrics['learning_curve_area_exp3'] = np.trapz(df['Average Test R@10 Exp3'])
#     metrics['learning_curve_area_exp8'] = np.trapz(df['Average Test R@10 Exp8'])
    
#     # 4. Stability (Higher is better)
#     metrics['stability_exp3'] = 1 / df['Average Test R@10 Exp3'].std()
#     metrics['stability_exp8'] = 1 / df['Average Test R@10 Exp8'].std()
    
#     # 5. Plasticity (Higher is better)
#     metrics['plasticity_exp3'] = df['Validation R@10 Exp3'].mean()
#     metrics['plasticity_exp8'] = df['Validation R@10 Exp8'].mean()
    
#     # 6. Plasticity-Stability Balance (Higher is better)
#     metrics['plasticity_stability_balance_exp3'] = metrics['plasticity_exp3'] * metrics['stability_exp3']
#     metrics['plasticity_stability_balance_exp8'] = metrics['plasticity_exp8'] * metrics['stability_exp8']
    
#     # 7. Incremental Learning Score (Higher is better)
#     def incremental_learning_score(series):
#         return np.sum(np.diff(series) > 0) / (len(series) - 1)
    
#     metrics['incremental_learning_exp3'] = incremental_learning_score(df['Average Test R@10 Exp3'])
#     metrics['incremental_learning_exp8'] = incremental_learning_score(df['Average Test R@10 Exp8'])
    
#     # 8. Catastrophic Forgetting Resistance (Higher is better)
#     def catastrophic_forgetting_resistance(series):
#         diffs = np.diff(series)
#         return 1 - (np.sum(diffs[diffs < 0]) / np.sum(np.abs(diffs)))
    
#     metrics['catastrophic_forgetting_resistance_exp3'] = catastrophic_forgetting_resistance(df['Average Test R@10 Exp3'])
#     metrics['catastrophic_forgetting_resistance_exp8'] = catastrophic_forgetting_resistance(df['Average Test R@10 Exp8'])
    
#     # Helper function to calculate metrics for both experiments
#     def calc_for_both(metric_name, func, *args):
#         metrics[f"{metric_name}_exp3"] = func(df['Average Test R@10 Exp3'], *args)
#         metrics[f"{metric_name}_exp8"] = func(df['Average Test R@10 Exp8'], *args)

#     # Average Accuracy (AA)
#     calc_for_both("AA", np.mean)

#     # Forgetting Measure (FM)
#     def forgetting_measure(series):
#         max_performance = np.maximum.accumulate(series)
#         return np.mean(max_performance - series)
#     calc_for_both("FM", forgetting_measure)

#     # Incremental Plasticity (IP)
#     def incremental_plasticity(series):
#         return np.mean(np.diff(series))
#     calc_for_both("IP", incremental_plasticity)

#     # Stability to Plasticity Ratio (SPR)
#     def spr(test_series, val_series):
#         return np.mean(test_series) / np.mean(val_series)
#     metrics["SPR_exp3"] = spr(df['Average Test R@10 Exp3'], df['Validation R@10 Exp3'])
#     metrics["SPR_exp8"] = spr(df['Average Test R@10 Exp8'], df['Validation R@10 Exp8'])

#     # Rate of Change in Test R@10 (RCT)
#     def rct(series):
#         return np.mean(np.diff(series))
#     calc_for_both("RCT", rct)

#     # Cumulative Progress (CP)
#     def cumulative_progress(series):
#         return np.sum(np.diff(series))
#     calc_for_both("CP", cumulative_progress)

#     # Average Incremental Accuracy (AIA)
#     def aia(series):
#         return np.mean(np.cumsum(series) / np.arange(1, len(series) + 1))
#     calc_for_both("AIA", aia)

#     # Remembering (REM) and Positive Backward Transfer (BWT+)
#     def rem_bwt(series):
#         bwt = series[-1] - series[0]
#         return 1 - abs(min(bwt, 0)), max(bwt, 0)
#     metrics["REM_exp3"], metrics["BWT+_exp3"] = rem_bwt(df['Average Test R@10 Exp3'])
#     metrics["REM_exp8"], metrics["BWT+_exp8"] = rem_bwt(df['Average Test R@10 Exp8'])

#     # Time-Weighted Cumulative Performance (TWCP)
#     def twcp(series, alpha=0.1):
#         weights = np.exp(alpha * np.arange(len(series)))
#         return np.sum(weights * series) / np.sum(weights)
#     calc_for_both("TWCP", twcp)

#     # Stability-Plasticity Score (SPS)
#     def sps(test_series, val_series, beta=0.5):
#         stability = 1 - (np.std(test_series) / np.mean(test_series))
#         plasticity = np.mean(np.diff(val_series))
#         return beta * stability + (1 - beta) * plasticity
#     metrics["SPS_exp3"] = sps(df['Average Test R@10 Exp3'], df['Validation R@10 Exp3'])
#     metrics["SPS_exp8"] = sps(df['Average Test R@10 Exp8'], df['Validation R@10 Exp8'])

#     # Catastrophic Forgetting Resilience (CFR)
#     def cfr(series):
#         return np.min(series) / np.max(series)
#     calc_for_both("CFR", cfr)

#     # Progressive Learning Index (PLI)
#     def pli(series):
#         return np.sum(np.maximum(0, np.diff(series))) / (len(series) - 1)
#     calc_for_both("PLI", pli)

#     return metrics

# # Calculate metrics
# metrics = calculate_metrics(df)

# # Compare metrics
# comparison = {}
# for metric in metrics:
#     if metric.endswith('_exp3'):
#         base_metric = metric[:-5]
#         exp8_metric = f"{base_metric}_exp8"
#         if base_metric in ['forgetting']:  # Lower is better
#             comparison[base_metric] = 'Exp3' if metrics[metric] < metrics[exp8_metric] else 'Exp8'
#         else:  # Higher is better
#             comparison[base_metric] = 'Exp3' if metrics[metric] > metrics[exp8_metric] else 'Exp8'

# # Print results
# print("Metrics:")
# for metric, value in metrics.items():
#     print(f"{metric}: {value:.4f}")

# print("\nComparison (which experiment performs better):")
# for metric, better_exp in comparison.items():
#     print(f"{metric}: {better_exp}")

# # Perform statistical tests
# statistical_tests = {}
# for metric in ['Validation R@10', 'Average Test R@10']:
#     t_stat, p_value = stats.ttest_rel(df[f'{metric} Exp3'], df[f'{metric} Exp8'])
#     statistical_tests[metric] = {'t_statistic': t_stat, 'p_value': p_value}

# print("\nStatistical Tests (Paired t-test):")
# for metric, results in statistical_tests.items():
#     print(f"{metric}:")
#     print(f"  t-statistic: {results['t_statistic']:.4f}")
#     print(f"  p-value: {results['p_value']:.4f}")
#     print(f"  Significant difference: {'Yes' if results['p_value'] < 0.05 else 'No'}")