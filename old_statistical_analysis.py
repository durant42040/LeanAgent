# Data for Experiment 3
data_exp3 = {
    'Repository': [
        'Compfiles', 'Mathematics in Lean Source', 'PrimeNumberTheoremAnd', 'Math Workshop',
        'FLT', 'PFR', 'SciLean', 'Debate', 'Matrix Cookbook', 'Con-nf',
        'Foundation', 'Saturn', 'LeanEuclid', 'Lean4Lean'
    ],
    'Validation R@10 Exp3': [61.55, 67.99, 62.11, 64.44, 69.52, 63.13, 69.13, 70.35, 69.23, 58.69, 67.33, 77.12, 75.33, 74.85],
    'Average Test R@10 Exp3': [60.65, 63.41, 62.55, 63.13, 64.13, 64.18, 65.39, 65.93, 66.85, 66.79, 66.67, 66.85, 68.39, 69.18],
    'task1 Test R@10: Compfiles': [
        60.64706363237122, 61.32526532789667, 61.6708414949816, 61.08772811083543, 60.261503729020575, 60.30689658891723, 60.60123583745497, 
        60.153335213407054, 59.7709911492489, 60.00540189142794, 59.91546810620901,  59.88792078948343, 59.64851162308727, 59.420134063412014],
    'task2 Test R@10: Mathematics in Lean Source': [
        65.48591635672048, 65.75192142681436, 66.4970212474497, 66.49549432082041, 68.10014741519532, 67.93505866948891, 67.88206930569434, 
        67.97385677238191, 67.61899831570183, 67.69791601581457, 67.581090432529, 67.75405538565178, 67.55591710838813],
    'task3 Test R@10: PrimeNumberTheoremAnd': [
        60.24074464461104, 61.601678946721826, 61.53315591144687, 62.06460041347961, 61.59168435062894, 62.286496517728665, 63.17427181726865, 
        62.95367306725815, 62.84812634406436, 62.796309821659605, 62.573145480848204, 62.1576880065672],
    'task4 Test R@10: Math Workshop': [
        63.32418885891475, 65.32513361607977, 64.5102791379126, 67.1694072669325, 66.1893594713575, 67.5335852605448, 68.069787398159,
        68.17676215211705, 67.79894096460394, 68.11922176655432, 68.37256090708857],
    'task5 Test R@10: FLT': [
        67.05795410771319, 67.50657961559557, 68.07826197730941, 68.94304048302321, 69.25704440208798, 69.06493687974346, 69.13220145373448, 
        69.16048162503891, 69.47002065174642, 69.48301090016267],
    'task6 Test R@10: PFR': [
        62.58480312148852, 64.14710648979965, 64.12721536773596, 63.79502213074667, 63.97614939811086, 63.862150608153875, 63.66221697616451,
        64.11137631469667, 63.98214095462508],
    'task7 Test R@10: SciLean': [
        68.18567421512381, 68.94310521480176, 70.20595528030704, 70.11859835977681, 70.07787376617783, 70.21904098724055, 69.74478926670498,
        69.6667197138306],
    'task8 Test R@10: Debate': [68.91867342976552, 68.8826460829178, 69.18231380479203, 69.31982755616717 , 69.58427923681404, 69.13118545804842, 69.48585868047269],
    'task9 Test R@10: Matrix Cookbook': [71.04190539789398, 71.1094077126274, 71.08151370983799,  71.44692514637913, 71.19988426372417, 71.55403847771102],
    'task10 Test R@10: Con-nf': [65.80145354021758, 66.0667469234885, 64.94783306581058, 66.0404405207776, 65.51765650080257 ],
    'task11 Test R@10: Foundation': [65.21872588006309,  64.71692632303098 , 65.3519623141716, 65.13856379408705 ],
    'task12 Test R@10: Saturn': [70.44871794871794, 73.2396449704142, 71.61242603550296 ],
    'task13 Test R@10: LeanEuclid': [82.70537124802527, 83.11611374407583],
    'task14 Test R@10: Lean4Lean': [81.45833333333331],
}

# Data for Experiment 8
data_exp8 = {
    'Repository': [
        'Compfiles', 'Mathematics in Lean Source', 'PrimeNumberTheoremAnd', 'Math Workshop',
        'FLT', 'PFR', 'SciLean', 'Debate', 'Matrix Cookbook', 'Con-nf',
        'Foundation', 'Saturn', 'LeanEuclid', 'Lean4Lean'
    ],
    'Validation R@10 Exp8': [62.79, 68.67, 61.33, 66.67, 65.92, 64.63, 68.99, 68.7, 72.74, 60.18, 64.22, 85.74, 81.55, 76.62],
    'Average Test R@10 Exp8': [58.75, 63.04, 63.23, 63.54, 65.08, 64.35, 65.39, 66.12, 67.13, 67.23, 67.15, 68.99, 68.8, 69.46],
    'task1 Test R@10: Compfiles': [
        58.753674683076795, 59.266322324139544, 59.11501222901359, 58.65061069597731, 59.08351152440277, 58.254012190948366, 58.31714930746805, 
        58.16705172694482, 58.2052292761354, 57.81466274231215, 57.931586927587, 58.19557776263805, 57.83569946245733, 58.135263114918466
    ],
    'task2 Test R@10: Mathematics in Lean Source': [
        66.82241247037165, 68.22394378997019, 68.68426125028768, 68.46477136753248, 68.52621401540969, 69.0316174088683, 69.85560219203676,
        71.13058457926404, 71.08407817561478, 70.98203735928827, 71.3442495324248, 70.89091325285804, 70.812148413589
    ],
    'task3 Test R@10: PrimeNumberTheoremAnd': [
        62.37342972083976, 63.14389800117126, 63.57248915109329, 62.72338659851425, 63.358305722154476, 64.10691753468002, 63.84201634681258,
        64.27687840450473, 64.27556330718753, 64.32555466833091, 64.09342765687724, 64.9165197040928
    ],
    'task4 Test R@10: Math Workshop': [
        63.692126983903755, 64.88112322603578, 63.9572265456425, 64.24406215475595, 65.50233141990883, 65.34918568101253, 65.3123566360483,
        65.3153365514187, 65.78415399457909, 64.77471389023222, 65.22434587068398
    ],
    'task5 Test R@10: FLT': [
        69.4153115431529, 69.43774305709876, 70.20327098838466, 70.4426156383421, 71.33675063495751, 71.06544237274925, 70.90625206477227,
        71.0986096866722, 71.13206954581167, 70.97284724070164
    ],
    'task6 Test R@10: PFR': [
        63.22184229942324, 63.58756427174265, 63.68275102010593, 63.94294965538092, 63.77168689248713, 63.62856794668035, 63.94054979500917,
        63.14677779628911, 62.79802453366899
    ],
    'task7 Test R@10: SciLean': [
        68.99506046579262, 68.18453403141949, 68.60493009000173, 69.21270209449912, 68.94233134861383, 69.55466069440035, 69.0684458531042,
        69.1317642961663
    ],
    'task8 Test R@10: Debate': [
        68.90809008062388, 70.17778949140212, 69.6991246532138, 69.75589568301386, 69.9558657259099, 69.73137122548178, 69.76256041434732 
    ],
    'task9 Test R@10: Matrix Cookbook': [
        71.60183467326324, 71.1446252160538, 70.97734804877662, 71.58040636612066, 71.22861530004387, 71.39100846243703
    ],
    'task10 Test R@10: Con-nf': [
        68.94028103044498, 68.89075917252147, 69.10787470725997, 67.92715651834504, 68.53508001561279
    ],
    'task11 Test R@10: Foundation': [
        67.02251895209642, 67.00373960937341, 66.70270105481373, 67.33863788793366
    ],
    'task12 Test R@10: Saturn': [85.92970521541949, 84.79591836734693, 86.49659863945578],
    'task13 Test R@10: LeanEuclid': [73.07777777777778, 73.46666666666667],
    'task14 Test R@10: Lean4Lean': [73.47417840375586],
}

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats

# Prepare data
repos = data_exp3['Repository']
n_repos = len(repos)

def calculate_percent_change(exp3_value, exp8_value):
    return ((exp3_value - exp8_value) / exp8_value) * 100

def format_comparison(comparison_df):
    formatted_output = ""
    for _, row in comparison_df.iterrows():
        metric = row['Metric']
        exp3_value = row['Experiment 3']
        exp8_value = row['Experiment 8']
        difference = row['Difference (Exp3 - Exp8)']
        percent_change = row['Percent Change']
        
        if percent_change > 0:
            better = f"Exp3 BETTER (+{percent_change:.2f}%)"
        elif percent_change < 0:
            better = f"Exp8 better ({percent_change:.2f}%)"
        else:
            better = "Tie"
        
        formatted_output += f"{metric}:\n"
        formatted_output += f"  Exp3: {exp3_value:.4f}\n"
        formatted_output += f"  Exp8: {exp8_value:.4f}\n"
        formatted_output += f"  Diff: {difference:.4f}\n"
        formatted_output += f"  {better}\n\n"
    
    return formatted_output

def calculate_AA(data, exp_name):
    test_accuracies = data[f'Average Test R@10 {exp_name}']
    return test_accuracies[-1]  # The last value represents AA_k

def calculate_AIA(data, exp_name):
    test_accuracies = data[f'Average Test R@10 {exp_name}']
    AA_values = [np.mean(test_accuracies[:i+1]) for i in range(len(test_accuracies))]
    return np.mean(AA_values)

# Helper function to calculate metrics
def calculate_metrics(data, exp_name):
    metrics = {}
    
    # # 1. Final Average Test R@10
    # metrics['Final_Avg_Test_R@10'] = data[f'Average Test R@10 {exp_name}'][-1]
    
    # 2. Area Under the Learning Curve (AULC)
    aulc = np.trapz(data[f'Average Test R@10 {exp_name}']) / n_repos
    metrics['AULC'] = aulc
    
    # # 3. Forgetting Measure
    # forgetting = []
    # for i in range(2, n_repos + 1):
    #     task_performance = [data[f'task{j} Test R@10: {repos[j-1]}'][i-j-1] for j in range(1, i)]
    #     forgetting.append(np.max(task_performance) - data[f'task{i-1} Test R@10: {repos[i-2]}'][-1])
    # metrics['Avg_Forgetting'] = np.mean(forgetting)
    
    # # 4. Forward Transfer
    # forward_transfer = []
    # for i in range(2, n_repos + 1):
    #     forward_transfer.append(data[f'task{i} Test R@10: {repos[i-1]}'][0] - data[f'task1 Test R@10: {repos[0]}'][0])
    # metrics['Avg_Forward_Transfer'] = np.mean(forward_transfer)
    
    # 5. Backward Transfer (A positive BWT indicates that learning new tasks improves performance on old ones, while a negative BWT suggests forgetting)
    backward_transfer = []
    for i in range(1, n_repos):
        backward_transfer.append(data[f'task{i} Test R@10: {repos[i-1]}'][-1] - data[f'task{i} Test R@10: {repos[i-1]}'][0])
    metrics['Avg_Backward_Transfer'] = np.mean(backward_transfer)
    
    # # 6. Stability (using Average Test R@10)
    # stability = np.mean(np.diff(data[f'Average Test R@10 {exp_name}']))
    # metrics['Stability'] = stability
    
    # # 7. Plasticity (using Validation R@10)
    # plasticity = np.mean(np.diff(data[f'Validation R@10 {exp_name}']))
    # metrics['Plasticity'] = plasticity

    # Extract the task-specific accuracies
    task_accuracies = {}
    for i in range(1, 15):  # Assuming 14 tasks
        key = f'task{i} Test R@10: {data["Repository"][i-1]}'
        if key in data:
            task_accuracies[i] = np.array(data[key])

    # Calculate min-ACC
    min_acc_values = []
    for k in range(2, len(task_accuracies) + 1):
        min_acc_sum = 0
        count = 0
        for i in range(1, k):
            if i in task_accuracies and len(task_accuracies[i][k-1:]) > 0:
                min_acc_sum += np.min(task_accuracies[i][k-1:])
                count += 1
        if count > 0:
            min_acc_values.append(min_acc_sum / count)
    
    metrics['min-ACC'] = np.mean(min_acc_values) if min_acc_values else 0

    # Worst-case Accuracy (WC-ACC)
    avg_test = np.array(data[f'Average Test R@10 {exp_name}'])
    tasks = len(avg_test)
    wc_acc_values = []
    for k in range(1, tasks + 1):
        if k == 1:
            wc_acc_values.append(avg_test[0])
        elif k-2 < len(min_acc_values):
            wc_acc = (1/k) * avg_test[k-1] + (1 - 1/k) * min_acc_values[k-2]
            wc_acc_values.append(wc_acc)
    
    metrics['WC-ACC'] = np.mean(wc_acc_values)

    # 3. Windowed Forgetting (WF)
    def calculate_WF(w):
        WF = 0
        for i in range(len(avg_test)):
            if i >= w:
                WF = max(WF, np.max(avg_test[i-w:i]) - avg_test[i])
        return WF
    
    # metrics['WF2'] = calculate_WF(2)
    metrics['WF5'] = calculate_WF(5)
    # metrics['WF7'] = calculate_WF(7)
    # metrics['WF10'] = calculate_WF(10)

    # # 7. Windowed Plasticity (WP)
    # window_size = 2  # We can adjust this
    # wp_values = [max(0, avg_test[i] - avg_test[max(0, i-window_size)]) for i in range(len(avg_test))]
    # metrics['WP2'] = np.mean(wp_values)

    window_size = 5  # We can adjust this
    wp_values = [max(0, avg_test[i] - avg_test[max(0, i-window_size)]) for i in range(len(avg_test))]
    metrics['WP5'] = np.mean(wp_values)

    # window_size = 7  # We can adjust this
    # wp_values = [max(0, avg_test[i] - avg_test[max(0, i-window_size)]) for i in range(len(avg_test))]
    # metrics['WP7'] = np.mean(wp_values)

    # window_size = 10 # We can adjust this
    # wp_values = [max(0, avg_test[i] - avg_test[max(0, i-window_size)]) for i in range(len(avg_test))]
    # metrics['WP10'] = np.mean(wp_values)

    return metrics

# Calculate metrics for both experiments
metrics_exp3 = calculate_metrics(data_exp3, 'Exp3')
metrics_exp8 = calculate_metrics(data_exp8, 'Exp8')

# For the first comparison
comparison = pd.DataFrame({
    'Metric': metrics_exp3.keys(),
    'Experiment 3': metrics_exp3.values(),
    'Experiment 8': metrics_exp8.values(),
    'Difference (Exp3 - Exp8)': [metrics_exp3[k] - metrics_exp8[k] for k in metrics_exp3.keys()],
    'Percent Change': [calculate_percent_change(metrics_exp3[k], metrics_exp8[k]) for k in metrics_exp3.keys()]
})

print("Main Metrics Comparison:")
print(format_comparison(comparison))

# Visualize learning curves
plt.figure(figsize=(12, 6))
plt.plot(data_exp3['Average Test R@10 Exp3'], label='Experiment 3')
plt.plot(data_exp8['Average Test R@10 Exp8'], label='Experiment 8')
plt.xlabel('Repository')
plt.ylabel('Average Test R@10')
plt.title('Learning Curves: Experiment 3 vs Experiment 8')
plt.legend()
plt.grid(True)
plt.show()

# Visualize validation R@10
plt.figure(figsize=(12, 6))
plt.plot(data_exp3['Validation R@10 Exp3'], label='Experiment 3')
plt.plot(data_exp8['Validation R@10 Exp8'], label='Experiment 8')
plt.xlabel('Repository')
plt.ylabel('Validation R@10')
plt.title('Validation R@10: Experiment 3 vs Experiment 8')
plt.legend()
plt.grid(True)
plt.show()

def calculate_additional_metrics(data, exp_name):
    metrics = {}
    
    # # 1. Average validation R@10 performance
    # metrics['Avg_Validation_R@10'] = np.mean(data[f'Validation R@10 {exp_name}'])
    
    # # 2. Average Accuracy (AA)
    # metrics['AA'] = calculate_AA(data, exp_name)
    
    # 3. Forgetting Measure (FM)
    fm_values = []
    for i in range(2, len(data['Repository']) + 1):
        task_performances = [data[f'task{j} Test R@10: {data["Repository"][j-1]}'][i-j-1] for j in range(1, i)]
        fm_values.append(np.max(task_performances) - data[f'task{i-1} Test R@10: {data["Repository"][i-2]}'][-1])
    metrics['FM'] = np.mean(fm_values)
    
    # 4. Incremental Plasticity (IP)
    ip_values = np.diff(data[f'Validation R@10 {exp_name}'])
    metrics['IP'] = np.mean(ip_values)
    
    # # 5. Stability to Plasticity Ratio (SPR)
    # metrics['SPR'] = np.mean(data[f'Average Test R@10 {exp_name}']) / np.mean(data[f'Validation R@10 {exp_name}'])
    
    # # 6. Rate of Change in Test R@10 (RCT)
    # rct_values = np.diff(data[f'Average Test R@10 {exp_name}'])
    # metrics['RCT'] = np.mean(rct_values)
    
    # # 7. Cumulative Progress (CP)
    # metrics['CP'] = np.sum(np.diff(data[f'Average Test R@10 {exp_name}']))
    
    # # 8. Average Incremental Accuracy (AIA)
    # metrics['AIA'] = calculate_AIA(data, exp_name)
    
    # # 9. Remembering (REM) and Positive Backward Transfer (BWT+)
    # bwt_values = []
    # for i in range(1, len(data['Repository'])):
    #     bwt_values.append(data[f'task{i} Test R@10: {data["Repository"][i-1]}'][-1] - data[f'task{i} Test R@10: {data["Repository"][i-1]}'][0])
    # metrics['REM'] = 1 - abs(min(0, np.mean(bwt_values)))
    # metrics['BWT+'] = max(0, np.mean(bwt_values))
    
    # 10. Time-Weighted Cumulative Performance (TWCP)
    # weights = np.arange(1, len(data['Repository']) + 1)
    # weights = np.exp(np.arange(len(data['Repository'])))
    # Inverted weights since earlier is more important to avoid CF
    weights = np.arange(len(data['Repository']), 0, -1)
    # weights = np.ones(len(data['Repository']))
    metrics['TWCP'] = np.sum(weights * data[f'Average Test R@10 {exp_name}']) / np.sum(weights)
    
    # # 11. Stability-Plasticity Score (SPS)
    # stability = 1 - (np.std(data[f'Average Test R@10 {exp_name}']) / np.mean(data[f'Average Test R@10 {exp_name}']))
    # plasticity = np.mean(np.diff(data[f'Validation R@10 {exp_name}']))
    # metrics["SPS_stability"] = stability
    # metrics["SPS_plasticity"] = plasticity
    # beta = 0.7
    # metrics['SPS'] = beta * stability + (1 - beta) * plasticity
    
    # 12. Catastrophic Forgetting Resilience (CFR)
    metrics['CFR'] = np.min(data[f'Average Test R@10 {exp_name}']) / np.max(data[f'Average Test R@10 {exp_name}'])
    
    # # 13. Progressive Learning Index (PLI)
    # pli_values = np.maximum(0, np.diff(data[f'Average Test R@10 {exp_name}']))
    # metrics['PLI'] = np.sum(pli_values) / len(pli_values)
    
    return metrics

# Calculate additional metrics for both experiments
additional_metrics_exp3 = calculate_additional_metrics(data_exp3, 'Exp3')
additional_metrics_exp8 = calculate_additional_metrics(data_exp8, 'Exp8')

# For the additional metrics comparison
additional_comparison = pd.DataFrame({
    'Metric': additional_metrics_exp3.keys(),
    'Experiment 3': additional_metrics_exp3.values(),
    'Experiment 8': additional_metrics_exp8.values(),
    'Difference (Exp3 - Exp8)': [additional_metrics_exp3[k] - additional_metrics_exp8[k] for k in additional_metrics_exp3.keys()],
    'Percent Change': [calculate_percent_change(additional_metrics_exp3[k], additional_metrics_exp8[k]) for k in additional_metrics_exp3.keys()]
})

print("Additional Metrics Comparison:")
print(format_comparison(additional_comparison))