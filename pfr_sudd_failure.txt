aylet) [2024-08-10 12:16:09,377 E 3764278 3764308] (raylet) file_system_monitor.cc:111: /tmp/ray/session_20
24-08-10_12-14-28_152038_3763083 is over 95% full, available space: 93622145024; capacity: 1887507697664. O
bject creation will fail if spilling is required.
 48%|██████████████████████████████▍                                 | 1373/2890 [01:46<05:08,  4.92it/s](r
aylet) [2024-08-10 12:16:19,384 E 3764278 3764308] (raylet) file_system_monitor.cc:111: /tmp/ray/session_20
24-08-10_12-14-28_152038_3763083 is over 95% full, available space: 93622116352; capacity: 1887507697664. O
bject creation will fail if spilling is required.
 54%|██████████████████████████████████▋                             | 1569/2890 [02:13<00:24, 53.16it/s](r
aylet) [2024-08-10 12:16:29,390 E 3764278 3764308] (raylet) file_system_monitor.cc:111: /tmp/ray/session_20
24-08-10_12-14-28_152038_3763083 is over 95% full, available space: 93622050816; capacity: 1887507697664. O
bject creation will fail if spilling is required.
 54%|██████████████████████████████████▊                             | 1570/2890 [02:13<29:02,  1.32s/it](r
aylet) [2024-08-10 12:16:39,395 E 3764278 3764308] (raylet) file_system_monitor.cc:111: /tmp/ray/session_20
24-08-10_12-14-28_152038_3763083 is over 95% full, available space: 93620973568; capacity: 1887507697664. O
bject creation will fail if spilling is required.
 60%|██████████████████████████████████████▏                         | 1725/2890 [02:45<00:32, 36.35it/s](r
aylet) [2024-08-10 12:16:59,407 E 3764278 3764308] (raylet) file_system_monitor.cc:111: /tmp/ray/session_20
24-08-10_12-14-28_152038_3763083 is over 95% full, available space: 93620920320; capacity: 1887507697664. O
bject creation will fail if spilling is required. [repeated 2x across cluster] (Ray deduplicates logs by de
fault. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observab
ility/user-guides/configure-logging.html#log-deduplication for more options.)
 71%|█████████████████████████████████████████████▋                  | 2065/2890 [03:23<30:23,  2.21s/it](r
aylet) [2024-08-10 12:17:29,425 E 3764278 3764308] (raylet) file_system_monitor.cc:111: /tmp/ray/session_20
24-08-10_12-14-28_152038_3763083 is over 95% full, available space: 93620772864; capacity: 1887507697664. O
bject creation will fail if spilling is required. [repeated 3x across cluster]
 80%|███████████████████████████████████████████████████▏            | 2314/2890 [04:10<33:06,  3.45s/it](r
aylet) [2024-08-10 12:18:09,446 E 3764278 3764308] (raylet) file_system_monitor.cc:111: /tmp/ray/session_20
24-08-10_12-14-28_152038_3763083 is over 95% full, available space: 93619138560; capacity: 1887507697664. O
bject creation will fail if spilling is required. [repeated 4x across cluster]
 85%|██████████████████████████████████████████████████████▏         | 2448/2890 [04:16<00:14, 31.05it/s](r
aylet) [2024-08-10 12:18:49,483 E 3764278 3764308] (raylet) file_system_monitor.cc:111: /tmp/ray/session_20
24-08-10_12-14-28_152038_3763083 is over 95% full, available space: 93618212864; capacity: 1887507697664. O
bject creation will fail if spilling is required. [repeated 4x across cluster]
 89%|█████████████████████████████████████████████████████████       | 2575/2890 [05:08<16:54,  3.22s/it](r
aylet) [2024-08-10 12:18:59,489 E 3764278 3764308] (raylet) file_system_monitor.cc:111: /tmp/ray/session_20
24-08-10_12-14-28_152038_3763083 is over 95% full, available space: 93618188288; capacity: 1887507697664. O
bject creation will fail if spilling is required.
(raylet) [2024-08-10 12:19:09,495 E 3764278 3764308] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_12-14-28_152038_3763083 is over 95% full, available space: 93618159616; capacity: 1887507697664.
 Object creation will fail if spilling is required.
100%|████████████████████████████████████████████████████████████████| 2890/2890 [05:11<00:00,  9.27it/s]
(raylet) [2024-08-10 12:19:39,510 E 3764278 3764308] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_12-14-28_152038_3763083 is over 95% full, available space: 93616762880; capacity: 1887507697664.
 Object creation will fail if spilling is required. [repeated 3x across cluster]
2024-08-10 12:19:46.579 | DEBUG    | lean_dojo.data_extraction.lean:get_dependencies:489 - Querying the dep
endencies of LeanGitRepo(url='https://github.com/teorth/pfr', commit='6a5082ee465f9e44cea479c7b741b3163162b
b7e')
2024-08-10 12:19:47.936 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the commi
t hash for lean4 v4.6.0-rc1
2024-08-10 12:19:50.190 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the commi
t hash for lean4 v4.7.0
2024-08-10 12:19:51.117 | DEBUG    | lean_dojo.data_extraction.lean:url_to_repo:68 - url_to_repo("https://g
ithub.com/leanprover-community/import-graph.git") failed. Retrying...
2024-08-10 12:19:52.237 | DEBUG    | lean_dojo.data_extraction.lean:url_to_repo:68 - url_to_repo("https://g
ithub.com/leanprover-community/import-graph.git") failed. Retrying...
2024-08-10 12:19:55.468 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the commi
t hash for lean4 v4.10.0
2024-08-10 12:19:56.496 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the commi
t hash for doc-gen4 main
2024-08-10 12:19:57.110 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the commi
t hash for lean4 v4.11.0-rc1
2024-08-10 12:19:57.748 | DEBUG    | lean_dojo.data_extraction.lean:get_dependencies:489 - Querying the dep
endencies of LeanGitRepo(url='https://github.com/YaelDillies/LeanAPAP', commit='f47447e44d8e82ab214ed8c1b19
9329141fc5b1f')
2024-08-10 12:19:58.514 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the commi
t hash for lean4 v4.9.0-rc1
2024-08-10 12:19:59.550 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the commi
t hash for lean4 v4.10.0-rc1
2024-08-10 12:20:00.300 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the commi
t hash for lean4 v4.9.0
2024-08-10 12:20:01.788 | DEBUG    | lean_dojo.data_extraction.lean:url_to_repo:68 - url_to_repo("https://g
ithub.com/leanprover-community/mathlib4.git") failed. Retrying...
2024-08-10 12:20:02.906 | DEBUG    | lean_dojo.data_extraction.lean:url_to_repo:68 - url_to_repo("https://g
ithub.com/leanprover-community/mathlib4.git") failed. Retrying...
2024-08-10 12:20:06.158 | DEBUG    | lean_dojo.data_extraction.lean:get_dependencies:489 - Querying the dep
endencies of LeanGitRepo(url='https://github.com/leanprover-community/mathlib4', commit='3dd071bc2260b3cf9a
71863d0dee1242fec41522')
2024-08-10 12:20:07.392 | DEBUG    | lean_dojo.data_extraction.lean:get_dependencies:489 - Querying the dep
endencies of LeanGitRepo(url='https://github.com/leanprover/doc-gen4', commit='6d8e3118ab526f8dfcabcbdf9f05
dc34e5c423a8')
2024-08-10 12:20:08.444 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the commi
t hash for lean4 v4.10.0-rc2
Following Github server redirection from /repos/mhuisi/lean4-cli to /repositories/341363356
2024-08-10 12:20:10.306 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the commi
t hash for lean4 nightly-2024-06-05
2024-08-10 12:20:11.583 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the commi
t hash for md4lean main
2024-08-10 12:20:11.781 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the commi
t hash for BibtexQuery master
2024-08-10 12:20:12.177 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the commi
t hash for lean4-unicode-basic main
2024-08-10 12:20:12.897 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the commi
t hash for lean4-cli nightly
2024-08-10 12:20:13.077 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the commi
t hash for lean4 nightly-2024-06-05
2024-08-10 12:20:14.314 | INFO     | generate_benchmark_lean4:main:346 - Failed to trace repo LeanGitRepo(u
rl='https://github.com/teorth/pfr', commit='6a5082ee465f9e44cea479c7b741b3163162bb7e') because of Invalid t
ag or branch: `nightly-2024-06-05` for Repository(full_name="leanprover/lean4")
2024-08-10 12:20:25.498 | INFO     | __main__:generate_dataset:246 - Finished generating benchmark at /raid
/adarsh/datasets_test/pfr_6a5082ee465f9e44cea479c7b741b3163162bb7e
2024-08-10 12:20:25.501 | INFO     | __main__:generate_dataset:247 - Merging datasets
2024-08-10 12:20:25.502 | INFO     | __main__:merge_datasets:74 - Merging datasets for random
2024-08-10 12:20:25.502 | INFO     | __main__:merge_datasets:80 - Processing train split
2024-08-10 12:20:25.502 | INFO     | __main__:merge_datasets:104 - Finished processing train split
2024-08-10 12:20:25.502 | INFO     | __main__:merge_datasets:80 - Processing val split
2024-08-10 12:20:25.502 | INFO     | __main__:merge_datasets:104 - Finished processing val split
2024-08-10 12:20:25.502 | INFO     | __main__:merge_datasets:80 - Processing test split
2024-08-10 12:20:25.503 | INFO     | __main__:merge_datasets:104 - Finished processing test split
2024-08-10 12:20:25.503 | INFO     | __main__:merge_datasets:105 - Finished merging datasets for random
2024-08-10 12:20:25.503 | INFO     | __main__:merge_datasets:74 - Merging datasets for novel_premises
2024-08-10 12:20:25.503 | INFO     | __main__:merge_datasets:80 - Processing train split
2024-08-10 12:20:25.503 | INFO     | __main__:merge_datasets:104 - Finished processing train split
2024-08-10 12:20:25.503 | INFO     | __main__:merge_datasets:80 - Processing val split
2024-08-10 12:20:25.503 | INFO     | __main__:merge_datasets:104 - Finished processing val split
2024-08-10 12:20:25.503 | INFO     | __main__:merge_datasets:80 - Processing test split
2024-08-10 12:20:25.503 | INFO     | __main__:merge_datasets:104 - Finished processing test split
2024-08-10 12:20:25.503 | INFO     | __main__:merge_datasets:105 - Finished merging datasets for novel_prem
ises
2024-08-10 12:20:25.503 | INFO     | __main__:merge_datasets:107 - Merging corpus
2024-08-10 12:20:25.504 | INFO     | __main__:merge_datasets:130 - Finished merging corpus
2024-08-10 12:20:25.504 | INFO     | __main__:merge_datasets:132 - Adding metadata
2024-08-10 12:20:25.504 | WARNING  | __main__:merge_datasets:152 - No metadata file found
2024-08-10 12:20:25.504 | INFO     | __main__:merge_datasets:154 - Deleting individual datasets
2024-08-10 12:20:25.504 | INFO     | __main__:generate_dataset:249 - Finished merging datasets
2024-08-10 12:20:25.735 | INFO     | __main__:main:635 - Latest PL checkpoint found: AK123321/pl-leancopilo
t-2
2024-08-10 12:20:25.842 | INFO     | __main__:download_pl_checkpoint:274 - Checkpoint downloaded to: /home/
adarsh/.cache/huggingface/hub/models--AK123321--pl-leancopilot-2/snapshots/d4955ed1972ad84c03dce9189162d039
12f2d86f/model.ckpt
2024-08-10 12:20:25.843 | INFO     | __main__:main:638 - Checkpoint path: /home/adarsh/.cache/huggingface/h
ub/models--AK123321--pl-leancopilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86f/model.ckpt
2024-08-10 12:20:25.843 | INFO     | __main__:train:464 - Training model with checkpoint: /home/adarsh/.cac
he/huggingface/hub/models--AK123321--pl-leancopilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86f/mo
del.ckpt
Seed set to 3407
Lightning automatically upgraded your loaded checkpoint from v0.0.0 to v2.4.0. To apply the upgrade to your
 files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../.cache/huggingface/hub
/models--AK123321--pl-leancopilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86f/model.ckpt`
2024-08-10 12:20:36.995 | INFO     | __main__:train:486 - Loaded premise retriever at /home/adarsh/.cache/h
uggingface/hub/models--AK123321--pl-leancopilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86f/model.
ckpt
2024-08-10 12:20:36.995 | INFO     | __main__:train:490 - Data path: /raid/adarsh/datasets_test/merged/rand
om
/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/transformers/tokenization_u
tils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by d
efault. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default.
For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
2024-08-10 12:20:37.092 | INFO     | common:__init__:199 - Building the corpus from /raid/adarsh/datasets_t
est/merged/corpus.jsonl
2024-08-10 12:20:37.093 | INFO     | __main__:main:647 - An error occurred: [Errno 2] No such file or direc
tory: '/raid/adarsh/datasets_test/merged/random/../metadata.json'
Traceback (most recent call last):
  File "/home/adarsh/ReProver/compute_server.py", line 641, in main
    train(model_checkpoint_path, merged_data_path, next_suffix)
  File "/home/adarsh/ReProver/compute_server.py", line 491, in train
    data_module = RetrievalDataModule(
  File "/home/adarsh/ReProver/retrieval/datamodule.py", line 246, in __init__
    metadata = json.load(open(os.path.join(data_path, "../metadata.json")))
FileNotFoundError: [Errno 2] No such file or directory: '/raid/adarsh/datasets_test/merged/random/../metada
ta.json'
(base) adarsh@tensorlab-DGX-Station-A100-920-23487-2531-000:~/ReProver(base) adarsh@tensorlab-DGX-Station-A
(base) adarsh@tensorlab-DGX-Station-A100-920-23487-2531-000:~/ReProver$ bash run_compute_server.sh
Script executed from: /home/adarsh/ReProver
[2024-08-10 13:02:16,217] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (
auto detect)
 [WARNING]  async_io requires the dev libaio .so object and headers but these were not found.
 [WARNING]  async_io: please install the libaio-dev package with apt
 [WARNING]  If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS enviro
nment variables to where it can be found.
 [WARNING]  Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 [WARNING]  sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
 [WARNING]  using untested triton version (3.0.0), only 1.0.0 is known to be compatible
/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/deepspeed/runtime/zero/line
ar.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_f
wd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/deepspeed/runtime/zero/line
ar.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_b
wd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
2024-08-10 13:02:21.068 | INFO     | __main__:check_progress_file:586 - Checking contents of /home/adarsh/m
iniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning/loops/progress.py
2024-08-10 13:02:21.068 | INFO     | __main__:check_progress_file:590 - Contents of progress.py:
2024-08-10 13:02:21.068 | INFO     | __main__:check_progress_file:591 - # Copyright The Lightning AI team.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import asdict, dataclass, field
from typing import Type

from typing_extensions import override


@dataclass
class _BaseProgress:
    """Mixin that implements state-loading utilities for dataclasses."""

    def state_dict(self) -> dict:
        return asdict(self)

    def load_state_dict(self, state_dict: dict) -> None:
        if state_dict["completed"] == None:
            state_dict["completed"] = 0
        self.__dict__.update(state_dict)

    @classmethod
    def from_state_dict(cls, state_dict: dict) -> "_BaseProgress":
        obj = cls()
        obj.load_state_dict(state_dict)
        return obj

    def reset(self) -> None:
        """Reset the object's state."""
        raise NotImplementedError


@dataclass
class _ReadyCompletedTracker(_BaseProgress):
    """Track an event's progress.

    Args:
        ready: Intended to track the number of events ready to start.
        completed: Intended to be incremented after the event completes (e.g. after ``on_*_end`` runs).

    These attributes should be increased in order, that is, :attr:`ready` first and :attr:`completed` last.

    """

    ready: int = 0
    completed: int = 0

    @override
    def reset(self) -> None:
        """Reset the state."""
        self.ready = 0
        self.completed = 0

    def reset_on_restart(self) -> None:
        """Reset the progress on restart.

        If there is a failure before all attributes are increased, restore the attributes to the last fully
 completed
        value.

        """
        self.ready = self.completed


@dataclass
class _StartedTracker(_ReadyCompletedTracker):
    """Track an event's progress.

    Args:
        ready: Intended to track the number of events ready to start.
        started: Intended to be incremented after the event is started (e.g. after ``on_*_start`` runs).
        completed: Intended to be incremented after the event completes (e.g. after ``on_*_end`` runs).

    These attributes should be increased in order, that is, :attr:`ready` first and :attr:`completed` last.

    """

    started: int = 0

    @override
    def reset(self) -> None:
        super().reset()
        self.started = 0

    @override
    def reset_on_restart(self) -> None:
        super().reset_on_restart()
        self.started = self.completed


@dataclass
class _ProcessedTracker(_StartedTracker):
    """Track an event's progress.

    Args:
        ready: Intended to track the number of events ready to start.
        started: Intended to be incremented after the event is started (e.g. after ``on_*_start`` runs).
        processed: Intended to be incremented after the event is processed.
        completed: Intended to be incremented after the event completes (e.g. after ``on_*_end`` runs).

    These attributes should be increased in order, that is, :attr:`ready` first and :attr:`completed` last.

    """

    processed: int = 0

    @override
    def reset(self) -> None:
        super().reset()
        self.processed = 0

    @override
    def reset_on_restart(self) -> None:
        super().reset_on_restart()
        self.processed = self.completed


@dataclass
class _Progress(_BaseProgress):
    """Track aggregated and current progress.

    Args:
        total: Intended to track the total progress of an event.
        current: Intended to track the current progress of an event.

    """

    total: _ReadyCompletedTracker = field(default_factory=_ProcessedTracker)
    current: _ReadyCompletedTracker = field(default_factory=_ProcessedTracker)

    def __post_init__(self) -> None:
        if self.total.__class__ is not self.current.__class__:
            raise ValueError("The `total` and `current` instances should be of the same class")

    def increment_ready(self) -> None:
        self.total.ready += 1
        self.current.ready += 1

    def increment_started(self) -> None:
        if not isinstance(self.total, _StartedTracker):
            raise TypeError(f"`{self.total.__class__.__name__}` doesn't have a `started` attribute")
        self.total.started += 1
        self.current.started += 1

    def increment_processed(self) -> None:
        if not isinstance(self.total, _ProcessedTracker):
            raise TypeError(f"`{self.total.__class__.__name__}` doesn't have a `processed` attribute")
        self.total.processed += 1
        self.current.processed += 1

    def increment_completed(self) -> None:
        self.total.completed += 1
        self.current.completed += 1

    @classmethod
    def from_defaults(cls, tracker_cls: Type[_ReadyCompletedTracker], **kwargs: int) -> "_Progress":
        """Utility function to easily create an instance from keyword arguments to both ``Tracker``s."""
        return cls(total=tracker_cls(**kwargs), current=tracker_cls(**kwargs))

    @override
    def reset(self) -> None:
        self.total.reset()
        self.current.reset()

    def reset_on_run(self) -> None:
        self.current.reset()

    def reset_on_restart(self) -> None:
        self.current.reset_on_restart()

    @override
    def load_state_dict(self, state_dict: dict) -> None:
        if state_dict["total"]["completed"] == None:
            state_dict["total"]["completed"] = 0
        self.total.load_state_dict(state_dict["total"])
        self.current.load_state_dict(state_dict["current"])


@dataclass
class _BatchProgress(_Progress):
    """Tracks batch progress.

    These counters are local to a trainer rank. By default, they are not globally synced across all ranks.

    Args:
        total: Tracks the total batch progress.
        current: Tracks the current batch progress.
        is_last_batch: Whether the batch is the last one. This is useful for iterable datasets.

    """

    is_last_batch: bool = False

    @override
    def reset(self) -> None:
        super().reset()
        self.is_last_batch = False

    @override
    def reset_on_run(self) -> None:
        super().reset_on_run()
        self.is_last_batch = False

    @override
    def load_state_dict(self, state_dict: dict) -> None:
        if state_dict["total"]["completed"] == None:
            state_dict["total"]["completed"] = 0
        super().load_state_dict(state_dict)
        self.is_last_batch = state_dict["is_last_batch"]


@dataclass
class _SchedulerProgress(_Progress):
    """Tracks scheduler progress.

    These counters are local to a trainer rank. By default, they are not globally synced across all ranks.

    Args:
        total: Tracks the total scheduler progress.
        current: Tracks the current scheduler progress.

    """

    total: _ReadyCompletedTracker = field(default_factory=_ReadyCompletedTracker)
    current: _ReadyCompletedTracker = field(default_factory=_ReadyCompletedTracker)


@dataclass
class _OptimizerProgress(_BaseProgress):
    """Track optimizer progress.

    Args:
        step: Tracks ``optimizer.step`` calls.
        zero_grad: Tracks ``optimizer.zero_grad`` calls.

    """

    step: _Progress = field(default_factory=lambda: _Progress.from_defaults(_ReadyCompletedTracker))
    zero_grad: _Progress = field(default_factory=lambda: _Progress.from_defaults(_StartedTracker))

    @override
    def reset(self) -> None:
        self.step.reset()
        self.zero_grad.reset()

    def reset_on_run(self) -> None:
        self.step.reset_on_run()
        self.zero_grad.reset_on_run()

    def reset_on_restart(self) -> None:
        self.step.reset_on_restart()
        self.zero_grad.reset_on_restart()

    @override
    def load_state_dict(self, state_dict: dict) -> None:
        if state_dict["step"]["total"]["completed"] == None:
            state_dict["step"]["total"]["completed"] = 0
        self.step.load_state_dict(state_dict["step"])
        self.zero_grad.load_state_dict(state_dict["zero_grad"])


@dataclass
class _OptimizationProgress(_BaseProgress):
    """Track optimization progress.

    Args:
        optimizer: Tracks optimizer progress.

    """

    optimizer: _OptimizerProgress = field(default_factory=_OptimizerProgress)

    @property
    def optimizer_steps(self) -> int:
        return self.optimizer.step.total.completed

    @override
    def reset(self) -> None:
        self.optimizer.reset()

    def reset_on_run(self) -> None:
        self.optimizer.reset_on_run()

    def reset_on_restart(self) -> None:
        self.optimizer.reset_on_restart()

    @override
    def load_state_dict(self, state_dict: dict) -> None:
        if state_dict["optimizer"]["step"]["total"]["completed"] == None:
            state_dict["optimizer"]["step"]["total"]["completed"] = 0
        self.optimizer.load_state_dict(state_dict["optimizer"])

2024-08-10 13:02:21.068 | INFO     | __main__:main:608 - Starting compute server...
2024-08-10 13:02:21.069 | INFO     | __main__:main:609 - Current working directory: /home/adarsh/ReProver
2024-08-10 13:02:21.069 | INFO     | __main__:main:613 - ROOT_DIR: /raid/adarsh
2024-08-10 13:02:21.069 | INFO     | __main__:main:614 - DATA_DIR: datasets_test
2024-08-10 13:02:21.069 | INFO     | __main__:main:618 - Configuring LeanDojo...
2024-08-10 13:02:21.072 | INFO     | generate_benchmark_lean4:configure_leandojo:294 - Current working dire
ctory: /home/adarsh/ReProver
2024-08-10 13:02:21.072 | INFO     | __main__:main:620 - LeanDojo configured
2024-08-10 13:02:21.072 | INFO     | __main__:main:628 - Unique URLs: {'https://github.com/Adarsh321123/new
-version-test.git'}
2024-08-10 13:02:21.072 | INFO     | __main__:main:629 - About to generate datasets...
2024-08-10 13:02:21.072 | INFO     | __main__:generate_dataset:226 - Generating 1 datasets
2024-08-10 13:02:21.072 | INFO     | __main__:generate_dataset:231 - Processing https://github.com/Adarsh32
1123/new-version-test.git
2024-08-10 13:02:21.291 | INFO     | __main__:get_compatible_commit:166 - Latest commit: f465306be03ced999c
aa157a85558a6c41b3e3f5
2024-08-10 13:02:21.291 | INFO     | __main__:get_compatible_commit:169 - Creating LeanGitRepo for https://
github.com/Adarsh321123/new-version-test
2024-08-10 13:02:21.717 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the commi
t hash for lean4 v4.8.0-rc1
2024-08-10 13:02:22.306 | INFO     | __main__:get_compatible_commit:171 - Getting config for https://github
.com/Adarsh321123/new-version-test.git
2024-08-10 13:02:22.306 | INFO     | __main__:get_compatible_commit:175 - Latest commit compatible for url
https://github.com/Adarsh321123/new-version-test.git
2024-08-10 13:02:22.306 | INFO     | __main__:generate_dataset:236 - Found compatible commit f465306be03ced
999caa157a85558a6c41b3e3f5 for https://github.com/Adarsh321123/new-version-test.git
2024-08-10 13:02:22.306 | INFO     | __main__:generate_dataset:237 - Lean version: v4.8.0-rc1
2024-08-10 13:02:22.306 | INFO     | __main__:generate_dataset:240 - Creating LeanGitRepo for https://githu
b.com/Adarsh321123/new-version-test
2024-08-10 13:02:22.306 | INFO     | __main__:generate_dataset:244 - Generating benchmark at /raid/adarsh/d
atasets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5
2024-08-10 13:02:22.306 | INFO     | generate_benchmark_lean4:main:301 - Generating dataset to go into /rai
d/adarsh/datasets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5
2024-08-10 13:02:22.306 | INFO     | generate_benchmark_lean4:main:308 - lean toolchain version: {'content'
: 'leanprover/lean4:v4.8.0-rc1\n'}
2024-08-10 13:02:22.306 | INFO     | generate_benchmark_lean4:main:310 - lean version v: v4.8.0-rc1
2024-08-10 13:02:22.307 | INFO     | generate_benchmark_lean4:main:311 - is supported: True
2024-08-10 13:02:22.307 | INFO     | generate_benchmark_lean4:main:321 - lean path1 /home/adarsh/.elan/tool
chains/leanprover--lean4---4.8.0-rc1
2024-08-10 13:02:22.307 | INFO     | generate_benchmark_lean4:main:322 - lean path2 /.elan/toolchains/leanp
rover--lean4---4.8.0-rc1
2024-08-10 13:02:22.307 | INFO     | generate_benchmark_lean4:main:323 - lean path3 ~/.elan/toolchains/lean
prover--lean4---4.8.0-rc1
2024-08-10 13:02:22.307 | INFO     | generate_benchmark_lean4:main:327 - Lean toolchain path 2 does not exi
st: /.elan/toolchains/leanprover--lean4---4.8.0-rc1
2024-08-10 13:02:22.307 | INFO     | generate_benchmark_lean4:main:329 - Lean toolchain path 3 does not exi
st: ~/.elan/toolchains/leanprover--lean4---4.8.0-rc1
2024-08-10 13:02:22.307 | INFO     | generate_benchmark_lean4:main:332 - Switched to Lean toolchain at: /ho
me/adarsh/.elan/toolchains/leanprover--lean4---4.8.0-rc1
2024-08-10 13:02:22.421 | INFO     | generate_benchmark_lean4:main:334 - lean --version: Lean (version 4.8.
0-rc1, x86_64-unknown-linux-gnu, commit dcccfb73cb24, Release)

2024-08-10 13:02:22.421 | INFO     | generate_benchmark_lean4:main:335 - repo: LeanGitRepo(url='https://git
hub.com/Adarsh321123/new-version-test', commit='f465306be03ced999caa157a85558a6c41b3e3f5')
2024-08-10 13:02:22.422 | INFO     | generate_benchmark_lean4:main:337 - Configuring LeanDojo again...
2024-08-10 13:02:22.425 | INFO     | generate_benchmark_lean4:configure_leandojo:294 - Current working dire
ctory: /home/adarsh/ReProver
2024-08-10 13:02:22.425 | INFO     | generate_benchmark_lean4:main:339 - LeanDojo configured
2024-08-10 13:02:22.425 | INFO     | generate_benchmark_lean4:main:342 - Tracing the repo...
2024-08-10 13:02:22.426 | DEBUG    | lean_dojo.data_extraction.trace:get_traced_repo_path:217 - The traced
repo is available in the cache.
2024-08-10 13:02:22.426 | INFO     | lean_dojo.data_extraction.trace:trace:246 - Loading the traced repo fr
om /raid/adarsh/.cache/lean_dojo/Adarsh321123-new-version-test-f465306be03ced999caa157a85558a6c41b3e3f5/new
-version-test
2024-08-10 13:02:22.430 | DEBUG    | lean_dojo.utils:execute:110 - git remote get-url origin
2024-08-10 13:02:22.433 | DEBUG    | lean_dojo.utils:execute:110 - git log -n 1
2024-08-10 13:02:22.829 | DEBUG    | lean_dojo.data_extraction.traced_data:load_from_disk:1169 - Loading 54
32 traced XML files from /raid/adarsh/.cache/lean_dojo/Adarsh321123-new-version-test-f465306be03ced999caa15
7a85558a6c41b3e3f5/new-version-test with 31 workers
2024-08-10 13:02:24,345 INFO worker.py:1772 -- Started a local Ray instance. View the dashboard at 127.0.0.
1:8267
  4%|██▍                                                                | 193/5432 [00:07<05:22, 16.25it/s]
(raylet) [2024-08-10 13:02:34,216 E 3948443 3948473] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-02-22_985372_3947514 is over 95% full, available space: 93605412864; capacity: 1887507697664.
 Object creation will fail if spilling is required.
  8%|█████▍                                                             | 441/5432 [00:16<02:26, 34.13it/s]
(raylet) [2024-08-10 13:02:44,221 E 3948443 3948473] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-02-22_985372_3947514 is over 95% full, available space: 93605367808; capacity: 1887507697664.
 Object creation will fail if spilling is required.
 13%|████████▍                                                          | 685/5432 [00:28<09:24,  8.41it/s]
(raylet) [2024-08-10 13:02:54,225 E 3948443 3948473] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-02-22_985372_3947514 is over 95% full, available space: 93605367808; capacity: 1887507697664.
 Object creation will fail if spilling is required.
 17%|███████████▏                                                       | 903/5432 [00:37<01:39, 45.50it/s]
(raylet) [2024-08-10 13:03:04,230 E 3948443 3948473] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-02-22_985372_3947514 is over 95% full, available space: 93605314560; capacity: 1887507697664.
 Object creation will fail if spilling is required.
 19%|████████████▍                                                     | 1020/5432 [00:46<01:15, 58.26it/s]
(raylet) [2024-08-10 13:03:14,236 E 3948443 3948473] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-02-22_985372_3947514 is over 95% full, available space: 93605318656; capacity: 1887507697664.
 Object creation will fail if spilling is required.
 19%|████████████▍                                                     | 1020/5432 [01:05<01:15, 58.26it/s]
(raylet) [2024-08-10 13:03:34,246 E 3948443 3948473] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-02-22_985372_3947514 is over 95% full, available space: 93604474880; capacity: 1887507697664.
 Object creation will fail if spilling is required. [repeated 2x across cluster] (Ray deduplicates logs by
default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observ
ability/user-guides/configure-logging.html#log-deduplication for more options.)
(raylet) [2024-08-10 13:03:44,251 E 3948443 3948473] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-02-22_985372_3947514 is over 95% full, available space: 93604466688; capacity: 1887507697664.
 Object creation will fail if spilling is required.
(raylet) [2024-08-10 13:03:54,255 E 3948443 3948473] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-02-22_985372_3947514 is over 95% full, available space: 93604446208; capacity: 1887507697664.
 Object creation will fail if spilling is required.
(raylet) [2024-08-10 13:04:04,258 E 3948443 3948473] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-02-22_985372_3947514 is over 95% full, available space: 93604425728; capacity: 1887507697664.
 Object creation will fail if spilling is required.
 19%|████████████                                                    | 1023/5432 [01:38<2:56:15,  2.40s/it]
(raylet) [2024-08-10 13:04:05,314 E 3948443 3948443] (raylet) node_manager.cc:3064: 1 Workers (tasks / acto
rs) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 6fcde2f75e6798
8c94a839b9b56fe401afb8b0e3db7b5a4f8b6a0b7f, IP: 131.215.143.185) over the last time period. To see more inf
ormation about the Workers killed on this node, use `ray logs raylet.out -ip 131.215.143.185`
(raylet)
(raylet) Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/lates
t/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing t
ask parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variabl
e `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `
RAY_memory_monitor_refresh_ms` to zero.
 20%|█████████████                                                     | 1080/5432 [01:39<10:26,  6.94it/s]
(raylet) [2024-08-10 13:04:14,263 E 3948443 3948473] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-02-22_985372_3947514 is over 95% full, available space: 93604364288; capacity: 1887507697664.
 Object creation will fail if spilling is required.
 20%|█████████████                                                     | 1080/5432 [01:55<10:26,  6.94it/s]
(raylet) [2024-08-10 13:04:24,268 E 3948443 3948473] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-02-22_985372_3947514 is over 95% full, available space: 93604343808; capacity: 1887507697664.
 Object creation will fail if spilling is required.
(raylet) [2024-08-10 13:04:34,273 E 3948443 3948473] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-02-22_985372_3947514 is over 95% full, available space: 93603323904; capacity: 1887507697664.
 Object creation will fail if spilling is required.
(raylet) [2024-08-10 13:04:44,277 E 3948443 3948473] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-02-22_985372_3947514 is over 95% full, available space: 93603307520; capacity: 1887507697664.
 Object creation will fail if spilling is required.
(raylet) [2024-08-10 13:04:54,282 E 3948443 3948473] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-02-22_985372_3947514 is over 95% full, available space: 93603295232; capacity: 1887507697664.
 Object creation will fail if spilling is required.
 20%|████████████▊                                                   | 1089/5432 [02:29<2:17:02,  1.89s/it]
(raylet) [2024-08-10 13:05:04,287 E 3948443 3948473] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-02-22_985372_3947514 is over 95% full, available space: 93603266560; capacity: 1887507697664.
 Object creation will fail if spilling is required.
 20%|████████████▊                                                   | 1089/5432 [02:45<2:17:02,  1.89s/it]
(raylet) [2024-08-10 13:05:14,292 E 3948443 3948473] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-02-22_985372_3947514 is over 95% full, available space: 93603270656; capacity: 1887507697664.
 Object creation will fail if spilling is required.
(raylet) [2024-08-10 13:05:24,297 E 3948443 3948473] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-02-22_985372_3947514 is over 95% full, available space: 93603246080; capacity: 1887507697664.
 Object creation will fail if spilling is required.
(raylet) [2024-08-10 13:05:34,301 E 3948443 3948473] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-02-22_985372_3947514 is over 95% full, available space: 93601681408; capacity: 1887507697664.
 Object creation will fail if spilling is required.
(raylet) [2024-08-10 13:05:44,195 E 3948443 3948443] (raylet) node_manager.cc:3064: 2 Workers (tasks / acto
rs) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 6fcde2f75e6798
8c94a839b9b56fe401afb8b0e3db7b5a4f8b6a0b7f, IP: 131.215.143.185) over the last time period. To see more inf
ormation about the Workers killed on this node, use `ray logs raylet.out -ip 131.215.143.185`
(raylet)
(raylet) Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/lates
t/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing t
ask parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variabl
e `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `
RAY_memory_monitor_refresh_ms` to zero.
 20%|████████████▊                                                   | 1091/5432 [03:17<5:45:31,  4.78s/it]
(raylet) [2024-08-10 13:05:44,306 E 3948443 3948473] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-02-22_985372_3947514 is over 95% full, available space: 93601624064; capacity: 1887507697664.
 Object creation will fail if spilling is required.
 20%|█████████████▍                                                    | 1109/5432 [03:17<12:51,  5.60it/s]
2024-08-10 13:05:49.848 | INFO     | generate_benchmark_lean4:main:346 - Failed to trace repo LeanGitRepo(u
rl='https://github.com/Adarsh321123/new-version-test', commit='f465306be03ced999caa157a85558a6c41b3e3f5') b
ecause of Task was killed due to the node running low on memory.
Memory on the node (IP: 131.215.143.185, ID: 6fcde2f75e67988c94a839b9b56fe401afb8b0e3db7b5a4f8b6a0b7f) wher
e the task (actor ID: a0c1fb305837788d1307f10e01000000, name=_TracedRepoHelper.__init__, pid=3955880, memor
y used=0.11GB) was running was 478.55GB / 503.73GB (0.950018), which exceeds the memory usage threshold of
0.95. Ray killed this worker (ID: aa9bf869363bfdb6d5945ca143e4df40ac67dc5c56c8f1ca153355ef) because it was
the most recently scheduled task; to see more information about memory usage on this node, use `ray logs ra
ylet.out -ip 131.215.143.185`. To see the logs of the worker, use `ray logs worker-aa9bf869363bfdb6d5945ca1
43e4df40ac67dc5c56c8f1ca153355ef*out -ip 131.215.143.185. Top 10 memory users:
PID     MEM(GB) COMMAND
1312425 28.10   python /home/adarsh/ReProver/main.py
2579903 9.37    /home/adarsh/miniconda3/envs/ReProver/bin/python /home/adarsh/ReProver/main.py
3412529 6.34    /home/adarsh/.elan/toolchains/leanprover--lean4---v4.7.0/bin/lean --server /home/adarsh/Put
namBench
1457158 4.31    python /home/adarsh/ReProver/compute_server.py
1192859 3.71    /home/adarsh/.vscode-server/cli/servers/Stable-89de5a8d4d6205e5b11647eb6a74844ca23d2573/ser
ver/node ...
3947514 3.12    python /home/adarsh/ReProver/compute_server.py
33227   2.97    /home/adarsh/.elan/toolchains/leanprover--lean4---v4.8.0-rc1/bin/lean --worker file:///home
/adarsh/L...
424107  2.75    /home/adarsh/.elan/toolchains/leanprover--lean4---v4.8.0-rc1/bin/lean --worker file:///home
/adarsh/L...
1346581 2.26    /home/adarsh/.elan/toolchains/leanprover--lean4---v4.8.0-rc1/bin/lean --worker file:///home
/adarsh/L...
521587  2.07    /home/adarsh/.elan/toolchains/leanprover--lean4---v4.8.0-rc1/bin/lean --worker file:///home
/adarsh/L...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-cor
e/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task paral
lelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the tas
k crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshol
d` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_m
s` to zero.
2024-08-10 13:05:49.849 | INFO     | __main__:generate_dataset:246 - Finished generating benchmark at /raid
/adarsh/datasets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5
2024-08-10 13:05:49.849 | INFO     | __main__:generate_dataset:247 - Merging datasets
2024-08-10 13:05:49.849 | INFO     | __main__:merge_datasets:74 - Merging datasets for random
2024-08-10 13:05:49.849 | INFO     | __main__:merge_datasets:80 - Processing train split
2024-08-10 13:05:49.850 | INFO     | __main__:merge_datasets:104 - Finished processing train split
2024-08-10 13:05:49.850 | INFO     | __main__:merge_datasets:80 - Processing val split
2024-08-10 13:05:49.850 | INFO     | __main__:merge_datasets:104 - Finished processing val split
2024-08-10 13:05:49.850 | INFO     | __main__:merge_datasets:80 - Processing test split
2024-08-10 13:05:49.850 | INFO     | __main__:merge_datasets:104 - Finished processing test split
2024-08-10 13:05:49.850 | INFO     | __main__:merge_datasets:105 - Finished merging datasets for random
2024-08-10 13:05:49.851 | INFO     | __main__:merge_datasets:74 - Merging datasets for novel_premises
2024-08-10 13:05:49.851 | INFO     | __main__:merge_datasets:80 - Processing train split
2024-08-10 13:05:49.851 | INFO     | __main__:merge_datasets:104 - Finished processing train split
2024-08-10 13:05:49.851 | INFO     | __main__:merge_datasets:80 - Processing val split
2024-08-10 13:05:49.851 | INFO     | __main__:merge_datasets:104 - Finished processing val split
2024-08-10 13:05:49.851 | INFO     | __main__:merge_datasets:80 - Processing test split
2024-08-10 13:05:49.851 | INFO     | __main__:merge_datasets:104 - Finished processing test split
2024-08-10 13:05:49.851 | INFO     | __main__:merge_datasets:105 - Finished merging datasets for novel_prem
ises
2024-08-10 13:05:49.852 | INFO     | __main__:merge_datasets:107 - Merging corpus
2024-08-10 13:05:49.852 | INFO     | __main__:merge_datasets:130 - Finished merging corpus
2024-08-10 13:05:49.852 | INFO     | __main__:merge_datasets:132 - Adding metadata
2024-08-10 13:05:49.852 | WARNING  | __main__:merge_datasets:152 - No metadata file found
2024-08-10 13:05:49.852 | INFO     | __main__:merge_datasets:154 - Deleting individual datasets
2024-08-10 13:05:49.852 | INFO     | __main__:generate_dataset:249 - Finished merging datasets
2024-08-10 13:05:49.991 | INFO     | __main__:main:635 - Latest PL checkpoint found: AK123321/pl-leancopilo
t-2
2024-08-10 13:05:50.104 | INFO     | __main__:download_pl_checkpoint:274 - Checkpoint downloaded to: /home/
adarsh/.cache/huggingface/hub/models--AK123321--pl-leancopilot-2/snapshots/d4955ed1972ad84c03dce9189162d039
12f2d86f/model.ckpt
2024-08-10 13:05:50.104 | INFO     | __main__:main:638 - Checkpoint path: /home/adarsh/.cache/huggingface/h
ub/models--AK123321--pl-leancopilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86f/model.ckpt
2024-08-10 13:05:50.104 | INFO     | __main__:train:464 - Training model with checkpoint: /home/adarsh/.cac
he/huggingface/hub/models--AK123321--pl-leancopilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86f/mo
del.ckpt
Seed set to 3407
Lightning automatically upgraded your loaded checkpoint from v0.0.0 to v2.4.0. To apply the upgrade to your
 files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../.cache/huggingface/hub
/models--AK123321--pl-leancopilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86f/model.ckpt`
2024-08-10 13:06:02.433 | INFO     | __main__:train:486 - Loaded premise retriever at /home/adarsh/.cache/h
uggingface/hub/models--AK123321--pl-leancopilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86f/model.
ckpt
2024-08-10 13:06:02.433 | INFO     | __main__:train:490 - Data path: /raid/adarsh/datasets_test/merged/rand
om
/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/transformers/tokenization_u
tils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by d
efault. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default.
For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
2024-08-10 13:06:02.524 | INFO     | common:__init__:199 - Building the corpus from /raid/adarsh/datasets_t
est/merged/corpus.jsonl
2024-08-10 13:06:02.525 | INFO     | __main__:main:647 - An error occurred: [Errno 2] No such file or direc
tory: '/raid/adarsh/datasets_test/merged/random/../metadata.json'
Traceback (most recent call last):
  File "/home/adarsh/ReProver/compute_server.py", line 641, in main
    train(model_checkpoint_path, merged_data_path, next_suffix)
  File "/home/adarsh/ReProver/compute_server.py", line 491, in train
    data_module = RetrievalDataModule(
  File "/home/adarsh/ReProver/retrieval/datamodule.py", line 246, in __init__
    metadata = json.load(open(os.path.join(data_path, "../metadata.json")))
FileNotFoundError: [Errno 2] No such file or directory: '/raid/adarsh/datasets_test/merged/random/../metada
ta.json'
(base) adarsh@tensorlab-DGX-Station-A100-920-23487-2531-000:~/ReProver$ bash run_compute_server.sh
Script executed from: /home/adarsh/ReProver
[2024-08-10 13:06:40,943] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (
auto detect)
 [WARNING]  async_io requires the dev libaio .so object and headers but these were not found.
 [WARNING]  async_io: please install the libaio-dev package with apt
 [WARNING]  If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS enviro
nment variables to where it can be found.
 [WARNING]  Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 [WARNING]  sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
 [WARNING]  using untested triton version (3.0.0), only 1.0.0 is known to be compatible
/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/deepspeed/runtime/zero/line
ar.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_f
wd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/deepspeed/runtime/zero/line
ar.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_b
wd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
2024-08-10 13:06:46.889 | INFO     | __main__:check_progress_file:586 - Checking contents of /home/adarsh/m
iniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning/loops/progress.py
2024-08-10 13:06:46.889 | INFO     | __main__:check_progress_file:590 - Contents of progress.py:
2024-08-10 13:06:46.889 | INFO     | __main__:check_progress_file:591 - # Copyright The Lightning AI team.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import asdict, dataclass, field
from typing import Type

from typing_extensions import override


@dataclass
class _BaseProgress:
    """Mixin that implements state-loading utilities for dataclasses."""

    def state_dict(self) -> dict:
        return asdict(self)

    def load_state_dict(self, state_dict: dict) -> None:
        if state_dict["completed"] == None:
            state_dict["completed"] = 0
        self.__dict__.update(state_dict)

    @classmethod
    def from_state_dict(cls, state_dict: dict) -> "_BaseProgress":
        obj = cls()
        obj.load_state_dict(state_dict)
        return obj

    def reset(self) -> None:
        """Reset the object's state."""
        raise NotImplementedError


@dataclass
class _ReadyCompletedTracker(_BaseProgress):
    """Track an event's progress.

    Args:
        ready: Intended to track the number of events ready to start.
        completed: Intended to be incremented after the event completes (e.g. after ``on_*_end`` runs).

    These attributes should be increased in order, that is, :attr:`ready` first and :attr:`completed` last.

    """

    ready: int = 0
    completed: int = 0

    @override
    def reset(self) -> None:
        """Reset the state."""
        self.ready = 0
        self.completed = 0

    def reset_on_restart(self) -> None:
        """Reset the progress on restart.

        If there is a failure before all attributes are increased, restore the attributes to the last fully
 completed
        value.

        """
        self.ready = self.completed


@dataclass
class _StartedTracker(_ReadyCompletedTracker):
    """Track an event's progress.

    Args:
        ready: Intended to track the number of events ready to start.
        started: Intended to be incremented after the event is started (e.g. after ``on_*_start`` runs).
        completed: Intended to be incremented after the event completes (e.g. after ``on_*_end`` runs).

    These attributes should be increased in order, that is, :attr:`ready` first and :attr:`completed` last.

    """

    started: int = 0

    @override
    def reset(self) -> None:
        super().reset()
        self.started = 0

    @override
    def reset_on_restart(self) -> None:
        super().reset_on_restart()
        self.started = self.completed


@dataclass
class _ProcessedTracker(_StartedTracker):
    """Track an event's progress.

    Args:
        ready: Intended to track the number of events ready to start.
        started: Intended to be incremented after the event is started (e.g. after ``on_*_start`` runs).
        processed: Intended to be incremented after the event is processed.
        completed: Intended to be incremented after the event completes (e.g. after ``on_*_end`` runs).

    These attributes should be increased in order, that is, :attr:`ready` first and :attr:`completed` last.

    """

    processed: int = 0

    @override
    def reset(self) -> None:
        super().reset()
        self.processed = 0

    @override
    def reset_on_restart(self) -> None:
        super().reset_on_restart()
        self.processed = self.completed


@dataclass
class _Progress(_BaseProgress):
    """Track aggregated and current progress.

    Args:
        total: Intended to track the total progress of an event.
        current: Intended to track the current progress of an event.

    """

    total: _ReadyCompletedTracker = field(default_factory=_ProcessedTracker)
    current: _ReadyCompletedTracker = field(default_factory=_ProcessedTracker)

    def __post_init__(self) -> None:
        if self.total.__class__ is not self.current.__class__:
            raise ValueError("The `total` and `current` instances should be of the same class")

    def increment_ready(self) -> None:
        self.total.ready += 1
        self.current.ready += 1

    def increment_started(self) -> None:
        if not isinstance(self.total, _StartedTracker):
            raise TypeError(f"`{self.total.__class__.__name__}` doesn't have a `started` attribute")
        self.total.started += 1
        self.current.started += 1

    def increment_processed(self) -> None:
        if not isinstance(self.total, _ProcessedTracker):
            raise TypeError(f"`{self.total.__class__.__name__}` doesn't have a `processed` attribute")
        self.total.processed += 1
        self.current.processed += 1

    def increment_completed(self) -> None:
        self.total.completed += 1
        self.current.completed += 1

    @classmethod
    def from_defaults(cls, tracker_cls: Type[_ReadyCompletedTracker], **kwargs: int) -> "_Progress":
        """Utility function to easily create an instance from keyword arguments to both ``Tracker``s."""
        return cls(total=tracker_cls(**kwargs), current=tracker_cls(**kwargs))

    @override
    def reset(self) -> None:
        self.total.reset()
        self.current.reset()

    def reset_on_run(self) -> None:
        self.current.reset()

    def reset_on_restart(self) -> None:
        self.current.reset_on_restart()

    @override
    def load_state_dict(self, state_dict: dict) -> None:
        if state_dict["total"]["completed"] == None:
            state_dict["total"]["completed"] = 0
        self.total.load_state_dict(state_dict["total"])
        self.current.load_state_dict(state_dict["current"])


@dataclass
class _BatchProgress(_Progress):
    """Tracks batch progress.

    These counters are local to a trainer rank. By default, they are not globally synced across all ranks.

    Args:
        total: Tracks the total batch progress.
        current: Tracks the current batch progress.
        is_last_batch: Whether the batch is the last one. This is useful for iterable datasets.

    """

    is_last_batch: bool = False

    @override
    def reset(self) -> None:
        super().reset()
        self.is_last_batch = False

    @override
    def reset_on_run(self) -> None:
        super().reset_on_run()
        self.is_last_batch = False

    @override
    def load_state_dict(self, state_dict: dict) -> None:
        if state_dict["total"]["completed"] == None:
            state_dict["total"]["completed"] = 0
        super().load_state_dict(state_dict)
        self.is_last_batch = state_dict["is_last_batch"]


@dataclass
class _SchedulerProgress(_Progress):
    """Tracks scheduler progress.

    These counters are local to a trainer rank. By default, they are not globally synced across all ranks.

    Args:
        total: Tracks the total scheduler progress.
        current: Tracks the current scheduler progress.

    """

    total: _ReadyCompletedTracker = field(default_factory=_ReadyCompletedTracker)
    current: _ReadyCompletedTracker = field(default_factory=_ReadyCompletedTracker)


@dataclass
class _OptimizerProgress(_BaseProgress):
    """Track optimizer progress.

    Args:
        step: Tracks ``optimizer.step`` calls.
        zero_grad: Tracks ``optimizer.zero_grad`` calls.

    """

    step: _Progress = field(default_factory=lambda: _Progress.from_defaults(_ReadyCompletedTracker))
    zero_grad: _Progress = field(default_factory=lambda: _Progress.from_defaults(_StartedTracker))

    @override
    def reset(self) -> None:
        self.step.reset()
        self.zero_grad.reset()

    def reset_on_run(self) -> None:
        self.step.reset_on_run()
        self.zero_grad.reset_on_run()

    def reset_on_restart(self) -> None:
        self.step.reset_on_restart()
        self.zero_grad.reset_on_restart()

    @override
    def load_state_dict(self, state_dict: dict) -> None:
        if state_dict["step"]["total"]["completed"] == None:
            state_dict["step"]["total"]["completed"] = 0
        self.step.load_state_dict(state_dict["step"])
        self.zero_grad.load_state_dict(state_dict["zero_grad"])


@dataclass
class _OptimizationProgress(_BaseProgress):
    """Track optimization progress.

    Args:
        optimizer: Tracks optimizer progress.

    """

    optimizer: _OptimizerProgress = field(default_factory=_OptimizerProgress)

    @property
    def optimizer_steps(self) -> int:
        return self.optimizer.step.total.completed

    @override
    def reset(self) -> None:
        self.optimizer.reset()

    def reset_on_run(self) -> None:
        self.optimizer.reset_on_run()

    def reset_on_restart(self) -> None:
        self.optimizer.reset_on_restart()

    @override
    def load_state_dict(self, state_dict: dict) -> None:
        if state_dict["optimizer"]["step"]["total"]["completed"] == None:
            state_dict["optimizer"]["step"]["total"]["completed"] = 0
        self.optimizer.load_state_dict(state_dict["optimizer"])

2024-08-10 13:06:46.890 | INFO     | __main__:main:608 - Starting compute server...
2024-08-10 13:06:46.890 | INFO     | __main__:main:609 - Current working directory: /home/adarsh/ReProver
2024-08-10 13:06:46.890 | INFO     | __main__:main:613 - ROOT_DIR: /raid/adarsh
2024-08-10 13:06:46.890 | INFO     | __main__:main:614 - DATA_DIR: datasets_test
2024-08-10 13:06:46.890 | INFO     | __main__:main:618 - Configuring LeanDojo...
2024-08-10 13:06:46.893 | INFO     | generate_benchmark_lean4:configure_leandojo:294 - Current working dire
ctory: /home/adarsh/ReProver
2024-08-10 13:06:46.893 | INFO     | __main__:main:620 - LeanDojo configured
2024-08-10 13:06:46.893 | INFO     | __main__:main:628 - Unique URLs: {'https://github.com/Adarsh321123/new
-version-test.git'}
2024-08-10 13:06:46.893 | INFO     | __main__:main:629 - About to generate datasets...
2024-08-10 13:06:46.893 | INFO     | __main__:generate_dataset:226 - Generating 1 datasets
2024-08-10 13:06:46.893 | INFO     | __main__:generate_dataset:231 - Processing https://github.com/Adarsh32
1123/new-version-test.git
2024-08-10 13:06:47.085 | INFO     | __main__:get_compatible_commit:166 - Latest commit: f465306be03ced999c
aa157a85558a6c41b3e3f5
2024-08-10 13:06:47.085 | INFO     | __main__:get_compatible_commit:169 - Creating LeanGitRepo for https://
github.com/Adarsh321123/new-version-test
2024-08-10 13:06:47.302 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the commi
t hash for lean4 v4.8.0-rc1
2024-08-10 13:06:48.115 | INFO     | __main__:get_compatible_commit:171 - Getting config for https://github
.com/Adarsh321123/new-version-test.git
2024-08-10 13:06:48.115 | INFO     | __main__:get_compatible_commit:175 - Latest commit compatible for url
https://github.com/Adarsh321123/new-version-test.git
2024-08-10 13:06:48.115 | INFO     | __main__:generate_dataset:236 - Found compatible commit f465306be03ced
999caa157a85558a6c41b3e3f5 for https://github.com/Adarsh321123/new-version-test.git
2024-08-10 13:06:48.115 | INFO     | __main__:generate_dataset:237 - Lean version: v4.8.0-rc1
2024-08-10 13:06:48.115 | INFO     | __main__:generate_dataset:240 - Creating LeanGitRepo for https://githu
b.com/Adarsh321123/new-version-test
2024-08-10 13:06:48.115 | INFO     | __main__:generate_dataset:244 - Generating benchmark at /raid/adarsh/d
atasets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5
2024-08-10 13:06:48.116 | INFO     | generate_benchmark_lean4:main:301 - Generating dataset to go into /rai
d/adarsh/datasets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5
2024-08-10 13:06:48.116 | INFO     | generate_benchmark_lean4:main:308 - lean toolchain version: {'content'
: 'leanprover/lean4:v4.8.0-rc1\n'}
2024-08-10 13:06:48.116 | INFO     | generate_benchmark_lean4:main:310 - lean version v: v4.8.0-rc1
2024-08-10 13:06:48.116 | INFO     | generate_benchmark_lean4:main:311 - is supported: True
2024-08-10 13:06:48.116 | INFO     | generate_benchmark_lean4:main:321 - lean path1 /home/adarsh/.elan/tool
chains/leanprover--lean4---4.8.0-rc1
2024-08-10 13:06:48.116 | INFO     | generate_benchmark_lean4:main:322 - lean path2 /.elan/toolchains/leanp
rover--lean4---4.8.0-rc1
2024-08-10 13:06:48.116 | INFO     | generate_benchmark_lean4:main:323 - lean path3 ~/.elan/toolchains/lean
prover--lean4---4.8.0-rc1
2024-08-10 13:06:48.116 | INFO     | generate_benchmark_lean4:main:327 - Lean toolchain path 2 does not exi
st: /.elan/toolchains/leanprover--lean4---4.8.0-rc1
2024-08-10 13:06:48.116 | INFO     | generate_benchmark_lean4:main:329 - Lean toolchain path 3 does not exi
st: ~/.elan/toolchains/leanprover--lean4---4.8.0-rc1
2024-08-10 13:06:48.116 | INFO     | generate_benchmark_lean4:main:332 - Switched to Lean toolchain at: /ho
me/adarsh/.elan/toolchains/leanprover--lean4---4.8.0-rc1
2024-08-10 13:06:48.150 | INFO     | generate_benchmark_lean4:main:334 - lean --version: Lean (version 4.8.
0-rc1, x86_64-unknown-linux-gnu, commit dcccfb73cb24, Release)

2024-08-10 13:06:48.150 | INFO     | generate_benchmark_lean4:main:335 - repo: LeanGitRepo(url='https://git
hub.com/Adarsh321123/new-version-test', commit='f465306be03ced999caa157a85558a6c41b3e3f5')
2024-08-10 13:06:48.150 | INFO     | generate_benchmark_lean4:main:337 - Configuring LeanDojo again...
2024-08-10 13:06:48.153 | INFO     | generate_benchmark_lean4:configure_leandojo:294 - Current working dire
ctory: /home/adarsh/ReProver
2024-08-10 13:06:48.153 | INFO     | generate_benchmark_lean4:main:339 - LeanDojo configured
2024-08-10 13:06:48.154 | INFO     | generate_benchmark_lean4:main:342 - Tracing the repo...
2024-08-10 13:06:48.154 | DEBUG    | lean_dojo.data_extraction.trace:get_traced_repo_path:217 - The traced
repo is available in the cache.
2024-08-10 13:06:48.154 | INFO     | lean_dojo.data_extraction.trace:trace:246 - Loading the traced repo fr
om /raid/adarsh/.cache/lean_dojo/Adarsh321123-new-version-test-f465306be03ced999caa157a85558a6c41b3e3f5/new
-version-test
2024-08-10 13:06:48.157 | DEBUG    | lean_dojo.utils:execute:110 - git remote get-url origin
2024-08-10 13:06:48.159 | DEBUG    | lean_dojo.utils:execute:110 - git log -n 1
2024-08-10 13:06:48.303 | DEBUG    | lean_dojo.data_extraction.traced_data:load_from_disk:1169 - Loading 54
32 traced XML files from /raid/adarsh/.cache/lean_dojo/Adarsh321123-new-version-test-f465306be03ced999caa15
7a85558a6c41b3e3f5/new-version-test with 31 workers
2024-08-10 13:06:49,681 INFO worker.py:1772 -- Started a local Ray instance. View the dashboard at 127.0.0.
1:8267
  4%|██▊                                                                | 227/5432 [00:06<01:46, 49.04it/s]
(raylet) [2024-08-10 13:06:59,569 E 3973094 3973123] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-06-48_458555_3972182 is over 95% full, available space: 93599064064; capacity: 1887507697664.
 Object creation will fail if spilling is required.
  8%|█████▍                                                             | 444/5432 [00:18<09:16,  8.97it/s]
(raylet) [2024-08-10 13:07:09,575 E 3973094 3973123] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-06-48_458555_3972182 is over 95% full, available space: 93599039488; capacity: 1887507697664.
 Object creation will fail if spilling is required.
 12%|████████▎                                                          | 675/5432 [00:27<08:37,  9.19it/s]
(raylet) [2024-08-10 13:07:19,580 E 3973094 3973123] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-06-48_458555_3972182 is over 95% full, available space: 93599027200; capacity: 1887507697664.
 Object creation will fail if spilling is required.
 17%|███████████                                                        | 897/5432 [00:35<01:37, 46.44it/s]
(raylet) [2024-08-10 13:07:29,584 E 3973094 3973123] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-06-48_458555_3972182 is over 95% full, available space: 93599006720; capacity: 1887507697664.
 Object creation will fail if spilling is required.
 19%|████████████▎                                                     | 1010/5432 [00:44<01:25, 51.72it/s]
(raylet) [2024-08-10 13:07:39,588 E 3973094 3973123] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-06-48_458555_3972182 is over 95% full, available space: 93598986240; capacity: 1887507697664.
 Object creation will fail if spilling is required.
 21%|██████████████                                                    | 1155/5432 [01:05<31:01,  2.30it/s]
(raylet) [2024-08-10 13:07:49,592 E 3973094 3973123] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-06-48_458555_3972182 is over 95% full, available space: 93598937088; capacity: 1887507697664.
 Object creation will fail if spilling is required.
 22%|██████████████▊                                                   | 1216/5432 [01:07<02:31, 27.82it/s]
(raylet) [2024-08-10 13:07:59,596 E 3973094 3973123] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-06-48_458555_3972182 is over 95% full, available space: 93598167040; capacity: 1887507697664.
 Object creation will fail if spilling is required.
 23%|███████████████▍                                                  | 1273/5432 [01:21<40:36,  1.71it/s]
(raylet) [2024-08-10 13:08:09,600 E 3973094 3973123] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-06-48_458555_3972182 is over 95% full, available space: 93598154752; capacity: 1887507697664.
 Object creation will fail if spilling is required.
 27%|█████████████████▊                                                | 1465/5432 [01:41<01:18, 50.52it/s]
(raylet) [2024-08-10 13:08:19,604 E 3973094 3973123] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-06-48_458555_3972182 is over 95% full, available space: 93598138368; capacity: 1887507697664.
 Object creation will fail if spilling is required.
 31%|████████████████████▎                                             | 1676/5432 [02:05<55:12,  1.13it/s]
(raylet) [2024-08-10 13:08:39,613 E 3973094 3973123] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-06-48_458555_3972182 is over 95% full, available space: 93598113792; capacity: 1887507697664.
 Object creation will fail if spilling is required. [repeated 2x across cluster] (Ray deduplicates logs by
default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observ
ability/user-guides/configure-logging.html#log-deduplication for more options.)
 36%|███████████████████████▎                                        | 1981/5432 [02:36<1:12:36,  1.26s/it]
(raylet) [2024-08-10 13:09:09,627 E 3973094 3973123] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-06-48_458555_3972182 is over 95% full, available space: 93596921856; capacity: 1887507697664.
 Object creation will fail if spilling is required. [repeated 3x across cluster]
 42%|██████████████████████████▊                                     | 2279/5432 [03:15<2:19:14,  2.65s/it]
(raylet) [2024-08-10 13:09:39,642 E 3973094 3973123] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-06-48_458555_3972182 is over 95% full, available space: 93596880896; capacity: 1887507697664.
 Object creation will fail if spilling is required. [repeated 3x across cluster]
 48%|██████████████████████████████▉                                 | 2626/5432 [04:06<1:48:40,  2.32s/it]
(raylet) [2024-08-10 13:10:19,659 E 3973094 3973123] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-06-48_458555_3972182 is over 95% full, available space: 93595291648; capacity: 1887507697664.
 Object creation will fail if spilling is required. [repeated 4x across cluster]
 59%|█████████████████████████████████████▌                          | 3191/5432 [05:07<1:30:32,  2.42s/it]
(raylet) [2024-08-10 13:11:09,681 E 3973094 3973123] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-06-48_458555_3972182 is over 95% full, available space: 93594468352; capacity: 1887507697664.
 Object creation will fail if spilling is required. [repeated 5x across cluster]
 69%|█████████████████████████████████████████████▏                    | 3723/5432 [05:17<00:56, 30.04it/s]
(raylet) [2024-08-10 13:12:09,707 E 3973094 3973123] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-06-48_458555_3972182 is over 95% full, available space: 93593051136; capacity: 1887507697664.
 Object creation will fail if spilling is required. [repeated 6x across cluster]
 71%|█████████████████████████████████████████████▌                  | 3865/5432 [06:28<1:42:04,  3.91s/it]
(raylet) [2024-08-10 13:12:19,711 E 3973094 3973123] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-06-48_458555_3972182 is over 95% full, available space: 93593042944; capacity: 1887507697664.
 Object creation will fail if spilling is required.
(raylet) [2024-08-10 13:12:29,716 E 3973094 3973123] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-06-48_458555_3972182 is over 95% full, available space: 93593034752; capacity: 1887507697664.
 Object creation will fail if spilling is required.
 78%|███████████████████████████████████████████████████▎              | 4219/5432 [06:37<00:35, 34.48it/s]
(raylet) [2024-08-10 13:13:29,739 E 3973094 3973123] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-06-48_458555_3972182 is over 95% full, available space: 93592129536; capacity: 1887507697664.
 Object creation will fail if spilling is required. [repeated 6x across cluster]
 82%|████████████████████████████████████████████████████▌           | 4460/5432 [08:04<1:09:45,  4.31s/it]
(raylet) [2024-08-10 13:13:39,744 E 3973094 3973123] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-06-48_458555_3972182 is over 95% full, available space: 93592109056; capacity: 1887507697664.
 Object creation will fail if spilling is required.
(raylet) [2024-08-10 13:13:49,748 E 3973094 3973123] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-06-48_458555_3972182 is over 95% full, available space: 93592064000; capacity: 1887507697664.
 Object creation will fail if spilling is required.
 91%|████████████████████████████████████████████████████████████▎     | 4969/5432 [08:17<00:12, 36.87it/s]
(raylet) [2024-08-10 13:15:09,783 E 3973094 3973123] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-06-48_458555_3972182 is over 95% full, available space: 93589925888; capacity: 1887507697664.
 Object creation will fail if spilling is required. [repeated 8x across cluster]
 99%|█████████████████████████████████████████████████████████████████ | 5352/5432 [10:00<02:30,  1.88s/it]
(raylet) [2024-08-10 13:15:19,790 E 3973094 3973123] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-06-48_458555_3972182 is over 95% full, available space: 93589913600; capacity: 1887507697664.
 Object creation will fail if spilling is required.
(raylet) [2024-08-10 13:15:29,796 E 3973094 3973123] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-06-48_458555_3972182 is over 95% full, available space: 93589905408; capacity: 1887507697664.
 Object creation will fail if spilling is required.
100%|██████████████████████████████████████████████████████████████████| 5432/5432 [10:01<00:00,  9.03it/s]
(raylet) [2024-08-10 13:16:49,831 E 3973094 3973123] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-06-48_458555_3972182 is over 95% full, available space: 93588635648; capacity: 1887507697664.
 Object creation will fail if spilling is required. [repeated 8x across cluster]
2024-08-10 13:16:56.583 | DEBUG    | lean_dojo.data_extraction.lean:get_dependencies:489 - Querying the dep
endencies of LeanGitRepo(url='https://github.com/Adarsh321123/new-version-test', commit='f465306be03ced999c
aa157a85558a6c41b3e3f5')
2024-08-10 13:16:57.565 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the commi
t hash for lean4 v4.8.0-rc2
2024-08-10 13:16:58.794 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the commi
t hash for lean4 v4.6.0-rc1
2024-08-10 13:17:01.144 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the commi
t hash for lean4 v4.7.0
2024-08-10 13:17:02.107 | DEBUG    | lean_dojo.data_extraction.lean:url_to_repo:68 - url_to_repo("https://g
ithub.com/leanprover-community/import-graph.git") failed. Retrying...
2024-08-10 13:17:03.244 | DEBUG    | lean_dojo.data_extraction.lean:url_to_repo:68 - url_to_repo("https://g
ithub.com/leanprover-community/import-graph.git") failed. Retrying...
2024-08-10 13:17:06.456 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the commi
t hash for lean4 v4.11.0-rc1
2024-08-10 13:17:07.079 | DEBUG    | lean_dojo.data_extraction.lean:get_dependencies:489 - Querying the dep
endencies of LeanGitRepo(url='https://github.com/leanprover-community/mathlib4', commit='3dd071bc2260b3cf9a
71863d0dee1242fec41522')
2024-08-10 13:17:07.666 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the commi
t hash for lean4 v4.10.0
2024-08-10 13:17:09.022 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the commi
t hash for lean4 v4.9.0
2024-08-10 13:17:39.898 | DEBUG    | lean_dojo.data_extraction.traced_data:check_sanity:1032 - Checking the
 sanity of TracedRepo(repo=LeanGitRepo(url='https://github.com/Adarsh321123/new-version-test', commit='f465
306be03ced999caa157a85558a6c41b3e3f5'), dependencies={'lean4': LeanGitRepo(url='https://github.com/leanprov
er/lean4', commit='daa22187642d4cf6954c39a23eab20d8a8675416'), 'batteries': LeanGitRepo(url='https://github
.com/leanprover-community/batteries', commit='dc167d260ff7ee9849b436037add06bed15104be'), 'Qq': LeanGitRepo
(url='https://github.com/leanprover-community/quote4', commit='71f54425e6fe0fa75f3aef33a2813a7898392222'),
'aesop': LeanGitRepo(url='https://github.com/leanprover-community/aesop', commit='0444234b4216e944d5be2ce42
a25d7410c67876f'), 'proofwidgets': LeanGitRepo(url='https://github.com/leanprover-community/ProofWidgets4',
 commit='a96aee5245720f588876021b6a0aa73efee49c76'), 'Cli': LeanGitRepo(url='https://github.com/leanprover/
lean4-cli', commit='2cf1030dc2ae6b3632c84a09350b675ef3e347d0'), 'mathlib': LeanGitRepo(url='https://github.
com/leanprover-community/mathlib4', commit='3dd071bc2260b3cf9a71863d0dee1242fec41522'), 'importGraph': Lean
GitRepo(url='https://github.com/leanprover-community/import-graph', commit='57bd2065f1dbea5e9235646fb836c7c
ea9ab03b6')}, root_dir=PosixPath('/raid/adarsh/.cache/lean_dojo/Adarsh321123-new-version-test-f465306be03ce
d999caa157a85558a6c41b3e3f5/new-version-test'))
2024-08-10 13:17:54.823 | INFO     | generate_benchmark_lean4:main:344 - Successfully traced the repo
2024-08-10 13:18:06.842 | INFO     | generate_benchmark_lean4:split_data:132 - 119751 theorems in total
2024-08-10 13:18:06.842 | INFO     | generate_benchmark_lean4:split_data:137 - 119751 theorems in total, wi
th 2395 for validation and 2395 for testing
2024-08-10 13:18:06.842 | INFO     | generate_benchmark_lean4:split_randomly:80 - Splitting the theorems ra
ndomly
2024-08-10 13:18:06.890 | INFO     | generate_benchmark_lean4:split_by_premise:94 - Splitting the theorems
by premises
2024-08-10 13:18:17.661 | INFO     | generate_benchmark_lean4:main:352 - Successfully split the data
2024-08-10 13:18:46.175 | INFO     | generate_benchmark_lean4:export_proofs:194 - 114961 theorems and 24235
9 tactics saved to /raid/adarsh/datasets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5/ran
dom/train.json
2024-08-10 13:18:47.300 | INFO     | generate_benchmark_lean4:export_proofs:194 - 2395 theorems and 5403 ta
ctics saved to /raid/adarsh/datasets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5/random/
val.json
2024-08-10 13:18:47.977 | INFO     | generate_benchmark_lean4:export_proofs:194 - 2395 theorems and 5321 ta
ctics saved to /raid/adarsh/datasets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5/random/
test.json
2024-08-10 13:19:17.113 | INFO     | generate_benchmark_lean4:export_proofs:194 - 114961 theorems and 23794
6 tactics saved to /raid/adarsh/datasets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5/nov
el_premises/train.json
2024-08-10 13:19:18.266 | INFO     | generate_benchmark_lean4:export_proofs:194 - 2395 theorems and 7436 ta
ctics saved to /raid/adarsh/datasets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5/novel_p
remises/val.json
2024-08-10 13:19:19.105 | INFO     | generate_benchmark_lean4:export_proofs:194 - 2395 theorems and 7701 ta
ctics saved to /raid/adarsh/datasets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5/novel_p
remises/test.json
2024-08-10 13:19:19.113 | INFO     | generate_benchmark_lean4:export_data:270 - Successfully exported the p
roofs
2024-08-10 13:19:55.611 | INFO     | generate_benchmark_lean4:export_premises:218 - 175685 theorems/definit
ions from 5432 files saved to /raid/adarsh/datasets_test/new-version-test_f465306be03ced999caa157a85558a6c4
1b3e3f5/corpus.jsonl
2024-08-10 13:19:55.612 | INFO     | generate_benchmark_lean4:export_data:274 - Successfully exported the p
remises
2024-08-10 13:19:55.613 | INFO     | generate_benchmark_lean4:export_data:282 - Successfully exported the m
etadata
2024-08-10 13:19:55.613 | INFO     | generate_benchmark_lean4:main:354 - Successfully exported the data
2024-08-10 13:19:55.906 | INFO     | __main__:generate_dataset:246 - Finished generating benchmark at /raid
/adarsh/datasets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5
2024-08-10 13:19:55.906 | INFO     | __main__:generate_dataset:247 - Merging datasets
2024-08-10 13:19:55.906 | INFO     | __main__:merge_datasets:74 - Merging datasets for random
2024-08-10 13:19:55.907 | INFO     | __main__:merge_datasets:80 - Processing train split
2024-08-10 13:19:55.907 | INFO     | __main__:merge_datasets:86 - Processing new-version-test_f465306be03ce
d999caa157a85558a6c41b3e3f5
2024-08-10 13:19:58.857 | INFO     | __main__:merge_datasets:98 - Deleted processed file: /raid/adarsh/data
sets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5/random/train.json
2024-08-10 13:20:07.759 | INFO     | __main__:merge_datasets:104 - Finished processing train split
2024-08-10 13:20:07.759 | INFO     | __main__:merge_datasets:80 - Processing val split
2024-08-10 13:20:07.771 | INFO     | __main__:merge_datasets:86 - Processing new-version-test_f465306be03ce
d999caa157a85558a6c41b3e3f5
2024-08-10 13:20:08.170 | INFO     | __main__:merge_datasets:98 - Deleted processed file: /raid/adarsh/data
sets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5/random/val.json
2024-08-10 13:20:08.363 | INFO     | __main__:merge_datasets:104 - Finished processing val split
2024-08-10 13:20:08.364 | INFO     | __main__:merge_datasets:80 - Processing test split
2024-08-10 13:20:08.364 | INFO     | __main__:merge_datasets:86 - Processing new-version-test_f465306be03ce
d999caa157a85558a6c41b3e3f5
2024-08-10 13:20:08.428 | INFO     | __main__:merge_datasets:98 - Deleted processed file: /raid/adarsh/data
sets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5/random/test.json
2024-08-10 13:20:08.617 | INFO     | __main__:merge_datasets:104 - Finished processing test split
2024-08-10 13:20:08.617 | INFO     | __main__:merge_datasets:105 - Finished merging datasets for random
2024-08-10 13:20:08.617 | INFO     | __main__:merge_datasets:74 - Merging datasets for novel_premises
2024-08-10 13:20:08.617 | INFO     | __main__:merge_datasets:80 - Processing train split
2024-08-10 13:20:08.617 | INFO     | __main__:merge_datasets:86 - Processing new-version-test_f465306be03ce
d999caa157a85558a6c41b3e3f5
2024-08-10 13:20:11.494 | INFO     | __main__:merge_datasets:98 - Deleted processed file: /raid/adarsh/data
sets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5/novel_premises/train.json
2024-08-10 13:20:20.257 | INFO     | __main__:merge_datasets:104 - Finished processing train split
2024-08-10 13:20:20.258 | INFO     | __main__:merge_datasets:80 - Processing val split
2024-08-10 13:20:20.268 | INFO     | __main__:merge_datasets:86 - Processing new-version-test_f465306be03ce
d999caa157a85558a6c41b3e3f5
2024-08-10 13:20:20.659 | INFO     | __main__:merge_datasets:98 - Deleted processed file: /raid/adarsh/data
sets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5/novel_premises/val.json
2024-08-10 13:20:20.906 | INFO     | __main__:merge_datasets:104 - Finished processing val split
2024-08-10 13:20:20.906 | INFO     | __main__:merge_datasets:80 - Processing test split
2024-08-10 13:20:20.906 | INFO     | __main__:merge_datasets:86 - Processing new-version-test_f465306be03ce
d999caa157a85558a6c41b3e3f5
2024-08-10 13:20:20.981 | INFO     | __main__:merge_datasets:98 - Deleted processed file: /raid/adarsh/data
sets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5/novel_premises/test.json
2024-08-10 13:20:21.251 | INFO     | __main__:merge_datasets:104 - Finished processing test split
2024-08-10 13:20:21.251 | INFO     | __main__:merge_datasets:105 - Finished merging datasets for novel_prem
ises
2024-08-10 13:20:21.251 | INFO     | __main__:merge_datasets:107 - Merging corpus
2024-08-10 13:20:21.251 | INFO     | __main__:merge_datasets:112 - Processing new-version-test_f465306be03c
ed999caa157a85558a6c41b3e3f5
2024-08-10 13:20:21.695 | INFO     | __main__:merge_datasets:124 - Deleted processed corpus file: /raid/ada
rsh/datasets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5/corpus.jsonl
2024-08-10 13:20:21.762 | INFO     | __main__:merge_datasets:130 - Finished merging corpus
2024-08-10 13:20:21.763 | INFO     | __main__:merge_datasets:132 - Adding metadata
2024-08-10 13:20:21.763 | INFO     | __main__:merge_datasets:137 - Checking for metadata in new-version-tes
t_f465306be03ced999caa157a85558a6c41b3e3f5
2024-08-10 13:20:21.763 | INFO     | __main__:merge_datasets:145 - Deleted processed metadata file: /raid/a
darsh/datasets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5/metadata.json
2024-08-10 13:20:21.763 | INFO     | __main__:merge_datasets:150 - Finished adding metadata
2024-08-10 13:20:21.763 | INFO     | __main__:merge_datasets:154 - Deleting individual datasets
2024-08-10 13:20:21.763 | INFO     | __main__:merge_datasets:158 - Deleting dataset: new-version-test_f4653
06be03ced999caa157a85558a6c41b3e3f5
2024-08-10 13:20:21.773 | INFO     | __main__:generate_dataset:249 - Finished merging datasets
2024-08-10 13:20:21.909 | INFO     | __main__:main:635 - Latest PL checkpoint found: AK123321/pl-leancopilo
t-2
2024-08-10 13:20:22.007 | INFO     | __main__:download_pl_checkpoint:274 - Checkpoint downloaded to: /home/
adarsh/.cache/huggingface/hub/models--AK123321--pl-leancopilot-2/snapshots/d4955ed1972ad84c03dce9189162d039
12f2d86f/model.ckpt
2024-08-10 13:20:22.007 | INFO     | __main__:main:638 - Checkpoint path: /home/adarsh/.cache/huggingface/h
ub/models--AK123321--pl-leancopilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86f/model.ckpt
2024-08-10 13:20:22.007 | INFO     | __main__:train:464 - Training model with checkpoint: /home/adarsh/.cac
he/huggingface/hub/models--AK123321--pl-leancopilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86f/mo
del.ckpt
Seed set to 3407
Lightning automatically upgraded your loaded checkpoint from v0.0.0 to v2.4.0. To apply the upgrade to your
 files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../.cache/huggingface/hub
/models--AK123321--pl-leancopilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86f/model.ckpt`
2024-08-10 13:20:56.366 | INFO     | __main__:train:486 - Loaded premise retriever at /home/adarsh/.cache/h
uggingface/hub/models--AK123321--pl-leancopilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86f/model.
ckpt
2024-08-10 13:20:56.367 | INFO     | __main__:train:490 - Data path: /raid/adarsh/datasets_test/merged/rand
om
/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/transformers/tokenization_u
tils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by d
efault. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default.
For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
2024-08-10 13:20:56.467 | INFO     | common:__init__:199 - Building the corpus from /raid/adarsh/datasets_t
est/merged/corpus.jsonl
100%|████████████████████████████████████████████████████████████| 114961/114961 [00:15<00:00, 7314.80it/s]
2024-08-10 13:21:39.149 | INFO     | retrieval.datamodule:load_or_cache_data:61 - Saved loaded data to cach
e /raid/adarsh/datasets_test/merged/random/cache_train/cached_data.pkl
Training dataset size: 353052
100%|████████████████████████████████████████████████████████████████| 2395/2395 [00:00<00:00, 6889.62it/s]
2024-08-10 13:21:39.601 | INFO     | retrieval.datamodule:load_or_cache_data:61 - Saved loaded data to cach
e /raid/adarsh/datasets_test/merged/random/cache_val/cached_data.pkl
Validation dataset size: 5403
100%|████████████████████████████████████████████████████████████████| 2395/2395 [00:00<00:00, 7248.54it/s]
2024-08-10 13:21:40.027 | INFO     | retrieval.datamodule:load_or_cache_data:61 - Saved loaded data to cach
e /raid/adarsh/datasets_test/merged/random/cache_pred/cached_data.pkl
Testing dataset size: 5321
2024-08-10 13:21:40.030 | INFO     | __main__:train:523 - Training dataset size after load: 353052
2024-08-10 13:21:40.030 | INFO     | __main__:train:524 - Validation dataset size after load: 5403
2024-08-10 13:21:40.030 | INFO     | __main__:train:525 - Testing dataset size after load: 5321
Using bfloat16 Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
2024-08-10 13:21:40.042 | INFO     | __main__:train:543 - Starting progressive training...
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 1 processes
----------------------------------------------------------------------------------------------------

2024-08-10 13:21:56.028 | INFO     | retrieval.datamodule:load_or_cache_data:52 - Loaded data from cache /r
aid/adarsh/datasets_test/merged/random/cache_train/cached_data.pkl
Training dataset size: 353052
2024-08-10 13:21:56.263 | INFO     | retrieval.datamodule:load_or_cache_data:52 - Loaded data from cache /r
aid/adarsh/datasets_test/merged/random/cache_val/cached_data.pkl
Validation dataset size: 5403
2024-08-10 13:21:56.305 | INFO     | retrieval.datamodule:load_or_cache_data:52 - Loaded data from cache /r
aid/adarsh/datasets_test/merged/random/cache_pred/cached_data.pkl
Testing dataset size: 5321
/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning/callbacks
/model_checkpoint.py:654: Checkpoint directory /raid/adarsh/checkpoints_test exists and is not empty.
Restoring states from the checkpoint path at /home/adarsh/.cache/huggingface/hub/models--AK123321--pl-leanc
opilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86f/model.ckpt
Lightning automatically upgraded your loaded checkpoint from v0.0.0 to v2.4.0. To apply the upgrade to your
 files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../.cache/huggingface/hub
/models--AK123321--pl-leancopilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86f/model.ckpt`
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
2024-08-10 13:21:58.522 | INFO     | common:get_optimizers:432 - Optimizing with AdamW

  | Name    | Type           | Params | Mode
---------------------------------------------------
0 | encoder | T5EncoderModel | 217 M  | train
---------------------------------------------------
217 M     Trainable params
0         Non-trainable params
217 M     Total params
870.630   Total estimated model params size (MB)
235       Modules in train mode
0         Modules in eval mode
Restored all states from the checkpoint at /home/adarsh/.cache/huggingface/hub/models--AK123321--pl-leancop
ilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86f/model.ckpt
Epoch 0:   0%|                                                                  | 0/176526 [00:00<?, ?it/s]
^Z
[4]+  Stopped                 bash run_compute_server.sh
(base) adarsh@tensorlab-DGX-Station-A100-920-23487-2531-000:~/ReProver$ bash run_compute_server.sh
Script executed from: /home/adarsh/ReProver
[2024-08-10 13:22:21,068] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (
auto detect)
 [WARNING]  async_io requires the dev libaio .so object and headers but these were not found.
 [WARNING]  async_io: please install the libaio-dev package with apt
 [WARNING]  If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS enviro
nment variables to where it can be found.
 [WARNING]  Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 [WARNING]  sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
 [WARNING]  using untested triton version (3.0.0), only 1.0.0 is known to be compatible
/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/deepspeed/runtime/zero/line
ar.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_f
wd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/deepspeed/runtime/zero/line
ar.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_b
wd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
2024-08-10 13:22:33.552 | INFO     | __main__:check_progress_file:586 - Checking contents of /home/adarsh/m
iniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning/loops/progress.py
2024-08-10 13:22:33.552 | INFO     | __main__:check_progress_file:590 - Contents of progress.py:
2024-08-10 13:22:33.552 | INFO     | __main__:check_progress_file:591 - # Copyright The Lightning AI team.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import asdict, dataclass, field
from typing import Type

from typing_extensions import override


@dataclass
class _BaseProgress:
    """Mixin that implements state-loading utilities for dataclasses."""

    def state_dict(self) -> dict:
        return asdict(self)

    def load_state_dict(self, state_dict: dict) -> None:
        if state_dict["completed"] == None:
            state_dict["completed"] = 0
        self.__dict__.update(state_dict)

    @classmethod
    def from_state_dict(cls, state_dict: dict) -> "_BaseProgress":
        obj = cls()
        obj.load_state_dict(state_dict)
        return obj

    def reset(self) -> None:
        """Reset the object's state."""
        raise NotImplementedError


@dataclass
class _ReadyCompletedTracker(_BaseProgress):
    """Track an event's progress.

    Args:
        ready: Intended to track the number of events ready to start.
        completed: Intended to be incremented after the event completes (e.g. after ``on_*_end`` runs).

    These attributes should be increased in order, that is, :attr:`ready` first and :attr:`completed` last.

    """

    ready: int = 0
    completed: int = 0

    @override
    def reset(self) -> None:
        """Reset the state."""
        self.ready = 0
        self.completed = 0

    def reset_on_restart(self) -> None:
        """Reset the progress on restart.

        If there is a failure before all attributes are increased, restore the attributes to the last fully
 completed
        value.

        """
        self.ready = self.completed


@dataclass
class _StartedTracker(_ReadyCompletedTracker):
    """Track an event's progress.

    Args:
        ready: Intended to track the number of events ready to start.
        started: Intended to be incremented after the event is started (e.g. after ``on_*_start`` runs).
        completed: Intended to be incremented after the event completes (e.g. after ``on_*_end`` runs).

    These attributes should be increased in order, that is, :attr:`ready` first and :attr:`completed` last.

    """

    started: int = 0

    @override
    def reset(self) -> None:
        super().reset()
        self.started = 0

    @override
    def reset_on_restart(self) -> None:
        super().reset_on_restart()
        self.started = self.completed


@dataclass
class _ProcessedTracker(_StartedTracker):
    """Track an event's progress.

    Args:
        ready: Intended to track the number of events ready to start.
        started: Intended to be incremented after the event is started (e.g. after ``on_*_start`` runs).
        processed: Intended to be incremented after the event is processed.
        completed: Intended to be incremented after the event completes (e.g. after ``on_*_end`` runs).

    These attributes should be increased in order, that is, :attr:`ready` first and :attr:`completed` last.

    """

    processed: int = 0

    @override
    def reset(self) -> None:
        super().reset()
        self.processed = 0

    @override
    def reset_on_restart(self) -> None:
        super().reset_on_restart()
        self.processed = self.completed


@dataclass
class _Progress(_BaseProgress):
    """Track aggregated and current progress.

    Args:
        total: Intended to track the total progress of an event.
        current: Intended to track the current progress of an event.

    """

    total: _ReadyCompletedTracker = field(default_factory=_ProcessedTracker)
    current: _ReadyCompletedTracker = field(default_factory=_ProcessedTracker)

    def __post_init__(self) -> None:
        if self.total.__class__ is not self.current.__class__:
            raise ValueError("The `total` and `current` instances should be of the same class")

    def increment_ready(self) -> None:
        self.total.ready += 1
        self.current.ready += 1

    def increment_started(self) -> None:
        if not isinstance(self.total, _StartedTracker):
            raise TypeError(f"`{self.total.__class__.__name__}` doesn't have a `started` attribute")
        self.total.started += 1
        self.current.started += 1

    def increment_processed(self) -> None:
        if not isinstance(self.total, _ProcessedTracker):
            raise TypeError(f"`{self.total.__class__.__name__}` doesn't have a `processed` attribute")
        self.total.processed += 1
        self.current.processed += 1

    def increment_completed(self) -> None:
        self.total.completed += 1
        self.current.completed += 1

    @classmethod
    def from_defaults(cls, tracker_cls: Type[_ReadyCompletedTracker], **kwargs: int) -> "_Progress":
        """Utility function to easily create an instance from keyword arguments to both ``Tracker``s."""
        return cls(total=tracker_cls(**kwargs), current=tracker_cls(**kwargs))

    @override
    def reset(self) -> None:
        self.total.reset()
        self.current.reset()

    def reset_on_run(self) -> None:
        self.current.reset()

    def reset_on_restart(self) -> None:
        self.current.reset_on_restart()

    @override
    def load_state_dict(self, state_dict: dict) -> None:
        if state_dict["total"]["completed"] == None:
            state_dict["total"]["completed"] = 0
        self.total.load_state_dict(state_dict["total"])
        self.current.load_state_dict(state_dict["current"])


@dataclass
class _BatchProgress(_Progress):
    """Tracks batch progress.

    These counters are local to a trainer rank. By default, they are not globally synced across all ranks.

    Args:
        total: Tracks the total batch progress.
        current: Tracks the current batch progress.
        is_last_batch: Whether the batch is the last one. This is useful for iterable datasets.

    """

    is_last_batch: bool = False

    @override
    def reset(self) -> None:
        super().reset()
        self.is_last_batch = False

    @override
    def reset_on_run(self) -> None:
        super().reset_on_run()
        self.is_last_batch = False

    @override
    def load_state_dict(self, state_dict: dict) -> None:
        if state_dict["total"]["completed"] == None:
            state_dict["total"]["completed"] = 0
        super().load_state_dict(state_dict)
        self.is_last_batch = state_dict["is_last_batch"]


@dataclass
class _SchedulerProgress(_Progress):
    """Tracks scheduler progress.

    These counters are local to a trainer rank. By default, they are not globally synced across all ranks.

    Args:
        total: Tracks the total scheduler progress.
        current: Tracks the current scheduler progress.

    """

    total: _ReadyCompletedTracker = field(default_factory=_ReadyCompletedTracker)
    current: _ReadyCompletedTracker = field(default_factory=_ReadyCompletedTracker)


@dataclass
class _OptimizerProgress(_BaseProgress):
    """Track optimizer progress.

    Args:
        step: Tracks ``optimizer.step`` calls.
        zero_grad: Tracks ``optimizer.zero_grad`` calls.

    """

    step: _Progress = field(default_factory=lambda: _Progress.from_defaults(_ReadyCompletedTracker))
    zero_grad: _Progress = field(default_factory=lambda: _Progress.from_defaults(_StartedTracker))

    @override
    def reset(self) -> None:
        self.step.reset()
        self.zero_grad.reset()

    def reset_on_run(self) -> None:
        self.step.reset_on_run()
        self.zero_grad.reset_on_run()

    def reset_on_restart(self) -> None:
        self.step.reset_on_restart()
        self.zero_grad.reset_on_restart()

    @override
    def load_state_dict(self, state_dict: dict) -> None:
        if state_dict["step"]["total"]["completed"] == None:
            state_dict["step"]["total"]["completed"] = 0
        self.step.load_state_dict(state_dict["step"])
        self.zero_grad.load_state_dict(state_dict["zero_grad"])


@dataclass
class _OptimizationProgress(_BaseProgress):
    """Track optimization progress.

    Args:
        optimizer: Tracks optimizer progress.

    """

    optimizer: _OptimizerProgress = field(default_factory=_OptimizerProgress)

    @property
    def optimizer_steps(self) -> int:
        return self.optimizer.step.total.completed

    @override
    def reset(self) -> None:
        self.optimizer.reset()

    def reset_on_run(self) -> None:
        self.optimizer.reset_on_run()

    def reset_on_restart(self) -> None:
        self.optimizer.reset_on_restart()

    @override
    def load_state_dict(self, state_dict: dict) -> None:
        if state_dict["optimizer"]["step"]["total"]["completed"] == None:
            state_dict["optimizer"]["step"]["total"]["completed"] = 0
        self.optimizer.load_state_dict(state_dict["optimizer"])

2024-08-10 13:22:33.552 | INFO     | __main__:main:608 - Starting compute server...
2024-08-10 13:22:33.553 | INFO     | __main__:main:609 - Current working directory: /home/adarsh/ReProver
2024-08-10 13:22:33.553 | INFO     | __main__:main:613 - ROOT_DIR: /raid/adarsh
2024-08-10 13:22:33.553 | INFO     | __main__:main:614 - DATA_DIR: datasets_test
2024-08-10 13:22:33.553 | INFO     | __main__:main:618 - Configuring LeanDojo...
2024-08-10 13:22:33.556 | INFO     | generate_benchmark_lean4:configure_leandojo:294 - Current working dire
ctory: /home/adarsh/ReProver
2024-08-10 13:22:33.556 | INFO     | __main__:main:620 - LeanDojo configured
2024-08-10 13:22:33.556 | INFO     | __main__:main:628 - Unique URLs: {'https://github.com/teorth/pfr.git'}
2024-08-10 13:22:33.556 | INFO     | __main__:main:629 - About to generate datasets...
2024-08-10 13:22:33.556 | INFO     | __main__:generate_dataset:226 - Generating 1 datasets
2024-08-10 13:22:33.556 | INFO     | __main__:generate_dataset:231 - Processing https://github.com/teorth/p
fr.git
2024-08-10 13:22:33.776 | INFO     | __main__:get_compatible_commit:166 - Latest commit: 6a5082ee465f9e44ce
a479c7b741b3163162bb7e
2024-08-10 13:22:33.776 | INFO     | __main__:get_compatible_commit:169 - Creating LeanGitRepo for https://
github.com/teorth/pfr
2024-08-10 13:22:34.182 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the commi
t hash for lean4 v4.8.0-rc1
2024-08-10 13:22:34.795 | INFO     | __main__:get_compatible_commit:171 - Getting config for https://github
.com/teorth/pfr.git
2024-08-10 13:22:34.795 | INFO     | __main__:get_compatible_commit:175 - Latest commit compatible for url
https://github.com/teorth/pfr.git
2024-08-10 13:22:34.795 | INFO     | __main__:generate_dataset:236 - Found compatible commit 6a5082ee465f9e
44cea479c7b741b3163162bb7e for https://github.com/teorth/pfr.git
2024-08-10 13:22:34.795 | INFO     | __main__:generate_dataset:237 - Lean version: v4.8.0-rc1
2024-08-10 13:22:34.795 | INFO     | __main__:generate_dataset:240 - Creating LeanGitRepo for https://githu
b.com/teorth/pfr
2024-08-10 13:22:34.795 | INFO     | __main__:generate_dataset:244 - Generating benchmark at /raid/adarsh/d
atasets_test/pfr_6a5082ee465f9e44cea479c7b741b3163162bb7e
2024-08-10 13:22:34.795 | INFO     | generate_benchmark_lean4:main:301 - Generating dataset to go into /rai
d/adarsh/datasets_test/pfr_6a5082ee465f9e44cea479c7b741b3163162bb7e
2024-08-10 13:22:34.795 | INFO     | generate_benchmark_lean4:main:308 - lean toolchain version: {'content'
: 'leanprover/lean4:v4.8.0-rc1\n'}
2024-08-10 13:22:34.796 | INFO     | generate_benchmark_lean4:main:310 - lean version v: v4.8.0-rc1
2024-08-10 13:22:34.796 | INFO     | generate_benchmark_lean4:main:311 - is supported: True
2024-08-10 13:22:34.796 | INFO     | generate_benchmark_lean4:main:321 - lean path1 /home/adarsh/.elan/tool
chains/leanprover--lean4---4.8.0-rc1
2024-08-10 13:22:34.796 | INFO     | generate_benchmark_lean4:main:322 - lean path2 /.elan/toolchains/leanp
rover--lean4---4.8.0-rc1
2024-08-10 13:22:34.796 | INFO     | generate_benchmark_lean4:main:323 - lean path3 ~/.elan/toolchains/lean
prover--lean4---4.8.0-rc1
2024-08-10 13:22:34.796 | INFO     | generate_benchmark_lean4:main:327 - Lean toolchain path 2 does not exi
st: /.elan/toolchains/leanprover--lean4---4.8.0-rc1
2024-08-10 13:22:34.796 | INFO     | generate_benchmark_lean4:main:329 - Lean toolchain path 3 does not exi
st: ~/.elan/toolchains/leanprover--lean4---4.8.0-rc1
2024-08-10 13:22:34.796 | INFO     | generate_benchmark_lean4:main:332 - Switched to Lean toolchain at: /ho
me/adarsh/.elan/toolchains/leanprover--lean4---4.8.0-rc1
2024-08-10 13:22:34.829 | INFO     | generate_benchmark_lean4:main:334 - lean --version: Lean (version 4.8.
0-rc1, x86_64-unknown-linux-gnu, commit dcccfb73cb24, Release)

2024-08-10 13:22:34.829 | INFO     | generate_benchmark_lean4:main:335 - repo: LeanGitRepo(url='https://git
hub.com/teorth/pfr', commit='6a5082ee465f9e44cea479c7b741b3163162bb7e')
2024-08-10 13:22:34.829 | INFO     | generate_benchmark_lean4:main:337 - Configuring LeanDojo again...
2024-08-10 13:22:34.832 | INFO     | generate_benchmark_lean4:configure_leandojo:294 - Current working dire
ctory: /home/adarsh/ReProver
2024-08-10 13:22:34.832 | INFO     | generate_benchmark_lean4:main:339 - LeanDojo configured
2024-08-10 13:22:34.833 | INFO     | generate_benchmark_lean4:main:342 - Tracing the repo...
2024-08-10 13:22:34.833 | DEBUG    | lean_dojo.data_extraction.trace:get_traced_repo_path:217 - The traced
repo is available in the cache.
2024-08-10 13:22:34.833 | INFO     | lean_dojo.data_extraction.trace:trace:246 - Loading the traced repo fr
om /raid/adarsh/.cache/lean_dojo/teorth-pfr-6a5082ee465f9e44cea479c7b741b3163162bb7e/pfr
2024-08-10 13:22:34.835 | DEBUG    | lean_dojo.utils:execute:110 - git remote get-url origin
2024-08-10 13:22:34.838 | DEBUG    | lean_dojo.utils:execute:110 - git log -n 1
2024-08-10 13:22:34.965 | DEBUG    | lean_dojo.data_extraction.traced_data:load_from_disk:1169 - Loading 28
90 traced XML files from /raid/adarsh/.cache/lean_dojo/teorth-pfr-6a5082ee465f9e44cea479c7b741b3163162bb7e/
pfr with 31 workers
2024-08-10 13:22:36,303 INFO worker.py:1772 -- Started a local Ray instance. View the dashboard at 127.0.0.
1:8267
  3%|██▎                                                                 | 99/2890 [00:05<01:02, 44.33it/s]
(raylet) [2024-08-10 13:22:46,199 E 4011078 4011108] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-22-35_117947_4010085 is over 95% full, available space: 93581070336; capacity: 1887507697664.
 Object creation will fail if spilling is required.
 14%|█████████▏                                                         | 397/2890 [00:18<03:46, 11.01it/s]
(raylet) [2024-08-10 13:22:56,205 E 4011078 4011108] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-22-35_117947_4010085 is over 95% full, available space: 93581041664; capacity: 1887507697664.
 Object creation will fail if spilling is required.
 21%|█████████████▊                                                     | 594/2890 [00:23<00:27, 83.50it/s]
(raylet) [2024-08-10 13:23:06,210 E 4011078 4011108] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-22-35_117947_4010085 is over 95% full, available space: 93581033472; capacity: 1887507697664.
 Object creation will fail if spilling is required.
 31%|████████████████████▉                                              | 901/2890 [00:35<00:28, 69.28it/s]
(raylet) [2024-08-10 13:23:16,215 E 4011078 4011108] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-22-35_117947_4010085 is over 95% full, available space: 93581025280; capacity: 1887507697664.
 Object creation will fail if spilling is required.
 34%|███████████████████████                                            | 997/2890 [00:44<00:49, 38.15it/s]
(raylet) [2024-08-10 13:23:26,220 E 4011078 4011108] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-22-35_117947_4010085 is over 95% full, available space: 93581004800; capacity: 1887507697664.
 Object creation will fail if spilling is required.
 38%|█████████████████████████                                         | 1098/2890 [01:05<14:26,  2.07it/s]
(raylet) [2024-08-10 13:23:36,224 E 4011078 4011108] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-22-35_117947_4010085 is over 95% full, available space: 93580939264; capacity: 1887507697664.
 Object creation will fail if spilling is required.
 39%|█████████████████████████▋                                        | 1126/2890 [01:06<02:45, 10.68it/s]
(raylet) [2024-08-10 13:23:46,229 E 4011078 4011108] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-22-35_117947_4010085 is over 95% full, available space: 93580070912; capacity: 1887507697664.
 Object creation will fail if spilling is required.
 41%|███████████████████████████▎                                      | 1198/2890 [01:20<00:38, 44.11it/s]
(raylet) [2024-08-10 13:23:56,233 E 4011078 4011108] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-22-35_117947_4010085 is over 95% full, available space: 93580058624; capacity: 1887507697664.
 Object creation will fail if spilling is required.
 47%|██████████████████████████████▊                                   | 1350/2890 [01:40<24:47,  1.04it/s]
(raylet) [2024-08-10 13:24:06,238 E 4011078 4011108] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-22-35_117947_4010085 is over 95% full, available space: 93580042240; capacity: 1887507697664.
 Object creation will fail if spilling is required.
 53%|███████████████████████████████████▎                              | 1544/2890 [02:04<00:23, 56.59it/s]
(raylet) [2024-08-10 13:24:26,247 E 4011078 4011108] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-22-35_117947_4010085 is over 95% full, available space: 93580013568; capacity: 1887507697664.
 Object creation will fail if spilling is required. [repeated 2x across cluster] (Ray deduplicates logs by
default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observ
ability/user-guides/configure-logging.html#log-deduplication for more options.)
 60%|███████████████████████████████████████▎                          | 1720/2890 [02:35<33:46,  1.73s/it]
(raylet) [2024-08-10 13:24:56,261 E 4011078 4011108] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-22-35_117947_4010085 is over 95% full, available space: 93578973184; capacity: 1887507697664.
 Object creation will fail if spilling is required. [repeated 3x across cluster]
 71%|██████████████████████████████████████████████▊                   | 2052/2890 [03:12<00:30, 27.75it/s]
(raylet) [2024-08-10 13:25:26,276 E 4011078 4011108] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-22-35_117947_4010085 is over 95% full, available space: 93578919936; capacity: 1887507697664.
 Object creation will fail if spilling is required. [repeated 3x across cluster]
 80%|████████████████████████████████████████████████████▋             | 2308/2890 [03:58<32:25,  3.34s/it]
(raylet) [2024-08-10 13:26:06,294 E 4011078 4011108] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-22-35_117947_4010085 is over 95% full, available space: 93577297920; capacity: 1887507697664.
 Object creation will fail if spilling is required. [repeated 4x across cluster]
 85%|████████████████████████████████████████████████████████▏         | 2462/2890 [04:05<00:16, 26.06it/s]
(raylet) [2024-08-10 13:26:46,315 E 4011078 4011108] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-22-35_117947_4010085 is over 95% full, available space: 93576396800; capacity: 1887507697664.
 Object creation will fail if spilling is required. [repeated 4x across cluster]
(raylet) [2024-08-10 13:26:56,319 E 4011078 4011108] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-22-35_117947_4010085 is over 95% full, available space: 93576392704; capacity: 1887507697664.
 Object creation will fail if spilling is required.
 85%|████████████████████████████████████████████████████████▏         | 2462/2890 [04:18<00:16, 26.06it/s]
(raylet) [2024-08-10 13:27:06,324 E 4011078 4011108] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-22-35_117947_4010085 is over 95% full, available space: 93576368128; capacity: 1887507697664.
 Object creation will fail if spilling is required.
(raylet) [2024-08-10 13:27:16,330 E 4011078 4011108] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-22-35_117947_4010085 is over 95% full, available space: 93576359936; capacity: 1887507697664.
 Object creation will fail if spilling is required.
(raylet) [2024-08-10 13:27:26,336 E 4011078 4011108] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_13-22-35_117947_4010085 is over 95% full, available space: 93576347648; capacity: 1887507697664.
 Object creation will fail if spilling is required.
 85%|████████████████████████████████████████████████████████▎         | 2467/2890 [04:51<00:50,  8.45it/s]
(raylet) [2024-08-10 13:27:36,207 E 4011078 4011078] (raylet) node_manager.cc:3064: 1 Workers (tasks / acto
rs) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: ad96fa9e8e3608
1c0268204da1a38bb6687bf9652c00b0dc9c3a9352, IP: 131.215.143.185) over the last time period. To see more inf
ormation about the Workers killed on this node, use `ray logs raylet.out -ip 131.215.143.185`
(raylet)
(raylet) Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/lates
t/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing t
ask parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variabl
e `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `
RAY_memory_monitor_refresh_ms` to zero.
2024-08-10 13:27:42.313 | INFO     | generate_benchmark_lean4:main:346 - Failed to trace repo LeanGitRepo(u
rl='https://github.com/teorth/pfr', commit='6a5082ee465f9e44cea479c7b741b3163162bb7e') because of Task was
killed due to the node running low on memory.
Memory on the node (IP: 131.215.143.185, ID: ad96fa9e8e36081c0268204da1a38bb6687bf9652c00b0dc9c3a9352) wher
e the task (actor ID: 3079fe725b49869d69639ad201000000, name=_TracedRepoHelper.__init__, pid=4018530, memor
y used=0.13GB) was running was 478.57GB / 503.73GB (0.950057), which exceeds the memory usage threshold of
0.95. Ray killed this worker (ID: 42310a93c5adab4c99af10077426e401042b9349c61a723bf807f9d8) because it was
the most recently scheduled task; to see more information about memory usage on this node, use `ray logs ra
ylet.out -ip 131.215.143.185`. To see the logs of the worker, use `ray logs worker-42310a93c5adab4c99af1007
7426e401042b9349c61a723bf807f9d8*out -ip 131.215.143.185. Top 10 memory users:
PID     MEM(GB) COMMAND
1312425 28.10   python /home/adarsh/ReProver/main.py
4010085 12.14   python /home/adarsh/ReProver/compute_server.py
3412529 6.34    /home/adarsh/.elan/toolchains/leanprover--lean4---v4.7.0/bin/lean --server /home/adarsh/Put
namBench
1457158 4.30    python /home/adarsh/ReProver/compute_server.py
1192859 3.83    /home/adarsh/.vscode-server/cli/servers/Stable-89de5a8d4d6205e5b11647eb6a74844ca23d2573/ser
ver/node ...
33227   2.97    /home/adarsh/.elan/toolchains/leanprover--lean4---v4.8.0-rc1/bin/lean --worker file:///home
/adarsh/L...
424107  2.75    /home/adarsh/.elan/toolchains/leanprover--lean4---v4.8.0-rc1/bin/lean --worker file:///home
/adarsh/L...
1346581 2.26    /home/adarsh/.elan/toolchains/leanprover--lean4---v4.8.0-rc1/bin/lean --worker file:///home
/adarsh/L...
521587  2.07    /home/adarsh/.elan/toolchains/leanprover--lean4---v4.8.0-rc1/bin/lean --worker file:///home
/adarsh/L...
1435800 1.58    python /home/adarsh/ReProver/compute_server.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-cor
e/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task paral
lelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the tas
k crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshol
d` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_m
s` to zero.
2024-08-10 13:27:42.314 | INFO     | __main__:generate_dataset:246 - Finished generating benchmark at /raid
/adarsh/datasets_test/pfr_6a5082ee465f9e44cea479c7b741b3163162bb7e
2024-08-10 13:27:42.314 | INFO     | __main__:generate_dataset:247 - Merging datasets
2024-08-10 13:27:42.314 | INFO     | __main__:merge_datasets:74 - Merging datasets for random
2024-08-10 13:27:42.314 | INFO     | __main__:merge_datasets:80 - Processing train split
2024-08-10 13:27:42.345 | INFO     | __main__:merge_datasets:104 - Finished processing train split
2024-08-10 13:27:42.345 | INFO     | __main__:merge_datasets:80 - Processing val split
2024-08-10 13:27:42.345 | INFO     | __main__:merge_datasets:104 - Finished processing val split
2024-08-10 13:27:42.345 | INFO     | __main__:merge_datasets:80 - Processing test split
2024-08-10 13:27:42.346 | INFO     | __main__:merge_datasets:104 - Finished processing test split
2024-08-10 13:27:42.346 | INFO     | __main__:merge_datasets:105 - Finished merging datasets for random
2024-08-10 13:27:42.346 | INFO     | __main__:merge_datasets:74 - Merging datasets for novel_premises
2024-08-10 13:27:42.346 | INFO     | __main__:merge_datasets:80 - Processing train split
2024-08-10 13:27:42.383 | INFO     | __main__:merge_datasets:104 - Finished processing train split
2024-08-10 13:27:42.383 | INFO     | __main__:merge_datasets:80 - Processing val split
2024-08-10 13:27:42.384 | INFO     | __main__:merge_datasets:104 - Finished processing val split
2024-08-10 13:27:42.384 | INFO     | __main__:merge_datasets:80 - Processing test split
2024-08-10 13:27:42.386 | INFO     | __main__:merge_datasets:104 - Finished processing test split
2024-08-10 13:27:42.386 | INFO     | __main__:merge_datasets:105 - Finished merging datasets for novel_prem
ises
2024-08-10 13:27:42.386 | INFO     | __main__:merge_datasets:107 - Merging corpus
2024-08-10 13:27:42.392 | INFO     | __main__:merge_datasets:130 - Finished merging corpus
2024-08-10 13:27:42.392 | INFO     | __main__:merge_datasets:132 - Adding metadata
2024-08-10 13:27:42.392 | WARNING  | __main__:merge_datasets:152 - No metadata file found
2024-08-10 13:27:42.392 | INFO     | __main__:merge_datasets:154 - Deleting individual datasets
2024-08-10 13:27:42.392 | INFO     | __main__:generate_dataset:249 - Finished merging datasets
2024-08-10 13:27:42.495 | INFO     | __main__:main:635 - Latest PL checkpoint found: AK123321/pl-leancopilo
t-2
2024-08-10 13:27:42.587 | INFO     | __main__:download_pl_checkpoint:274 - Checkpoint downloaded to: /home/
adarsh/.cache/huggingface/hub/models--AK123321--pl-leancopilot-2/snapshots/d4955ed1972ad84c03dce9189162d039
12f2d86f/model.ckpt
2024-08-10 13:27:42.587 | INFO     | __main__:main:638 - Checkpoint path: /home/adarsh/.cache/huggingface/h
ub/models--AK123321--pl-leancopilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86f/model.ckpt
2024-08-10 13:27:42.587 | INFO     | __main__:train:464 - Training model with checkpoint: /home/adarsh/.cac
he/huggingface/hub/models--AK123321--pl-leancopilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86f/mo
del.ckpt
Seed set to 3407
Lightning automatically upgraded your loaded checkpoint from v0.0.0 to v2.4.0. To apply the upgrade to your
 files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../.cache/huggingface/hub
/models--AK123321--pl-leancopilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86f/model.ckpt`

