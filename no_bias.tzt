        [-0.2248,  0.1532, -0.1966,  ..., -0.3271, -0.2946, -0.1856],
        [ 0.1535,  0.2292,  0.0295,  ...,  0.0128, -0.2862,  0.5315],
        [ 0.0711,  0.0205,  0.1816,  ..., -0.0649,  0.3558,  0.3410]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0572,  0.4688, -0.2918,  ...,  0.1257,  0.0799,  0.2528],
        [-0.3230,  0.3559, -0.1250,  ...,  0.0397, -0.2007,  0.1174],
        [-0.3170, -0.0768,  0.5749,  ..., -0.5142, -0.1652,  0.2295],
        ...,
        [-0.0058,  0.1800,  0.0115,  ..., -0.3106, -0.4688,  0.4927],
        [ 0.2665, -0.3065,  0.4775,  ...,  0.1766,  0.5556,  0.3142],
        [-0.1044,  0.0115,  0.3047,  ...,  0.3341, -0.2085,  0.7893]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-1.6229, -0.7989, -0.1762,  ...,  0.7302, -0.0056, -0.4107],
        [-0.8133,  0.1951,  0.7650,  ...,  0.0983, -0.8194, -0.4034],
        [ 0.0508, -0.0375, -0.5530,  ..., -1.0933, -0.8757, -0.2173],
        ...,
        [-0.0663,  0.3344, -0.2816,  ...,  0.5560,  0.4427,  0.5559],
        [-0.5000,  1.2333,  0.9080,  ...,  0.5852,  0.2781,  0.6077],
        [-0.8129, -0.2898, -0.5408,  ...,  0.2723,  0.2295, -0.0988]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.0737, 0.0949, 0.0527,  ..., 0.0787, 0.0452, 0.0988], device='cuda:0',
       requires_grad=True), Parameter containing:
tensor([[-0.1441, -0.6989,  0.0203,  ..., -0.3199, -0.2050, -0.0984],
        [ 0.2917, -0.0128,  0.3315,  ..., -0.3744,  0.4178,  0.3169],
        [-0.1353,  0.0194, -0.2589,  ..., -0.2124, -0.1394,  0.1254],
        ...,
        [-0.2580, -0.1620, -0.0212,  ...,  0.0404, -0.2567, -0.0198],
        [ 0.2855, -0.1196, -0.0620,  ...,  0.2951, -0.1850,  0.7106],
        [ 0.1893, -0.2171, -0.4357,  ...,  0.0713,  0.0501, -0.1533]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.6357, -0.1994, -0.3425,  ..., -0.5515, -0.3147,  0.8501],
        [-0.7417,  0.4008, -0.8617,  ..., -0.2875,  0.8585,  0.7500],
        [ 0.3856, -0.4907,  0.1627,  ..., -0.0426, -0.4690, -0.0856],
        ...,
        [ 0.7029, -0.0394, -0.0613,  ..., -0.3636, -0.0075,  0.2635],
        [-0.5717,  0.5050,  0.7643,  ..., -0.6229,  0.0815, -0.1127],
        [ 0.7883,  0.4660, -0.4783,  ...,  0.6271, -0.2404,  0.2002]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0801,  0.4506,  0.0284,  ...,  0.3239,  0.0103, -0.3774],
        [-0.2287, -0.1287,  0.6161,  ...,  0.3847,  0.0053,  0.1072],
        [ 0.1642, -0.4020,  0.1451,  ..., -0.0462,  0.1173, -0.0022],
        ...,
        [ 0.0751, -0.1163,  0.4217,  ...,  0.3714,  0.0218, -0.0451],
        [ 0.1151,  0.3072, -0.0107,  ...,  0.0584, -0.1231,  0.3260],
        [ 0.3611, -0.0367,  0.2915,  ..., -0.3073,  0.2915,  0.0498]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.1542, 0.1307, 0.1214,  ..., 0.1359, 0.1457, 0.1467], device='cuda:0',
       requires_grad=True), Parameter containing:
tensor([[ 0.0299,  0.0141,  0.0450,  ..., -0.0111,  0.0408, -0.0405],
        [-0.0533, -0.0420, -0.0588,  ..., -0.0510, -0.0163,  0.0481],
        [ 0.0163, -0.1194, -0.0396,  ...,  0.0350,  0.0704,  0.0993],
        ...,
        [ 0.0254,  0.0954, -0.1116,  ...,  0.0216,  0.0240, -0.0394],
        [-0.0523,  0.0166, -0.1044,  ..., -0.0251,  0.1284, -0.0376],
        [ 0.1119, -0.0345,  0.0425,  ..., -0.1244, -0.0634, -0.1063]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0650, -0.2416,  0.3096,  ...,  0.0849,  0.3642, -0.2868],
        [-0.0065,  0.3962, -0.1026,  ...,  0.0722,  0.2544, -0.2903],
        [-0.0370, -0.2742,  0.0148,  ..., -0.0913,  0.2557,  0.0040],
        ...,
        [ 0.1495,  0.3908, -0.1205,  ...,  0.2065,  0.0075,  0.1839],
        [ 0.3768,  0.0400,  0.1184,  ..., -0.1817,  0.1518, -0.0810],
        [-0.2820,  0.1453, -0.0732,  ...,  0.1679,  0.1350, -0.1489]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.1655, -0.1385, -0.4528,  ..., -0.0760,  0.6173,  0.6571],
        [-0.2924, -0.2485,  0.4310,  ...,  0.0925,  0.4615, -0.2199],
        [-0.6890,  0.4856, -0.0306,  ...,  0.0252,  0.3093,  0.7753],
        ...,
        [-0.2987, -0.9105,  0.0068,  ..., -0.0669,  0.1357, -0.1866],
        [ 0.5730,  0.1304, -0.0356,  ...,  0.3542, -0.2504,  0.5501],
        [-0.0836,  0.2835, -0.1224,  ...,  0.1000,  0.1610, -0.1313]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.6289,  0.2035, -0.1930,  ...,  1.5665,  1.1281, -0.6367],
        [-1.0038,  0.9949, -0.2226,  ..., -0.3668,  0.5547, -0.7030],
        [ 0.6625,  0.3239,  1.2193,  ...,  0.5539, -0.0802, -1.3021],
        ...,
        [-1.2168,  0.4132, -0.1640,  ..., -0.5716,  0.4474, -0.9060],
        [-0.2225, -0.0917, -1.2647,  ..., -0.0139, -0.3167, -1.0787],
        [-1.8028,  0.6774, -0.3207,  ...,  1.0618,  0.6614,  0.2591]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.0855, 0.0999, 0.0523,  ..., 0.0899, 0.0530, 0.1326], device='cuda:0',
       requires_grad=True), Parameter containing:
tensor([[ 0.1253,  0.1114, -0.0479,  ..., -0.0152,  0.0755,  0.0052],
        [-0.3960, -0.2596, -0.1775,  ...,  0.3007, -0.4077,  0.5067],
        [-0.1731,  0.1564,  0.1930,  ...,  0.0721,  0.2071, -0.0501],
        ...,
        [ 0.1273, -0.0310,  0.2682,  ...,  0.6793, -0.1057,  0.0515],
        [ 0.3916, -0.0503,  0.0758,  ...,  0.0813, -0.1773, -0.1895],
        [-0.0697,  0.1386,  0.4689,  ..., -0.2309, -0.1424,  0.1817]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.3246, -0.4408,  0.3114,  ..., -0.5664,  0.2495,  0.4076],
        [ 0.7161, -0.2713,  0.2448,  ..., -0.2339,  0.6984,  0.5393],
        [ 0.4861,  0.2653,  0.0933,  ..., -0.4618, -0.0829, -0.1616],
        ...,
        [ 0.0622,  0.4587, -0.7578,  ...,  0.0296, -0.0691, -0.5779],
        [ 0.3184,  0.0437,  0.7975,  ...,  0.5444,  0.4549, -0.4066],
        [-0.0094, -1.3397,  0.2705,  ...,  0.3469, -0.2775,  0.7077]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.3898, -0.2491, -0.2620,  ..., -0.4365,  0.2724,  0.0874],
        [-0.1939, -0.1219, -0.3211,  ...,  0.4565, -0.3018, -0.5680],
        [-0.3549, -0.0848, -0.0908,  ..., -0.2742,  0.4007, -0.3513],
        ...,
        [ 0.4133, -0.0998,  0.1488,  ..., -0.0812,  0.1090,  0.0834],
        [ 0.0082,  0.6446, -0.2152,  ..., -0.1521,  0.1666, -0.0356],
        [-0.3114,  0.0879, -0.0057,  ...,  0.1024,  0.0960, -0.0427]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.1569, 0.1411, 0.0913,  ..., 0.1594, 0.1120, 0.1515], device='cuda:0',
       requires_grad=True), Parameter containing:
tensor([[ 8.6493e-02,  1.1864e-01, -4.7127e-02,  ...,  1.0400e-03,
         -5.9485e-05, -2.5289e-02],
        [ 5.2656e-02, -1.0736e-01,  3.9772e-03,  ..., -6.1616e-02,
         -4.7377e-02,  7.2044e-02],
        [-9.6736e-03,  4.6710e-02, -6.9292e-02,  ..., -9.5216e-02,
          6.3163e-02,  4.8108e-03],
        ...,
        [ 1.3032e-02,  3.6994e-02,  2.6077e-02,  ...,  2.1818e-02,
         -1.0377e-02,  2.8291e-02],
        [-7.8823e-03, -5.5021e-02, -2.2140e-02,  ..., -1.1071e-01,
         -1.3177e-02,  5.7225e-02],
        [ 8.0026e-02,  4.3949e-03,  7.7743e-02,  ..., -3.6224e-02,
         -2.3697e-02, -5.3692e-02]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.2798,  0.0813,  0.2118,  ...,  0.0982, -0.0074,  0.0439],
        [-0.3636,  0.0675, -0.2933,  ..., -0.1665,  0.6828,  0.2201],
        [-0.2880, -0.1682,  0.1961,  ..., -0.0322, -0.0826, -0.0628],
        ...,
        [ 0.0673,  0.0874, -0.0328,  ...,  0.0731,  0.3178, -0.0430],
        [-0.0087,  0.0148, -0.2120,  ...,  0.1693, -0.0599, -0.7099],
        [-0.3132, -0.2017, -0.0438,  ..., -0.0622,  0.0612, -0.2143]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.3505,  0.6262, -0.4424,  ...,  0.2246, -0.0524,  0.1840],
        [ 0.2866, -0.4179, -0.7077,  ..., -0.5903, -0.1015,  0.0459],
        [-0.7709,  0.3221, -0.5639,  ..., -0.4898, -0.3641, -0.3166],
        ...,
        [-0.1669, -0.0688, -0.0847,  ..., -0.0845, -0.8840,  0.3713],
        [-0.0337, -0.4713,  0.0445,  ..., -0.7061,  0.2692,  0.1070],
        [ 0.2038,  0.3147,  0.6953,  ..., -0.3917, -0.1026, -0.1711]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 1.2451, -0.8443,  0.4343,  ..., -0.1703, -0.4211,  0.6494],
        [ 0.2124, -0.7410,  1.2796,  ...,  0.1461, -0.7157,  0.8489],
        [-0.3484,  0.0041, -1.2870,  ...,  0.2090, -0.3942,  0.2705],
        ...,
        [ 0.3678, -0.2484, -0.1503,  ..., -0.2978,  0.4094, -0.1555],
        [ 1.7720,  1.5209, -0.6756,  ..., -0.2656,  0.6469, -1.6392],
        [-1.2195, -1.3076,  0.6422,  ..., -0.0501, -1.5321,  0.1757]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.0996, 0.1369, 0.1192,  ..., 0.1216, 0.1223, 0.1058], device='cuda:0',
       requires_grad=True), Parameter containing:
tensor([[ 0.1100,  0.3739, -0.0676,  ...,  0.2458,  0.1458,  0.0560],
        [-0.0868,  0.0880, -0.2089,  ...,  0.0624, -0.1598, -0.1113],
        [-0.3942,  0.0527, -0.3513,  ..., -0.0639, -0.2992, -0.3435],
        ...,
        [ 0.0707, -0.1719, -0.0423,  ..., -0.0787,  0.3035,  0.3511],
        [-0.1455, -0.0999, -0.0554,  ...,  0.2592,  0.0434, -0.0381],
        [-0.2601, -0.2403,  0.1251,  ..., -0.1829, -0.1257, -0.0931]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-1.6298e-01,  8.6156e-01, -5.0606e-01,  ...,  5.1014e-05,
         -5.0381e-01, -1.8974e-01],
        [ 5.9578e-01, -7.0933e-01,  2.3952e-01,  ..., -7.2817e-02,
          8.2570e-02,  1.2350e-01],
        [ 6.8266e-01,  1.0560e-01, -4.5402e-01,  ..., -2.6749e-01,
         -5.0903e-01,  2.8004e-01],
        ...,
        [ 3.0947e-01,  1.8922e-01, -2.3270e-02,  ..., -3.7609e-01,
          1.5632e-01, -8.4038e-01],
        [-5.2663e-01,  5.3586e-01,  2.3423e-01,  ...,  3.7255e-01,
          1.5604e-01,  7.1411e-01],
        [ 2.2940e-02, -1.8292e-01,  1.5026e-01,  ...,  7.4481e-02,
          8.2993e-02, -4.3909e-01]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.1423, -0.2143, -0.2092,  ...,  0.1844, -0.4574, -0.1602],
        [-0.2354, -0.1411,  0.4054,  ...,  0.0567,  0.2876,  0.0250],
        [ 0.1356,  0.4131, -0.2987,  ...,  0.6200,  0.5001,  0.0645],
        ...,
        [ 0.1118,  0.0562, -0.1591,  ...,  0.4490,  0.1238, -0.1818],
        [-0.0117,  0.4209, -0.0390,  ...,  0.1107,  0.2282, -0.0656],
        [-0.1796, -0.0412, -0.0280,  ...,  0.0361,  0.3768,  0.0324]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.1479, 0.2154, 0.1530,  ..., 0.1695, 0.2211, 0.1593], device='cuda:0',
       requires_grad=True), Parameter containing:
tensor([[ 0.0516,  0.0035,  0.1411,  ..., -0.0244,  0.0687,  0.0476],
        [ 0.0537, -0.0393, -0.0208,  ..., -0.0977,  0.0085, -0.0581],
        [-0.0378,  0.0126,  0.0403,  ..., -0.0910,  0.0040, -0.0414],
        ...,
        [-0.0209, -0.0337,  0.0251,  ..., -0.0004, -0.0596, -0.0109],
        [ 0.0893,  0.0157,  0.0444,  ...,  0.0117, -0.0490, -0.0151],
        [-0.0550, -0.0424, -0.0489,  ...,  0.0187, -0.0524,  0.0948]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.2763,  0.0063,  0.1739,  ...,  0.0411,  0.0536,  0.1755],
        [-0.0418,  0.0979,  0.1338,  ..., -0.3364, -0.2638, -0.0035],
        [-0.0045,  0.1995,  0.0348,  ...,  0.1860,  0.1535, -0.2688],
        ...,
        [ 0.1037,  0.0535, -0.0966,  ...,  0.1127,  0.3156, -0.1310],
        [-0.1069, -0.1620,  0.0448,  ..., -0.0478,  0.0006, -0.1222],
        [-0.1869,  0.0381, -0.2885,  ..., -0.2219, -0.2124, -0.3899]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.4877,  0.1451,  0.1326,  ...,  0.0575,  0.6135,  0.1244],
        [-0.0822, -0.3104,  0.0972,  ...,  0.0632, -0.6959, -0.1641],
        [ 0.1739, -0.4995, -0.0028,  ..., -0.1688, -1.4590,  0.1476],
        ...,
        [ 0.1275, -0.3603,  0.0760,  ..., -0.2712,  0.0186,  0.0933],
        [-0.5587, -0.5279, -0.0277,  ..., -0.6918,  0.1871,  0.9558],
        [-0.1556,  0.6855,  0.7137,  ...,  0.1907,  0.7152,  0.2788]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-1.2846,  2.1369,  0.6453,  ..., -0.3144,  0.0234, -0.6893],
        [-0.5632,  1.2324,  1.2322,  ...,  0.0715,  2.0219, -0.7431],
        [ 0.0655, -1.6631,  0.0405,  ..., -0.5864, -1.4614,  0.6042],
        ...,
        [ 0.1519, -1.7112,  0.7306,  ..., -0.6011, -0.3382,  0.6658],
        [-1.4551,  0.8165,  2.4118,  ...,  1.4825, -0.6532, -0.2357],
        [-1.5091,  0.1921, -0.9644,  ...,  1.1462,  2.2858,  0.0225]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.1365, 0.1049, 0.1191,  ..., 0.1025, 0.1155, 0.0759], device='cuda:0',
       requires_grad=True), Parameter containing:
tensor([[-0.0140,  0.0372,  0.1210,  ...,  0.1836, -0.0540,  0.2473],
        [-0.6006,  0.3372, -0.2792,  ..., -0.1119,  0.5309,  0.1772],
        [-0.2203, -0.1154, -0.3822,  ...,  0.1103,  0.1227, -0.2907],
        ...,
        [-0.3121,  0.1479, -0.1092,  ...,  0.0177, -0.1840,  0.3334],
        [-0.1653, -0.0355,  0.3166,  ..., -0.2119, -0.0347, -0.0394],
        [-0.3057,  0.3041,  0.0766,  ...,  0.2570,  0.4409,  0.2518]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.1237,  0.7064, -0.5562,  ...,  0.1906, -0.5588,  0.3537],
        [-0.1214, -0.0990, -0.1345,  ..., -0.4322, -0.6950,  0.2207],
        [-0.0884,  0.1357, -0.4938,  ...,  0.2214, -0.0591,  1.0437],
        ...,
        [-0.2984, -0.0783, -0.0792,  ...,  0.0224,  0.1268,  0.4522],
        [ 1.0064,  0.4652, -0.3044,  ...,  0.2635, -0.6092, -0.1047],
        [ 0.2121, -0.0997, -0.3504,  ..., -0.0315, -0.0804, -0.0994]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.1782,  0.1593,  0.4505,  ...,  0.3956, -0.2527, -0.1428],
        [-0.4770,  0.2457,  0.1796,  ...,  0.1075,  0.0525, -0.1166],
        [-0.0325,  0.0029,  0.2620,  ...,  0.6440,  0.2343,  0.4092],
        ...,
        [ 0.3554,  0.1586, -0.1154,  ...,  0.3968, -0.2212,  0.0627],
        [-0.0311, -0.0334, -0.0052,  ...,  0.2411, -0.4357,  0.5295],
        [-0.2876,  0.3109, -0.2913,  ...,  0.0064,  0.0754,  0.1752]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.2294, 0.2282, 0.2305,  ..., 0.2305, 0.1807, 0.2635], device='cuda:0',
       requires_grad=True), Parameter containing:
tensor([[ 0.1276, -0.1427,  0.0006,  ..., -0.0309, -0.0609, -0.0128],
        [ 0.1189,  0.0186, -0.0693,  ..., -0.0953, -0.0466, -0.0186],
        [ 0.0523,  0.0962,  0.0851,  ..., -0.0359,  0.0393, -0.0167],
        ...,
        [-0.0536, -0.0329,  0.0339,  ...,  0.0392, -0.0646,  0.0695],
        [ 0.0642,  0.0605,  0.0522,  ..., -0.0301,  0.0338,  0.0660],
        [ 0.0352, -0.0558,  0.0696,  ..., -0.0126, -0.0834, -0.0577]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.1180, -0.1986, -0.0719,  ...,  0.0347,  0.2008, -0.0864],
        [ 0.0952,  0.4581,  0.1093,  ...,  0.1242, -0.0813, -0.0890],
        [ 0.1181, -0.0282,  0.2471,  ...,  0.2522,  0.3288,  0.0262],
        ...,
        [ 0.1264, -0.0135,  0.0449,  ..., -0.0530,  0.3325,  0.0614],
        [-0.2530,  0.0022, -0.0011,  ..., -0.2533, -0.1990, -0.1171],
        [ 0.0564, -0.0744, -0.1049,  ..., -0.2278,  0.0127, -0.1235]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.5116,  0.6095, -0.5631,  ...,  0.4092, -0.8412, -0.0064],
        [-0.1712,  0.3258, -0.1802,  ...,  0.9267,  0.2434,  0.4619],
        [ 0.0089,  0.2403,  0.6610,  ...,  1.7142,  0.5375,  0.0317],
        ...,
        [-0.0798, -0.1781, -0.3796,  ..., -0.4339, -0.1339,  0.3110],
        [-0.8814, -0.1316, -1.1257,  ..., -0.3955, -1.0864, -0.7173],
        [ 0.5727, -0.2390,  0.4667,  ...,  1.0632,  0.4963, -0.2129]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 1.7135,  3.8380, -2.6523,  ..., -0.7154, -0.3215, -0.1082],
        [-1.2288,  0.0951, -1.9631,  ..., -0.3462, -1.1982, -0.6106],
        [ 1.2525,  0.5383, -0.3817,  ..., -1.0700,  0.3258, -0.5983],
        ...,
        [ 1.2328, -0.8091,  0.6681,  ..., -0.1252,  0.1450,  1.1767],
        [-0.6297, -0.8704,  1.2809,  ..., -0.2287,  0.0722, -0.9306],
        [ 0.5459, -1.9060, -1.2948,  ..., -0.9005,  0.3789, -1.4101]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.1427, 0.1544, 0.1239,  ..., 0.0939, 0.1138, 0.1386], device='cuda:0',
       requires_grad=True), Parameter containing:
tensor([[ 0.1993,  0.2604, -0.2535,  ...,  0.3120,  0.4333, -0.0982],
        [ 0.3434,  0.4375, -0.5399,  ..., -0.0813,  0.1070, -0.2742],
        [-0.2646,  0.0296,  0.2048,  ..., -0.3274,  0.4359,  0.5073],
        ...,
        [-0.1151,  0.0597,  0.3334,  ...,  0.2567, -0.1773, -0.1595],
        [ 0.3380, -0.5024, -0.4156,  ...,  0.1767,  0.3242,  0.1550],
        [-0.1813, -0.0975,  0.0080,  ...,  0.0378, -0.1066, -0.0893]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.7414, -0.5195, -0.3591,  ..., -0.7358,  0.0965, -0.1873],
        [ 0.2408,  0.1636, -0.1045,  ...,  0.1721, -0.6865,  0.1944],
        [ 0.0727, -0.6169, -0.7796,  ...,  0.3420,  0.3019,  0.4412],
        ...,
        [-0.6075, -0.2431, -0.0707,  ...,  1.4163,  0.3386, -0.1313],
        [ 0.9421, -0.5123, -0.2810,  ..., -0.9093,  0.1325,  0.4669],
        [ 0.6322, -0.1317,  0.0049,  ...,  0.3472, -0.0298, -0.7130]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.2562,  0.1017, -0.3700,  ...,  0.0710,  0.2994, -0.0775],
        [-0.2004, -0.2603, -0.0225,  ..., -0.1340, -0.1427, -0.3519],
        [-0.1073, -0.0645,  0.0726,  ...,  0.2102,  0.3191,  0.0545],
        ...,
        [-0.0972, -0.1650,  0.4695,  ..., -0.1034,  0.1430,  0.3171],
        [ 0.2793,  0.2281,  0.1058,  ..., -0.3787, -0.1323, -0.2522],
        [ 0.0591, -0.2865,  0.0658,  ...,  0.2430,  0.0266,  0.2200]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.2855, 0.2796, 0.2470,  ..., 0.2666, 0.2642, 0.2299], device='cuda:0',
       requires_grad=True), Parameter containing:
tensor([[-0.0805, -0.0913,  0.0175,  ...,  0.0199,  0.0065,  0.0431],
        [ 0.0076,  0.1492,  0.0539,  ..., -0.0513,  0.0242,  0.0238],
        [ 0.0421, -0.0002, -0.0261,  ...,  0.0303, -0.0381, -0.0127],
        ...,
        [-0.0319, -0.0087, -0.0502,  ..., -0.0307, -0.1263,  0.1179],
        [-0.0121, -0.0399, -0.0095,  ..., -0.0503,  0.0041,  0.0340],
        [-0.0449,  0.0774, -0.0383,  ..., -0.0495, -0.0087, -0.0475]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.1321, -0.0292,  0.0907,  ..., -0.0552,  0.1094, -0.0192],
        [-0.0142, -0.2271, -0.0111,  ...,  0.0354,  0.0005,  0.0493],
        [ 0.1310,  0.0887, -0.0687,  ..., -0.1802, -0.0110, -0.2185],
        ...,
        [ 0.0026, -0.0839, -0.1120,  ..., -0.0831, -0.0400, -0.0223],
        [ 0.0769,  0.1374,  0.2118,  ...,  0.1133, -0.0877, -0.3143],
        [-0.1370,  0.0369,  0.0794,  ..., -0.0099,  0.0421, -0.1456]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0018, -0.5465, -0.4072,  ..., -1.5510,  0.8466,  0.2418],
        [ 0.2240,  0.7699, -0.3570,  ...,  0.3180,  0.0525,  0.7328],
        [ 1.2197, -0.3681,  0.0137,  ..., -0.2988, -0.0935,  0.7294],
        ...,
        [-0.2582, -0.4749,  0.1073,  ..., -1.2317, -1.2940, -1.0855],
        [ 0.3543, -0.2584,  0.1214,  ..., -0.2028,  0.5534, -0.2258],
        [-0.5196, -0.3596, -0.7594,  ...,  0.7671, -0.5869,  0.1517]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-1.0232,  0.6950, -1.9791,  ...,  2.0967,  0.2306, -1.4733],
        [-1.0376, -0.0941,  1.2199,  ..., -0.8670, -0.8324, -1.0786],
        [-0.6776, -0.7308, -0.0360,  ..., -1.1271,  1.5589, -0.0682],
        ...,
        [ 0.1306, -0.6288, -1.1920,  ..., -0.0175, -1.4862,  0.9176],
        [-0.8768, -0.5463, -0.1574,  ..., -0.9485,  2.0887, -0.9220],
        [-0.7826, -0.1444, -1.2836,  ..., -2.1502, -1.0948,  1.0031]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.1520, 0.1088, 0.1451,  ..., 0.1366, 0.1744, 0.1249], device='cuda:0',
       requires_grad=True), Parameter containing:
tensor([[-0.2062, -0.2904, -0.2301,  ..., -0.2212,  0.1345, -0.4788],
        [ 0.1040, -0.1853,  0.0366,  ...,  0.0500,  0.0277,  0.0500],
        [-0.0785,  0.0884, -0.1886,  ...,  0.2319, -0.1261, -0.2594],
        ...,
        [ 0.1055, -0.2745, -0.3189,  ..., -0.1778,  0.3139,  0.2554],
        [ 0.2875,  0.0237,  0.1872,  ..., -0.1184,  0.1479,  0.2387],
        [ 0.1896,  0.2996, -0.1831,  ...,  0.0407, -0.1218,  0.0633]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.7146,  0.0647,  0.8174,  ..., -0.3913, -0.2337,  0.5820],
        [-0.4101, -0.7741, -0.7172,  ..., -0.6382,  0.0416, -0.2746],
        [-0.5334, -0.0030, -0.5524,  ..., -0.3482,  0.0528, -0.8476],
        ...,
        [-0.1314,  0.0551, -0.1294,  ..., -0.3498, -0.0990, -0.6694],
        [-0.1093, -0.2959,  0.0245,  ...,  0.2867, -0.4898,  0.6693],
        [ 0.1602, -0.0936, -0.3012,  ..., -0.0407, -0.4539,  0.7652]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.1864,  0.1729,  0.4431,  ...,  0.2863,  0.4689, -0.4054],
        [-0.0323,  0.4270, -0.0535,  ...,  0.3542, -0.1821, -0.3075],
        [-0.2271, -0.5569, -0.1246,  ..., -0.0009,  0.2307, -0.1488],
        ...,
        [ 0.4407, -0.1904,  0.0637,  ..., -0.1452,  0.2995, -0.0700],
        [-0.2923, -0.1130, -0.1031,  ..., -0.6558, -0.1590,  0.1511],
        [ 0.4852,  0.0285,  0.0818,  ..., -0.1192, -0.5484, -0.2566]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.3765, 0.4078, 0.4467,  ..., 0.4712, 0.4193, 0.4027], device='cuda:0',
       requires_grad=True), Parameter containing:
tensor([0.0905, 0.1211, 0.0987,  ..., 0.1089, 0.1354, 0.1249], device='cuda:0',
       requires_grad=True)], 'rank': 128, 'update_proj_gap': 200, 'scale': 0.25, 'proj_type': 'std'}]
ipdb> optimizer = GaLoreAdamW(param_groups, lr=lr)
*** NameError: name 'GaLoreAdamW' is not defined
ipdb> from galore_torch import GaLoreAdamW
ipdb> optimizer = GaLoreAdamW(param_groups, lr=lr)
/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/galore_torch/adamw.py:48:
 FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use t
he PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this
 warning
  warnings.warn(
ipdb> exit()
2024-08-10 15:27:06.859 | INFO     | __main__:main:651 - An error occurred:
Traceback (most recent call last):
  File "/home/adarsh/ReProver/compute_server.py", line 645, in main
    train(model_checkpoint_path, merged_data_path, next_suffix)
  File "/home/adarsh/ReProver/compute_server.py", line 545, in train
    trainer.fit(model, datamodule=data_module, ckpt_path=model_checkpoint_path)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/trainer/call.py", line 46, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/trainer/trainer.py", line 957, in _run
    self.strategy.setup(self)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/strategies/ddp.py", line 174, in setup
    self.setup_optimizers(trainer)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/strategies/strategy.py", line 138, in setup_optimizers
    self.optimizers, self.lr_scheduler_configs = _init_optimizers_and_lr_schedulers(self.lightning_module
)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/core/optimizer.py", line 179, in _init_optimizers_and_lr_schedulers
    optim_conf = call._call_lightning_module_hook(model.trainer, "configure_optimizers", pl_module=model)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/trainer/call.py", line 167, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/adarsh/ReProver/retrieval/model.py", line 254, in configure_optimizers
    # return get_optimizers(
  File "/home/adarsh/ReProver/common.py", line 434, in get_optimizers
    import ipdb; ipdb.set_trace()
  File "/home/adarsh/ReProver/common.py", line 434, in get_optimizers
    import ipdb; ipdb.set_trace()
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/bdb.py", line 90, in trace_disp
atch
    return self.dispatch_line(frame)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/bdb.py", line 115, in dispatch_
line
    if self.quitting: raise BdbQuit
bdb.BdbQuit
^C
^Z
[7]+  Stopped                 bash run_compute_server.sh
(ReProverComputeServer) adarsh@tensorlab-DGX-Station-A100-920-23487-2531-000:~/ReProver$
(ReProverComputeServer) adarsh@tensorlab-DGX-Station-A100-920-23487-2531-000:~/ReProver$ bash run_compute
_server.sh
Script executed from: /home/adarsh/ReProver
[2024-08-10 15:29:32,718] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda
 (auto detect)
 [WARNING]  async_io requires the dev libaio .so object and headers but these were not found.
 [WARNING]  async_io: please install the libaio-dev package with apt
 [WARNING]  If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS envi
ronment variables to where it can be found.
 [WARNING]  Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 [WARNING]  sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
 [WARNING]  using untested triton version (3.0.0), only 1.0.0 is known to be compatible
/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/deepspeed/runtime/zero/li
near.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.cust
om_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/deepspeed/runtime/zero/li
near.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.cust
om_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
2024-08-10 15:29:39.708 | INFO     | __main__:check_progress_file:586 - Checking contents of /home/adarsh
/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning/loops/progress.py
2024-08-10 15:29:39.708 | INFO     | __main__:check_progress_file:590 - Contents of progress.py:
2024-08-10 15:29:39.708 | INFO     | __main__:check_progress_file:591 - # Copyright The Lightning AI team
.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import asdict, dataclass, field
from typing import Type

from typing_extensions import override


@dataclass
class _BaseProgress:
    """Mixin that implements state-loading utilities for dataclasses."""

    def state_dict(self) -> dict:
        return asdict(self)

    def load_state_dict(self, state_dict: dict) -> None:
        if state_dict["completed"] == None:
            state_dict["completed"] = 0
        self.__dict__.update(state_dict)

    @classmethod
    def from_state_dict(cls, state_dict: dict) -> "_BaseProgress":
        obj = cls()
        obj.load_state_dict(state_dict)
        return obj

    def reset(self) -> None:
        """Reset the object's state."""
        raise NotImplementedError


@dataclass
class _ReadyCompletedTracker(_BaseProgress):
    """Track an event's progress.

    Args:
        ready: Intended to track the number of events ready to start.
        completed: Intended to be incremented after the event completes (e.g. after ``on_*_end`` runs).

    These attributes should be increased in order, that is, :attr:`ready` first and :attr:`completed` las
t.

    """

    ready: int = 0
    completed: int = 0

    @override
    def reset(self) -> None:
        """Reset the state."""
        self.ready = 0
        self.completed = 0

    def reset_on_restart(self) -> None:
        """Reset the progress on restart.

        If there is a failure before all attributes are increased, restore the attributes to the last ful
ly completed
        value.

        """
        self.ready = self.completed


@dataclass
class _StartedTracker(_ReadyCompletedTracker):
    """Track an event's progress.

    Args:
        ready: Intended to track the number of events ready to start.
        started: Intended to be incremented after the event is started (e.g. after ``on_*_start`` runs).
        completed: Intended to be incremented after the event completes (e.g. after ``on_*_end`` runs).

    These attributes should be increased in order, that is, :attr:`ready` first and :attr:`completed` las
t.

    """

    started: int = 0

    @override
    def reset(self) -> None:
        super().reset()
        self.started = 0

    @override
    def reset_on_restart(self) -> None:
        super().reset_on_restart()
        self.started = self.completed


@dataclass
class _ProcessedTracker(_StartedTracker):
    """Track an event's progress.

    Args:
        ready: Intended to track the number of events ready to start.
        started: Intended to be incremented after the event is started (e.g. after ``on_*_start`` runs).
        processed: Intended to be incremented after the event is processed.
        completed: Intended to be incremented after the event completes (e.g. after ``on_*_end`` runs).

    These attributes should be increased in order, that is, :attr:`ready` first and :attr:`completed` las
t.

    """

    processed: int = 0

    @override
    def reset(self) -> None:
        super().reset()
        self.processed = 0

    @override
    def reset_on_restart(self) -> None:
        super().reset_on_restart()
        self.processed = self.completed


@dataclass
class _Progress(_BaseProgress):
    """Track aggregated and current progress.

    Args:
        total: Intended to track the total progress of an event.
        current: Intended to track the current progress of an event.

    """

    total: _ReadyCompletedTracker = field(default_factory=_ProcessedTracker)
    current: _ReadyCompletedTracker = field(default_factory=_ProcessedTracker)

    def __post_init__(self) -> None:
        if self.total.__class__ is not self.current.__class__:
            raise ValueError("The `total` and `current` instances should be of the same class")

    def increment_ready(self) -> None:
        self.total.ready += 1
        self.current.ready += 1

    def increment_started(self) -> None:
        if not isinstance(self.total, _StartedTracker):
            raise TypeError(f"`{self.total.__class__.__name__}` doesn't have a `started` attribute")
        self.total.started += 1
        self.current.started += 1

    def increment_processed(self) -> None:
        if not isinstance(self.total, _ProcessedTracker):
            raise TypeError(f"`{self.total.__class__.__name__}` doesn't have a `processed` attribute")
        self.total.processed += 1
        self.current.processed += 1

    def increment_completed(self) -> None:
        self.total.completed += 1
        self.current.completed += 1

    @classmethod
    def from_defaults(cls, tracker_cls: Type[_ReadyCompletedTracker], **kwargs: int) -> "_Progress":
        """Utility function to easily create an instance from keyword arguments to both ``Tracker``s."""
        return cls(total=tracker_cls(**kwargs), current=tracker_cls(**kwargs))

    @override
    def reset(self) -> None:
        self.total.reset()
        self.current.reset()

    def reset_on_run(self) -> None:
        self.current.reset()

    def reset_on_restart(self) -> None:
        self.current.reset_on_restart()

    @override
    def load_state_dict(self, state_dict: dict) -> None:
        if state_dict["total"]["completed"] == None:
            state_dict["total"]["completed"] = 0
        self.total.load_state_dict(state_dict["total"])
        self.current.load_state_dict(state_dict["current"])


@dataclass
class _BatchProgress(_Progress):
    """Tracks batch progress.

    These counters are local to a trainer rank. By default, they are not globally synced across all ranks
.

    Args:
        total: Tracks the total batch progress.
        current: Tracks the current batch progress.
        is_last_batch: Whether the batch is the last one. This is useful for iterable datasets.

    """

    is_last_batch: bool = False

    @override
    def reset(self) -> None:
        super().reset()
        self.is_last_batch = False

    @override
    def reset_on_run(self) -> None:
        super().reset_on_run()
        self.is_last_batch = False

    @override
    def load_state_dict(self, state_dict: dict) -> None:
        if state_dict["total"]["completed"] == None:
            state_dict["total"]["completed"] = 0
        super().load_state_dict(state_dict)
        self.is_last_batch = state_dict["is_last_batch"]


@dataclass
class _SchedulerProgress(_Progress):
    """Tracks scheduler progress.

    These counters are local to a trainer rank. By default, they are not globally synced across all ranks
.

    Args:
        total: Tracks the total scheduler progress.
        current: Tracks the current scheduler progress.

    """

    total: _ReadyCompletedTracker = field(default_factory=_ReadyCompletedTracker)
    current: _ReadyCompletedTracker = field(default_factory=_ReadyCompletedTracker)


@dataclass
class _OptimizerProgress(_BaseProgress):
    """Track optimizer progress.

    Args:
        step: Tracks ``optimizer.step`` calls.
        zero_grad: Tracks ``optimizer.zero_grad`` calls.

    """

    step: _Progress = field(default_factory=lambda: _Progress.from_defaults(_ReadyCompletedTracker))
    zero_grad: _Progress = field(default_factory=lambda: _Progress.from_defaults(_StartedTracker))

    @override
    def reset(self) -> None:
        self.step.reset()
        self.zero_grad.reset()

    def reset_on_run(self) -> None:
        self.step.reset_on_run()
        self.zero_grad.reset_on_run()

    def reset_on_restart(self) -> None:
        self.step.reset_on_restart()
        self.zero_grad.reset_on_restart()

    @override
    def load_state_dict(self, state_dict: dict) -> None:
        if state_dict["step"]["total"]["completed"] == None:
            state_dict["step"]["total"]["completed"] = 0
        self.step.load_state_dict(state_dict["step"])
        self.zero_grad.load_state_dict(state_dict["zero_grad"])


@dataclass
class _OptimizationProgress(_BaseProgress):
    """Track optimization progress.

    Args:
        optimizer: Tracks optimizer progress.

    """

    optimizer: _OptimizerProgress = field(default_factory=_OptimizerProgress)

    @property
    def optimizer_steps(self) -> int:
        return self.optimizer.step.total.completed

    @override
    def reset(self) -> None:
        self.optimizer.reset()

    def reset_on_run(self) -> None:
        self.optimizer.reset_on_run()

    def reset_on_restart(self) -> None:
        self.optimizer.reset_on_restart()

    @override
    def load_state_dict(self, state_dict: dict) -> None:
        if state_dict["optimizer"]["step"]["total"]["completed"] == None:
            state_dict["optimizer"]["step"]["total"]["completed"] = 0
        self.optimizer.load_state_dict(state_dict["optimizer"])

2024-08-10 15:29:39.708 | INFO     | __main__:main:608 - Starting compute server...
2024-08-10 15:29:39.708 | INFO     | __main__:main:609 - Current working directory: /home/adarsh/ReProver
2024-08-10 15:29:39.709 | INFO     | __main__:main:617 - ROOT_DIR: /raid/adarsh
2024-08-10 15:29:39.709 | INFO     | __main__:main:618 - DATA_DIR: datasets_test
2024-08-10 15:29:39.709 | INFO     | __main__:main:622 - Configuring LeanDojo...
2024-08-10 15:29:39.712 | INFO     | generate_benchmark_lean4:configure_leandojo:294 - Current working di
rectory: /home/adarsh/ReProver
2024-08-10 15:29:39.712 | INFO     | __main__:main:624 - LeanDojo configured
2024-08-10 15:29:39.712 | INFO     | __main__:main:632 - Unique URLs: {'https://github.com/Adarsh321123/n
ew-version-test.git'}
2024-08-10 15:29:39.712 | INFO     | __main__:main:633 - About to generate datasets...
2024-08-10 15:29:39.712 | INFO     | __main__:generate_dataset:226 - Generating 1 datasets
2024-08-10 15:29:39.712 | INFO     | __main__:generate_dataset:231 - Processing https://github.com/Adarsh
321123/new-version-test.git
2024-08-10 15:29:39.911 | INFO     | __main__:get_compatible_commit:166 - Latest commit: f465306be03ced99
9caa157a85558a6c41b3e3f5
2024-08-10 15:29:39.911 | INFO     | __main__:get_compatible_commit:169 - Creating LeanGitRepo for https:
//github.com/Adarsh321123/new-version-test
2024-08-10 15:29:40.308 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the com
mit hash for lean4 v4.8.0-rc1
2024-08-10 15:29:40.950 | INFO     | __main__:get_compatible_commit:171 - Getting config for https://gith
ub.com/Adarsh321123/new-version-test.git
2024-08-10 15:29:40.950 | INFO     | __main__:get_compatible_commit:175 - Latest commit compatible for ur
l https://github.com/Adarsh321123/new-version-test.git
2024-08-10 15:29:40.951 | INFO     | __main__:generate_dataset:236 - Found compatible commit f465306be03c
ed999caa157a85558a6c41b3e3f5 for https://github.com/Adarsh321123/new-version-test.git
2024-08-10 15:29:40.951 | INFO     | __main__:generate_dataset:237 - Lean version: v4.8.0-rc1
2024-08-10 15:29:40.951 | INFO     | __main__:generate_dataset:240 - Creating LeanGitRepo for https://git
hub.com/Adarsh321123/new-version-test
2024-08-10 15:29:40.951 | INFO     | __main__:generate_dataset:244 - Generating benchmark at /raid/adarsh
/datasets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5
2024-08-10 15:29:40.951 | INFO     | generate_benchmark_lean4:main:301 - Generating dataset to go into /r
aid/adarsh/datasets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5
2024-08-10 15:29:40.951 | INFO     | generate_benchmark_lean4:main:308 - lean toolchain version: {'conten
t': 'leanprover/lean4:v4.8.0-rc1\n'}
2024-08-10 15:29:40.951 | INFO     | generate_benchmark_lean4:main:310 - lean version v: v4.8.0-rc1
2024-08-10 15:29:40.951 | INFO     | generate_benchmark_lean4:main:311 - is supported: True
2024-08-10 15:29:40.951 | INFO     | generate_benchmark_lean4:main:321 - lean path1 /home/adarsh/.elan/to
olchains/leanprover--lean4---4.8.0-rc1
2024-08-10 15:29:40.951 | INFO     | generate_benchmark_lean4:main:322 - lean path2 /.elan/toolchains/lea
nprover--lean4---4.8.0-rc1
2024-08-10 15:29:40.951 | INFO     | generate_benchmark_lean4:main:323 - lean path3 ~/.elan/toolchains/le
anprover--lean4---4.8.0-rc1
2024-08-10 15:29:40.951 | INFO     | generate_benchmark_lean4:main:327 - Lean toolchain path 2 does not e
xist: /.elan/toolchains/leanprover--lean4---4.8.0-rc1
2024-08-10 15:29:40.951 | INFO     | generate_benchmark_lean4:main:329 - Lean toolchain path 3 does not e
xist: ~/.elan/toolchains/leanprover--lean4---4.8.0-rc1
2024-08-10 15:29:40.952 | INFO     | generate_benchmark_lean4:main:332 - Switched to Lean toolchain at: /
home/adarsh/.elan/toolchains/leanprover--lean4---4.8.0-rc1
2024-08-10 15:29:40.984 | INFO     | generate_benchmark_lean4:main:334 - lean --version: Lean (version 4.
8.0-rc1, x86_64-unknown-linux-gnu, commit dcccfb73cb24, Release)

2024-08-10 15:29:40.985 | INFO     | generate_benchmark_lean4:main:335 - repo: LeanGitRepo(url='https://g
ithub.com/Adarsh321123/new-version-test', commit='f465306be03ced999caa157a85558a6c41b3e3f5')
2024-08-10 15:29:40.985 | INFO     | generate_benchmark_lean4:main:337 - Configuring LeanDojo again...
2024-08-10 15:29:40.988 | INFO     | generate_benchmark_lean4:configure_leandojo:294 - Current working di
rectory: /home/adarsh/ReProver
2024-08-10 15:29:40.988 | INFO     | generate_benchmark_lean4:main:339 - LeanDojo configured
2024-08-10 15:29:40.988 | INFO     | generate_benchmark_lean4:main:342 - Tracing the repo...
2024-08-10 15:29:40.988 | DEBUG    | lean_dojo.data_extraction.trace:get_traced_repo_path:217 - The trace
d repo is available in the cache.
2024-08-10 15:29:40.988 | INFO     | lean_dojo.data_extraction.trace:trace:246 - Loading the traced repo
from /raid/adarsh/.cache/lean_dojo/Adarsh321123-new-version-test-f465306be03ced999caa157a85558a6c41b3e3f5
/new-version-test
2024-08-10 15:29:40.991 | DEBUG    | lean_dojo.utils:execute:110 - git remote get-url origin
2024-08-10 15:29:40.994 | DEBUG    | lean_dojo.utils:execute:110 - git log -n 1
2024-08-10 15:29:41.133 | DEBUG    | lean_dojo.data_extraction.traced_data:load_from_disk:1169 - Loading
5432 traced XML files from /raid/adarsh/.cache/lean_dojo/Adarsh321123-new-version-test-f465306be03ced999c
aa157a85558a6c41b3e3f5/new-version-test with 31 workers
2024-08-10 15:29:42,477 INFO worker.py:1772 -- Started a local Ray instance. View the dashboard at 127.0.
0.1:8267
  4%|██▋                                                              | 229/5432 [00:06<01:39, 52.40it/s]
(raylet) [2024-08-10 15:29:52,346 E 194859 194888] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-29-41_255172_193856 is over 95% full, available space: 85735563264; capacity: 1887507697664
. Object creation will fail if spilling is required.
  8%|█████▎                                                           | 440/5432 [00:15<02:50, 29.35it/s]
(raylet) [2024-08-10 15:30:02,352 E 194859 194888] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-29-41_255172_193856 is over 95% full, available space: 86258606080; capacity: 1887507697664
. Object creation will fail if spilling is required.
 12%|████████                                                         | 677/5432 [00:24<01:31, 51.70it/s]
(raylet) [2024-08-10 15:30:12,358 E 194859 194888] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-29-41_255172_193856 is over 95% full, available space: 87680667648; capacity: 1887507697664
. Object creation will fail if spilling is required.
 16%|██████████▍                                                      | 870/5432 [00:37<02:34, 29.52it/s]
(raylet) [2024-08-10 15:30:22,363 E 194859 194888] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-29-41_255172_193856 is over 95% full, available space: 89236983808; capacity: 1887507697664
. Object creation will fail if spilling is required.
 18%|███████████▊                                                    | 1000/5432 [00:47<03:23, 21.75it/s]
(raylet) [2024-08-10 15:30:32,369 E 194859 194888] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-29-41_255172_193856 is over 95% full, available space: 85742280704; capacity: 1887507697664
. Object creation will fail if spilling is required.
 19%|████████████▍                                                   | 1055/5432 [00:57<07:46,  9.38it/s]
(raylet) [2024-08-10 15:30:42,641 E 194859 194888] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-29-41_255172_193856 is over 95% full, available space: 87033561088; capacity: 1887507697664
. Object creation will fail if spilling is required.
 21%|█████████████▍                                                  | 1142/5432 [01:11<02:54, 24.65it/s]
(raylet) [2024-08-10 15:30:52,736 E 194859 194888] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-29-41_255172_193856 is over 95% full, available space: 87260688384; capacity: 1887507697664
. Object creation will fail if spilling is required.
 24%|███████████████                                                 | 1282/5432 [01:14<01:34, 43.82it/s]
(raylet) [2024-08-10 15:31:02,742 E 194859 194888] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-29-41_255172_193856 is over 95% full, available space: 85734559744; capacity: 1887507697664
. Object creation will fail if spilling is required.
 24%|███████████████▏                                                | 1284/5432 [01:28<51:52,  1.33it/s]
(raylet) [2024-08-10 15:31:12,746 E 194859 194888] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-29-41_255172_193856 is over 95% full, available space: 85734535168; capacity: 1887507697664
. Object creation will fail if spilling is required.
 27%|█████████████████▍                                              | 1485/5432 [01:48<00:43, 91.61it/s]
(raylet) [2024-08-10 15:31:22,753 E 194859 194888] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-29-41_255172_193856 is over 95% full, available space: 85734518784; capacity: 1887507697664
. Object creation will fail if spilling is required.
 27%|█████████████████▌                                              | 1489/5432 [01:48<39:42,  1.65it/s]
(raylet) [2024-08-10 15:31:32,761 E 194859 194888] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-29-41_255172_193856 is over 95% full, available space: 85734506496; capacity: 1887507697664
. Object creation will fail if spilling is required.
 31%|███████████████████▉                                            | 1687/5432 [02:12<53:02,  1.18it/s]
(raylet) [2024-08-10 15:31:42,767 E 194859 194888] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-29-41_255172_193856 is over 95% full, available space: 85734445056; capacity: 1887507697664
. Object creation will fail if spilling is required.
(raylet) [2024-08-10 15:31:52,771 E 194859 194888] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-29-41_255172_193856 is over 95% full, available space: 85733482496; capacity: 1887507697664
. Object creation will fail if spilling is required.
 36%|███████████████████████▎                                        | 1980/5432 [02:17<01:00, 56.94it/s]
(raylet) [2024-08-10 15:32:02,777 E 194859 194888] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-29-41_255172_193856 is over 95% full, available space: 85733462016; capacity: 1887507697664
. Object creation will fail if spilling is required.
 37%|██████████████████████▊                                       | 2000/5432 [02:44<1:03:18,  1.11s/it]
(raylet) [2024-08-10 15:32:12,783 E 194859 194888] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-29-41_255172_193856 is over 95% full, available space: 85733445632; capacity: 1887507697664
. Object creation will fail if spilling is required.
 42%|██████████████████████████▏                                   | 2297/5432 [03:22<1:53:57,  2.18s/it]
(raylet) [2024-08-10 15:32:42,799 E 194859 194888] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-29-41_255172_193856 is over 95% full, available space: 85733359616; capacity: 1887507697664
. Object creation will fail if spilling is required. [repeated 3x across cluster] (Ray deduplicates logs
by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-o
bservability/user-guides/configure-logging.html#log-deduplication for more options.)
 47%|█████████████████████████████▉                                  | 2538/5432 [03:27<00:56, 51.57it/s]
(raylet) [2024-08-10 15:33:12,814 E 194859 194888] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-29-41_255172_193856 is over 95% full, available space: 85731672064; capacity: 1887507697664
. Object creation will fail if spilling is required. [repeated 3x across cluster]
 49%|██████████████████████████████▍                               | 2666/5432 [04:11<1:40:02,  2.17s/it]
(raylet) [2024-08-10 15:33:22,819 E 194859 194888] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-29-41_255172_193856 is over 95% full, available space: 85731647488; capacity: 1887507697664
. Object creation will fail if spilling is required.
(raylet) [2024-08-10 15:33:32,824 E 194859 194888] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-29-41_255172_193856 is over 95% full, available space: 85731635200; capacity: 1887507697664
. Object creation will fail if spilling is required.
 56%|███████████████████████████████████▋                            | 3029/5432 [04:17<00:35, 67.08it/s]
(raylet) [2024-08-10 15:34:02,849 E 194859 194888] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-29-41_255172_193856 is over 95% full, available space: 85730787328; capacity: 1887507697664
. Object creation will fail if spilling is required. [repeated 3x across cluster]
 59%|█████████████████████████████████████▉                          | 3215/5432 [05:11<00:51, 42.89it/s]
(raylet) [2024-08-10 15:34:12,856 E 194859 194888] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-29-41_255172_193856 is over 95% full, available space: 85730754560; capacity: 1887507697664
. Object creation will fail if spilling is required.
(raylet) [2024-08-10 15:34:22,863 E 194859 194888] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-29-41_255172_193856 is over 95% full, available space: 85730758656; capacity: 1887507697664
. Object creation will fail if spilling is required.
 67%|██████████████████████████████████████████▊                     | 3635/5432 [05:18<00:45, 39.17it/s]
(raylet) [2024-08-10 15:35:02,887 E 194859 194888] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-29-41_255172_193856 is over 95% full, available space: 85729370112; capacity: 1887507697664
. Object creation will fail if spilling is required. [repeated 4x across cluster]
 71%|█████████████████████████████████████████████▋                  | 3877/5432 [05:23<00:46, 33.56it/s]
(raylet) [2024-08-10 15:35:12,893 E 194859 194888] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-29-41_255172_193856 is over 95% full, available space: 85729353728; capacity: 1887507697664
. Object creation will fail if spilling is required.
 71%|████████████████████████████████████████████▎                 | 3880/5432 [06:22<1:39:55,  3.86s/it]
(raylet) [2024-08-10 15:35:22,900 E 194859 194888] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-29-41_255172_193856 is over 95% full, available space: 85729341440; capacity: 1887507697664
. Object creation will fail if spilling is required.
 77%|████████████████████████████████████████████████▉               | 4158/5432 [06:28<00:31, 40.49it/s]
(raylet) [2024-08-10 15:36:12,936 E 194859 194888] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-29-41_255172_193856 is over 95% full, available space: 85728440320; capacity: 1887507697664
. Object creation will fail if spilling is required. [repeated 5x across cluster]
 83%|████████████████████████████████████████████████████▉           | 4496/5432 [06:36<00:24, 38.12it/s]
(raylet) [2024-08-10 15:36:22,944 E 194859 194888] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-29-41_255172_193856 is over 95% full, available space: 85728428032; capacity: 1887507697664
. Object creation will fail if spilling is required.
 83%|███████████████████████████████████████████████████▎          | 4498/5432 [07:48<1:19:26,  5.10s/it]
(raylet) [2024-08-10 15:36:32,947 E 194859 194888] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-29-41_255172_193856 is over 95% full, available space: 85728419840; capacity: 1887507697664
. Object creation will fail if spilling is required.
 90%|█████████████████████████████████████████████████████████▎      | 4868/5432 [07:58<00:11, 48.87it/s]
(raylet) [2024-08-10 15:37:42,983 E 194859 194888] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-29-41_255172_193856 is over 95% full, available space: 85726982144; capacity: 1887507697664
. Object creation will fail if spilling is required. [repeated 7x across cluster]
100%|███████████████████████████████████████████████████████████████▊| 5415/5432 [09:35<00:00, 96.36it/s]
(raylet) [2024-08-10 15:37:52,990 E 194859 194888] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-29-41_255172_193856 is over 95% full, available space: 85726126080; capacity: 1887507697664
. Object creation will fail if spilling is required.
100%|████████████████████████████████████████████████████████████████| 5432/5432 [09:35<00:00,  9.44it/s]
2024-08-10 15:39:21.680 | DEBUG    | lean_dojo.data_extraction.lean:get_dependencies:489 - Querying the d
ependencies of LeanGitRepo(url='https://github.com/Adarsh321123/new-version-test', commit='f465306be03ced
999caa157a85558a6c41b3e3f5')
2024-08-10 15:39:22.648 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the com
mit hash for lean4 v4.8.0-rc2
2024-08-10 15:39:23.954 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the com
mit hash for lean4 v4.6.0-rc1
2024-08-10 15:39:26.171 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the com
mit hash for lean4 v4.7.0
2024-08-10 15:39:27.150 | DEBUG    | lean_dojo.data_extraction.lean:url_to_repo:68 - url_to_repo("https:/
/github.com/leanprover-community/import-graph.git") failed. Retrying...
2024-08-10 15:39:28.272 | DEBUG    | lean_dojo.data_extraction.lean:url_to_repo:68 - url_to_repo("https:/
/github.com/leanprover-community/import-graph.git") failed. Retrying...
2024-08-10 15:39:32.123 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the com
mit hash for lean4 v4.11.0-rc1
2024-08-10 15:39:32.681 | DEBUG    | lean_dojo.data_extraction.lean:get_dependencies:489 - Querying the d
ependencies of LeanGitRepo(url='https://github.com/leanprover-community/mathlib4', commit='19796eea79b36c
81ba5b1de34f5500e0348c84a2')
2024-08-10 15:39:33.349 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the com
mit hash for lean4 v4.10.0
2024-08-10 15:39:34.446 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the com
mit hash for lean4 v4.9.0
2024-08-10 15:40:02.680 | DEBUG    | lean_dojo.data_extraction.traced_data:check_sanity:1032 - Checking t
he sanity of TracedRepo(repo=LeanGitRepo(url='https://github.com/Adarsh321123/new-version-test', commit='
f465306be03ced999caa157a85558a6c41b3e3f5'), dependencies={'lean4': LeanGitRepo(url='https://github.com/le
anprover/lean4', commit='daa22187642d4cf6954c39a23eab20d8a8675416'), 'batteries': LeanGitRepo(url='https:
//github.com/leanprover-community/batteries', commit='dc167d260ff7ee9849b436037add06bed15104be'), 'Qq': L
eanGitRepo(url='https://github.com/leanprover-community/quote4', commit='71f54425e6fe0fa75f3aef33a2813a78
98392222'), 'aesop': LeanGitRepo(url='https://github.com/leanprover-community/aesop', commit='0444234b421
6e944d5be2ce42a25d7410c67876f'), 'proofwidgets': LeanGitRepo(url='https://github.com/leanprover-community
/ProofWidgets4', commit='a96aee5245720f588876021b6a0aa73efee49c76'), 'Cli': LeanGitRepo(url='https://gith
ub.com/leanprover/lean4-cli', commit='2cf1030dc2ae6b3632c84a09350b675ef3e347d0'), 'mathlib': LeanGitRepo(
url='https://github.com/leanprover-community/mathlib4', commit='19796eea79b36c81ba5b1de34f5500e0348c84a2'
), 'importGraph': LeanGitRepo(url='https://github.com/leanprover-community/import-graph', commit='57bd206
5f1dbea5e9235646fb836c7cea9ab03b6')}, root_dir=PosixPath('/raid/adarsh/.cache/lean_dojo/Adarsh321123-new-
version-test-f465306be03ced999caa157a85558a6c41b3e3f5/new-version-test'))
2024-08-10 15:40:17.003 | INFO     | generate_benchmark_lean4:main:344 - Successfully traced the repo
2024-08-10 15:40:28.614 | INFO     | generate_benchmark_lean4:split_data:132 - 119751 theorems in total
2024-08-10 15:40:28.614 | INFO     | generate_benchmark_lean4:split_data:137 - 119751 theorems in total,
with 2395 for validation and 2395 for testing
2024-08-10 15:40:28.615 | INFO     | generate_benchmark_lean4:split_randomly:80 - Splitting the theorems
randomly
2024-08-10 15:40:28.663 | INFO     | generate_benchmark_lean4:split_by_premise:94 - Splitting the theorem
s by premises
2024-08-10 15:40:38.962 | INFO     | generate_benchmark_lean4:main:352 - Successfully split the data
2024-08-10 15:41:07.028 | INFO     | generate_benchmark_lean4:export_proofs:194 - 114961 theorems and 242
909 tactics saved to /raid/adarsh/datasets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5
/random/train.json
2024-08-10 15:41:07.948 | INFO     | generate_benchmark_lean4:export_proofs:194 - 2395 theorems and 5246
tactics saved to /raid/adarsh/datasets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5/ran
dom/val.json
2024-08-10 15:41:08.539 | INFO     | generate_benchmark_lean4:export_proofs:194 - 2395 theorems and 4928
tactics saved to /raid/adarsh/datasets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5/ran
dom/test.json
2024-08-10 15:41:34.382 | INFO     | generate_benchmark_lean4:export_proofs:194 - 114961 theorems and 238
146 tactics saved to /raid/adarsh/datasets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5
/novel_premises/train.json
2024-08-10 15:41:35.372 | INFO     | generate_benchmark_lean4:export_proofs:194 - 2395 theorems and 7493
tactics saved to /raid/adarsh/datasets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5/nov
el_premises/val.json
2024-08-10 15:41:36.094 | INFO     | generate_benchmark_lean4:export_proofs:194 - 2395 theorems and 7444
tactics saved to /raid/adarsh/datasets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5/nov
el_premises/test.json
2024-08-10 15:41:36.101 | INFO     | generate_benchmark_lean4:export_data:270 - Successfully exported the
 proofs
2024-08-10 15:42:06.849 | INFO     | generate_benchmark_lean4:export_premises:218 - 175685 theorems/defin
itions from 5432 files saved to /raid/adarsh/datasets_test/new-version-test_f465306be03ced999caa157a85558
a6c41b3e3f5/corpus.jsonl
2024-08-10 15:42:06.850 | INFO     | generate_benchmark_lean4:export_data:274 - Successfully exported the
 premises
2024-08-10 15:42:06.850 | INFO     | generate_benchmark_lean4:export_data:282 - Successfully exported the
 metadata
2024-08-10 15:42:06.850 | INFO     | generate_benchmark_lean4:main:354 - Successfully exported the data
2024-08-10 15:42:06.976 | INFO     | __main__:generate_dataset:246 - Finished generating benchmark at /ra
id/adarsh/datasets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5
2024-08-10 15:42:06.976 | INFO     | __main__:generate_dataset:247 - Merging datasets
2024-08-10 15:42:06.976 | INFO     | __main__:merge_datasets:74 - Merging datasets for random
2024-08-10 15:42:06.976 | INFO     | __main__:merge_datasets:80 - Processing train split
2024-08-10 15:42:06.976 | INFO     | __main__:merge_datasets:86 - Processing new-version-test_f465306be03
ced999caa157a85558a6c41b3e3f5
2024-08-10 15:42:09.928 | INFO     | __main__:merge_datasets:98 - Deleted processed file: /raid/adarsh/da
tasets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5/random/train.json
2024-08-10 15:42:18.679 | INFO     | __main__:merge_datasets:104 - Finished processing train split
2024-08-10 15:42:18.679 | INFO     | __main__:merge_datasets:80 - Processing val split
2024-08-10 15:42:18.691 | INFO     | __main__:merge_datasets:86 - Processing new-version-test_f465306be03
ced999caa157a85558a6c41b3e3f5
2024-08-10 15:42:19.038 | INFO     | __main__:merge_datasets:98 - Deleted processed file: /raid/adarsh/da
tasets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5/random/val.json
2024-08-10 15:42:19.232 | INFO     | __main__:merge_datasets:104 - Finished processing val split
2024-08-10 15:42:19.232 | INFO     | __main__:merge_datasets:80 - Processing test split
2024-08-10 15:42:19.232 | INFO     | __main__:merge_datasets:86 - Processing new-version-test_f465306be03
ced999caa157a85558a6c41b3e3f5
2024-08-10 15:42:19.290 | INFO     | __main__:merge_datasets:98 - Deleted processed file: /raid/adarsh/da
tasets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5/random/test.json
2024-08-10 15:42:19.476 | INFO     | __main__:merge_datasets:104 - Finished processing test split
2024-08-10 15:42:19.476 | INFO     | __main__:merge_datasets:105 - Finished merging datasets for random
2024-08-10 15:42:19.476 | INFO     | __main__:merge_datasets:74 - Merging datasets for novel_premises
2024-08-10 15:42:19.477 | INFO     | __main__:merge_datasets:80 - Processing train split
2024-08-10 15:42:19.477 | INFO     | __main__:merge_datasets:86 - Processing new-version-test_f465306be03
ced999caa157a85558a6c41b3e3f5
2024-08-10 15:42:22.264 | INFO     | __main__:merge_datasets:98 - Deleted processed file: /raid/adarsh/da
tasets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5/novel_premises/train.json
2024-08-10 15:42:31.030 | INFO     | __main__:merge_datasets:104 - Finished processing train split
2024-08-10 15:42:31.031 | INFO     | __main__:merge_datasets:80 - Processing val split
2024-08-10 15:42:31.041 | INFO     | __main__:merge_datasets:86 - Processing new-version-test_f465306be03
ced999caa157a85558a6c41b3e3f5
2024-08-10 15:42:31.389 | INFO     | __main__:merge_datasets:98 - Deleted processed file: /raid/adarsh/da
tasets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5/novel_premises/val.json
2024-08-10 15:42:31.664 | INFO     | __main__:merge_datasets:104 - Finished processing val split
2024-08-10 15:42:31.664 | INFO     | __main__:merge_datasets:80 - Processing test split
2024-08-10 15:42:31.665 | INFO     | __main__:merge_datasets:86 - Processing new-version-test_f465306be03
ced999caa157a85558a6c41b3e3f5
2024-08-10 15:42:31.740 | INFO     | __main__:merge_datasets:98 - Deleted processed file: /raid/adarsh/da
tasets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5/novel_premises/test.json
2024-08-10 15:42:32.000 | INFO     | __main__:merge_datasets:104 - Finished processing test split
2024-08-10 15:42:32.000 | INFO     | __main__:merge_datasets:105 - Finished merging datasets for novel_pr
emises
2024-08-10 15:42:32.000 | INFO     | __main__:merge_datasets:107 - Merging corpus
2024-08-10 15:42:32.000 | INFO     | __main__:merge_datasets:112 - Processing new-version-test_f465306be0
3ced999caa157a85558a6c41b3e3f5
2024-08-10 15:42:32.449 | INFO     | __main__:merge_datasets:124 - Deleted processed corpus file: /raid/a
darsh/datasets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5/corpus.jsonl
2024-08-10 15:42:32.513 | INFO     | __main__:merge_datasets:130 - Finished merging corpus
2024-08-10 15:42:32.513 | INFO     | __main__:merge_datasets:132 - Adding metadata
2024-08-10 15:42:32.513 | INFO     | __main__:merge_datasets:137 - Checking for metadata in new-version-t
est_f465306be03ced999caa157a85558a6c41b3e3f5
2024-08-10 15:42:32.514 | INFO     | __main__:merge_datasets:145 - Deleted processed metadata file: /raid
/adarsh/datasets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5/metadata.json
2024-08-10 15:42:32.514 | INFO     | __main__:merge_datasets:150 - Finished adding metadata
2024-08-10 15:42:32.514 | INFO     | __main__:merge_datasets:154 - Deleting individual datasets
2024-08-10 15:42:32.514 | INFO     | __main__:merge_datasets:158 - Deleting dataset: new-version-test_f46
5306be03ced999caa157a85558a6c41b3e3f5
2024-08-10 15:42:32.525 | INFO     | __main__:generate_dataset:249 - Finished merging datasets
2024-08-10 15:42:32.634 | INFO     | __main__:main:639 - Latest PL checkpoint found: AK123321/pl-leancopi
lot-2
2024-08-10 15:42:32.733 | INFO     | __main__:download_pl_checkpoint:274 - Checkpoint downloaded to: /hom
e/adarsh/.cache/huggingface/hub/models--AK123321--pl-leancopilot-2/snapshots/d4955ed1972ad84c03dce9189162
d03912f2d86f/model.ckpt
2024-08-10 15:42:32.734 | INFO     | __main__:main:642 - Checkpoint path: /home/adarsh/.cache/huggingface
/hub/models--AK123321--pl-leancopilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86f/model.ckpt
2024-08-10 15:42:32.734 | INFO     | __main__:train:464 - Training model with checkpoint: /home/adarsh/.c
ache/huggingface/hub/models--AK123321--pl-leancopilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86
f/model.ckpt
Seed set to 3407
Lightning automatically upgraded your loaded checkpoint from v0.0.0 to v2.4.0. To apply the upgrade to yo
ur files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../.cache/huggingface
/hub/models--AK123321--pl-leancopilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86f/model.ckpt`
2024-08-10 15:42:58.250 | INFO     | __main__:train:486 - Loaded premise retriever at /home/adarsh/.cache
/huggingface/hub/models--AK123321--pl-leancopilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86f/mo
del.ckpt
2024-08-10 15:42:58.251 | INFO     | __main__:train:490 - Data path: /raid/adarsh/datasets_test/merged/ra
ndom
/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/transformers/tokenization
_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True`
by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by def
ault. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
2024-08-10 15:42:58.347 | INFO     | common:__init__:200 - Building the corpus from /raid/adarsh/datasets
_test/merged/corpus.jsonl
2024-08-10 15:43:19.523 | INFO     | retrieval.datamodule:load_or_cache_data:52 - Loaded data from cache
/raid/adarsh/datasets_test/merged/random/cache_train/cached_data.pkl
Training dataset size: 353052
2024-08-10 15:43:19.555 | INFO     | retrieval.datamodule:load_or_cache_data:52 - Loaded data from cache
/raid/adarsh/datasets_test/merged/random/cache_val/cached_data.pkl
Validation dataset size: 5403
2024-08-10 15:43:19.586 | INFO     | retrieval.datamodule:load_or_cache_data:52 - Loaded data from cache
/raid/adarsh/datasets_test/merged/random/cache_pred/cached_data.pkl
Testing dataset size: 5321
2024-08-10 15:43:19.588 | INFO     | __main__:train:523 - Training dataset size after load: 353052
2024-08-10 15:43:19.588 | INFO     | __main__:train:524 - Validation dataset size after load: 5403
2024-08-10 15:43:19.588 | INFO     | __main__:train:525 - Testing dataset size after load: 5321
Using bfloat16 Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
2024-08-10 15:43:19.598 | INFO     | __main__:train:543 - Starting progressive training...
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 1 processes
----------------------------------------------------------------------------------------------------

2024-08-10 15:43:31.912 | INFO     | retrieval.datamodule:load_or_cache_data:52 - Loaded data from cache
/raid/adarsh/datasets_test/merged/random/cache_train/cached_data.pkl
Training dataset size: 353052
2024-08-10 15:43:32.111 | INFO     | retrieval.datamodule:load_or_cache_data:52 - Loaded data from cache
/raid/adarsh/datasets_test/merged/random/cache_val/cached_data.pkl
Validation dataset size: 5403
2024-08-10 15:43:32.150 | INFO     | retrieval.datamodule:load_or_cache_data:52 - Loaded data from cache
/raid/adarsh/datasets_test/merged/random/cache_pred/cached_data.pkl
Testing dataset size: 5321
/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning/callbac
ks/model_checkpoint.py:654: Checkpoint directory /raid/adarsh/checkpoints_test exists and is not empty.
Restoring states from the checkpoint path at /home/adarsh/.cache/huggingface/hub/models--AK123321--pl-lea
ncopilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86f/model.ckpt
Lightning automatically upgraded your loaded checkpoint from v0.0.0 to v2.4.0. To apply the upgrade to yo
ur files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../.cache/huggingface
/hub/models--AK123321--pl-leancopilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86f/model.ckpt`
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
2024-08-10 15:43:33.433 | INFO     | common:get_optimizers:434 - Optimizing with GaLoreAdamW
/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/galore_torch/adamw.py:48:
 FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use t
he PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this
 warning
  warnings.warn(

  | Name    | Type           | Params | Mode
---------------------------------------------------
0 | encoder | T5EncoderModel | 217 M  | train
---------------------------------------------------
217 M     Trainable params
0         Non-trainable params
217 M     Total params
870.630   Total estimated model params size (MB)
235       Modules in train mode
0         Modules in eval mode
2024-08-10 15:43:37.776 | INFO     | __main__:main:651 - An error occurred: loaded state dict has a diffe
rent number of parameter groups
Traceback (most recent call last):
  File "/home/adarsh/ReProver/compute_server.py", line 645, in main
    train(model_checkpoint_path, merged_data_path, next_suffix)
  File "/home/adarsh/ReProver/compute_server.py", line 545, in train
    trainer.fit(model, datamodule=data_module, ckpt_path=model_checkpoint_path)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/trainer/call.py", line 46, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/trainer/trainer.py", line 972, in _run
    self._checkpoint_connector.restore_training_state()
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/trainer/connectors/checkpoint_connector.py", line 298, in restore_training_state
    self.restore_optimizers_and_schedulers()
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/trainer/connectors/checkpoint_connector.py", line 368, in restore_optimizers_and_schedulers
    self.restore_optimizers()
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/trainer/connectors/checkpoint_connector.py", line 383, in restore_optimizers
    self.trainer.strategy.load_optimizer_state_dict(self._loaded_checkpoint)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/strategies/strategy.py", line 376, in load_optimizer_state_dict
    optimizer.load_state_dict(opt_state)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/torch/_compile.py
", line 31, in inner
    return disable_fn(*args, **kwargs)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/torch/_dynamo/eva
l_frame.py", line 600, in _fn
    return fn(*args, **kwargs)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/torch/optim/optim
izer.py", line 848, in load_state_dict
    raise ValueError(
ValueError: loaded state dict has a different number of parameter groups
(ReProverComputeServer) adarsh@tensorlab-DGX-Station-A100-920-23487-2531-000:~/ReProver$(ReProverComputeS
(ReProverComputeServer) adarsh@tensorlab-DGX-Station-A100-920-23487-2531-000:~/ReProver$ bash run_compute
_server.sh
Script executed from: /home/adarsh/ReProver
[2024-08-10 15:51:41,518] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda
 (auto detect)
 [WARNING]  async_io requires the dev libaio .so object and headers but these were not found.
 [WARNING]  async_io: please install the libaio-dev package with apt
 [WARNING]  If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS envi
ronment variables to where it can be found.
 [WARNING]  Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 [WARNING]  sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
 [WARNING]  using untested triton version (3.0.0), only 1.0.0 is known to be compatible
/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/deepspeed/runtime/zero/li
near.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.cust
om_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/deepspeed/runtime/zero/li
near.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.cust
om_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
2024-08-10 15:51:50.114 | INFO     | __main__:check_progress_file:586 - Checking contents of /home/adarsh
/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning/loops/progress.py
2024-08-10 15:51:50.115 | INFO     | __main__:check_progress_file:590 - Contents of progress.py:
2024-08-10 15:51:50.115 | INFO     | __main__:check_progress_file:591 - # Copyright The Lightning AI team
.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import asdict, dataclass, field
from typing import Type

from typing_extensions import override


@dataclass
class _BaseProgress:
    """Mixin that implements state-loading utilities for dataclasses."""

    def state_dict(self) -> dict:
        return asdict(self)

    def load_state_dict(self, state_dict: dict) -> None:
        if state_dict["completed"] == None:
            state_dict["completed"] = 0
        self.__dict__.update(state_dict)

    @classmethod
    def from_state_dict(cls, state_dict: dict) -> "_BaseProgress":
        obj = cls()
        obj.load_state_dict(state_dict)
        return obj

    def reset(self) -> None:
        """Reset the object's state."""
        raise NotImplementedError


@dataclass
class _ReadyCompletedTracker(_BaseProgress):
    """Track an event's progress.

    Args:
        ready: Intended to track the number of events ready to start.
        completed: Intended to be incremented after the event completes (e.g. after ``on_*_end`` runs).

    These attributes should be increased in order, that is, :attr:`ready` first and :attr:`completed` las
t.

    """

    ready: int = 0
    completed: int = 0

    @override
    def reset(self) -> None:
        """Reset the state."""
        self.ready = 0
        self.completed = 0

    def reset_on_restart(self) -> None:
        """Reset the progress on restart.

        If there is a failure before all attributes are increased, restore the attributes to the last ful
ly completed
        value.

        """
        self.ready = self.completed


@dataclass
class _StartedTracker(_ReadyCompletedTracker):
    """Track an event's progress.

    Args:
        ready: Intended to track the number of events ready to start.
        started: Intended to be incremented after the event is started (e.g. after ``on_*_start`` runs).
        completed: Intended to be incremented after the event completes (e.g. after ``on_*_end`` runs).

    These attributes should be increased in order, that is, :attr:`ready` first and :attr:`completed` las
t.

    """

    started: int = 0

    @override
    def reset(self) -> None:
        super().reset()
        self.started = 0

    @override
    def reset_on_restart(self) -> None:
        super().reset_on_restart()
        self.started = self.completed


@dataclass
class _ProcessedTracker(_StartedTracker):
    """Track an event's progress.

    Args:
        ready: Intended to track the number of events ready to start.
        started: Intended to be incremented after the event is started (e.g. after ``on_*_start`` runs).
        processed: Intended to be incremented after the event is processed.
        completed: Intended to be incremented after the event completes (e.g. after ``on_*_end`` runs).

    These attributes should be increased in order, that is, :attr:`ready` first and :attr:`completed` las
t.

    """

    processed: int = 0

    @override
    def reset(self) -> None:
        super().reset()
        self.processed = 0

    @override
    def reset_on_restart(self) -> None:
        super().reset_on_restart()
        self.processed = self.completed


@dataclass
class _Progress(_BaseProgress):
    """Track aggregated and current progress.

    Args:
        total: Intended to track the total progress of an event.
        current: Intended to track the current progress of an event.

    """

    total: _ReadyCompletedTracker = field(default_factory=_ProcessedTracker)
    current: _ReadyCompletedTracker = field(default_factory=_ProcessedTracker)

    def __post_init__(self) -> None:
        if self.total.__class__ is not self.current.__class__:
            raise ValueError("The `total` and `current` instances should be of the same class")

    def increment_ready(self) -> None:
        self.total.ready += 1
        self.current.ready += 1

    def increment_started(self) -> None:
        if not isinstance(self.total, _StartedTracker):
            raise TypeError(f"`{self.total.__class__.__name__}` doesn't have a `started` attribute")
        self.total.started += 1
        self.current.started += 1

    def increment_processed(self) -> None:
        if not isinstance(self.total, _ProcessedTracker):
            raise TypeError(f"`{self.total.__class__.__name__}` doesn't have a `processed` attribute")
        self.total.processed += 1
        self.current.processed += 1

    def increment_completed(self) -> None:
        self.total.completed += 1
        self.current.completed += 1

    @classmethod
    def from_defaults(cls, tracker_cls: Type[_ReadyCompletedTracker], **kwargs: int) -> "_Progress":
        """Utility function to easily create an instance from keyword arguments to both ``Tracker``s."""
        return cls(total=tracker_cls(**kwargs), current=tracker_cls(**kwargs))

    @override
    def reset(self) -> None:
        self.total.reset()
        self.current.reset()

    def reset_on_run(self) -> None:
        self.current.reset()

    def reset_on_restart(self) -> None:
        self.current.reset_on_restart()

    @override
    def load_state_dict(self, state_dict: dict) -> None:
        if state_dict["total"]["completed"] == None:
            state_dict["total"]["completed"] = 0
        self.total.load_state_dict(state_dict["total"])
        self.current.load_state_dict(state_dict["current"])


@dataclass
class _BatchProgress(_Progress):
    """Tracks batch progress.

    These counters are local to a trainer rank. By default, they are not globally synced across all ranks
.

    Args:
        total: Tracks the total batch progress.
        current: Tracks the current batch progress.
        is_last_batch: Whether the batch is the last one. This is useful for iterable datasets.

    """

    is_last_batch: bool = False

    @override
    def reset(self) -> None:
        super().reset()
        self.is_last_batch = False

    @override
    def reset_on_run(self) -> None:
        super().reset_on_run()
        self.is_last_batch = False

    @override
    def load_state_dict(self, state_dict: dict) -> None:
        if state_dict["total"]["completed"] == None:
            state_dict["total"]["completed"] = 0
        super().load_state_dict(state_dict)
        self.is_last_batch = state_dict["is_last_batch"]


@dataclass
class _SchedulerProgress(_Progress):
    """Tracks scheduler progress.

    These counters are local to a trainer rank. By default, they are not globally synced across all ranks
.

    Args:
        total: Tracks the total scheduler progress.
        current: Tracks the current scheduler progress.

    """

    total: _ReadyCompletedTracker = field(default_factory=_ReadyCompletedTracker)
    current: _ReadyCompletedTracker = field(default_factory=_ReadyCompletedTracker)


@dataclass
class _OptimizerProgress(_BaseProgress):
    """Track optimizer progress.

    Args:
        step: Tracks ``optimizer.step`` calls.
        zero_grad: Tracks ``optimizer.zero_grad`` calls.

    """

    step: _Progress = field(default_factory=lambda: _Progress.from_defaults(_ReadyCompletedTracker))
    zero_grad: _Progress = field(default_factory=lambda: _Progress.from_defaults(_StartedTracker))

    @override
    def reset(self) -> None:
        self.step.reset()
        self.zero_grad.reset()

    def reset_on_run(self) -> None:
        self.step.reset_on_run()
        self.zero_grad.reset_on_run()

    def reset_on_restart(self) -> None:
        self.step.reset_on_restart()
        self.zero_grad.reset_on_restart()

    @override
    def load_state_dict(self, state_dict: dict) -> None:
        if state_dict["step"]["total"]["completed"] == None:
            state_dict["step"]["total"]["completed"] = 0
        self.step.load_state_dict(state_dict["step"])
        self.zero_grad.load_state_dict(state_dict["zero_grad"])


@dataclass
class _OptimizationProgress(_BaseProgress):
    """Track optimization progress.

    Args:
        optimizer: Tracks optimizer progress.

    """

    optimizer: _OptimizerProgress = field(default_factory=_OptimizerProgress)

    @property
    def optimizer_steps(self) -> int:
        return self.optimizer.step.total.completed

    @override
    def reset(self) -> None:
        self.optimizer.reset()

    def reset_on_run(self) -> None:
        self.optimizer.reset_on_run()

    def reset_on_restart(self) -> None:
        self.optimizer.reset_on_restart()

    @override
    def load_state_dict(self, state_dict: dict) -> None:
        if state_dict["optimizer"]["step"]["total"]["completed"] == None:
            state_dict["optimizer"]["step"]["total"]["completed"] = 0
        self.optimizer.load_state_dict(state_dict["optimizer"])

2024-08-10 15:51:50.115 | INFO     | __main__:main:608 - Starting compute server...
2024-08-10 15:51:50.115 | INFO     | __main__:main:609 - Current working directory: /home/adarsh/ReProver
2024-08-10 15:51:50.115 | INFO     | __main__:main:617 - ROOT_DIR: /raid/adarsh
2024-08-10 15:51:50.115 | INFO     | __main__:main:618 - DATA_DIR: datasets_test
2024-08-10 15:51:50.115 | INFO     | __main__:main:622 - Configuring LeanDojo...
2024-08-10 15:51:50.119 | INFO     | generate_benchmark_lean4:configure_leandojo:294 - Current working di
rectory: /home/adarsh/ReProver
2024-08-10 15:51:50.119 | INFO     | __main__:main:624 - LeanDojo configured
2024-08-10 15:51:50.119 | INFO     | __main__:main:632 - Unique URLs: {'https://github.com/Adarsh321123/n
ew-version-test.git'}
2024-08-10 15:51:50.119 | INFO     | __main__:main:633 - About to generate datasets...
2024-08-10 15:51:50.119 | INFO     | __main__:generate_dataset:226 - Generating 1 datasets
2024-08-10 15:51:50.119 | INFO     | __main__:generate_dataset:231 - Processing https://github.com/Adarsh
321123/new-version-test.git
2024-08-10 15:51:50.306 | INFO     | __main__:get_compatible_commit:166 - Latest commit: f465306be03ced99
9caa157a85558a6c41b3e3f5
2024-08-10 15:51:50.306 | INFO     | __main__:get_compatible_commit:169 - Creating LeanGitRepo for https:
//github.com/Adarsh321123/new-version-test
2024-08-10 15:51:50.676 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the com
mit hash for lean4 v4.8.0-rc1
2024-08-10 15:51:51.272 | INFO     | __main__:get_compatible_commit:171 - Getting config for https://gith
ub.com/Adarsh321123/new-version-test.git
2024-08-10 15:51:51.272 | INFO     | __main__:get_compatible_commit:175 - Latest commit compatible for ur
l https://github.com/Adarsh321123/new-version-test.git
2024-08-10 15:51:51.272 | INFO     | __main__:generate_dataset:236 - Found compatible commit f465306be03c
ed999caa157a85558a6c41b3e3f5 for https://github.com/Adarsh321123/new-version-test.git
2024-08-10 15:51:51.272 | INFO     | __main__:generate_dataset:237 - Lean version: v4.8.0-rc1
2024-08-10 15:51:51.272 | INFO     | __main__:generate_dataset:240 - Creating LeanGitRepo for https://git
hub.com/Adarsh321123/new-version-test
2024-08-10 15:51:51.272 | INFO     | __main__:generate_dataset:244 - Generating benchmark at /raid/adarsh
/datasets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5
2024-08-10 15:51:51.273 | INFO     | generate_benchmark_lean4:main:301 - Generating dataset to go into /r
aid/adarsh/datasets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5
2024-08-10 15:51:51.273 | INFO     | generate_benchmark_lean4:main:308 - lean toolchain version: {'conten
t': 'leanprover/lean4:v4.8.0-rc1\n'}
2024-08-10 15:51:51.273 | INFO     | generate_benchmark_lean4:main:310 - lean version v: v4.8.0-rc1
2024-08-10 15:51:51.273 | INFO     | generate_benchmark_lean4:main:311 - is supported: True
2024-08-10 15:51:51.273 | INFO     | generate_benchmark_lean4:main:321 - lean path1 /home/adarsh/.elan/to
olchains/leanprover--lean4---4.8.0-rc1
2024-08-10 15:51:51.273 | INFO     | generate_benchmark_lean4:main:322 - lean path2 /.elan/toolchains/lea
nprover--lean4---4.8.0-rc1
2024-08-10 15:51:51.273 | INFO     | generate_benchmark_lean4:main:323 - lean path3 ~/.elan/toolchains/le
anprover--lean4---4.8.0-rc1
2024-08-10 15:51:51.273 | INFO     | generate_benchmark_lean4:main:327 - Lean toolchain path 2 does not e
xist: /.elan/toolchains/leanprover--lean4---4.8.0-rc1
2024-08-10 15:51:51.273 | INFO     | generate_benchmark_lean4:main:329 - Lean toolchain path 3 does not e
xist: ~/.elan/toolchains/leanprover--lean4---4.8.0-rc1
2024-08-10 15:51:51.273 | INFO     | generate_benchmark_lean4:main:332 - Switched to Lean toolchain at: /
home/adarsh/.elan/toolchains/leanprover--lean4---4.8.0-rc1
2024-08-10 15:51:51.306 | INFO     | generate_benchmark_lean4:main:334 - lean --version: Lean (version 4.
8.0-rc1, x86_64-unknown-linux-gnu, commit dcccfb73cb24, Release)

2024-08-10 15:51:51.306 | INFO     | generate_benchmark_lean4:main:335 - repo: LeanGitRepo(url='https://g
ithub.com/Adarsh321123/new-version-test', commit='f465306be03ced999caa157a85558a6c41b3e3f5')
2024-08-10 15:51:51.306 | INFO     | generate_benchmark_lean4:main:337 - Configuring LeanDojo again...
2024-08-10 15:51:51.309 | INFO     | generate_benchmark_lean4:configure_leandojo:294 - Current working di
rectory: /home/adarsh/ReProver
2024-08-10 15:51:51.309 | INFO     | generate_benchmark_lean4:main:339 - LeanDojo configured
2024-08-10 15:51:51.309 | INFO     | generate_benchmark_lean4:main:342 - Tracing the repo...
2024-08-10 15:51:51.310 | DEBUG    | lean_dojo.data_extraction.trace:get_traced_repo_path:217 - The trace
d repo is available in the cache.
2024-08-10 15:51:51.310 | INFO     | lean_dojo.data_extraction.trace:trace:246 - Loading the traced repo
from /raid/adarsh/.cache/lean_dojo/Adarsh321123-new-version-test-f465306be03ced999caa157a85558a6c41b3e3f5
/new-version-test
2024-08-10 15:51:51.312 | DEBUG    | lean_dojo.utils:execute:110 - git remote get-url origin
2024-08-10 15:51:51.315 | DEBUG    | lean_dojo.utils:execute:110 - git log -n 1
2024-08-10 15:51:51.453 | DEBUG    | lean_dojo.data_extraction.traced_data:load_from_disk:1169 - Loading
5432 traced XML files from /raid/adarsh/.cache/lean_dojo/Adarsh321123-new-version-test-f465306be03ced999c
aa157a85558a6c41b3e3f5/new-version-test with 31 workers
2024-08-10 15:51:52,753 INFO worker.py:1772 -- Started a local Ray instance. View the dashboard at 127.0.
0.1:8267
  3%|██▏                                                              | 188/5432 [00:06<03:53, 22.46it/s]
(raylet) [2024-08-10 15:52:02,729 E 250501 250531] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-51-51_574039_249508 is over 95% full, available space: 88087744512; capacity: 1887507697664
. Object creation will fail if spilling is required.
  8%|████▉                                                            | 410/5432 [00:16<04:57, 16.90it/s]
(raylet) [2024-08-10 15:52:12,737 E 250501 250531] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-51-51_574039_249508 is over 95% full, available space: 91312033792; capacity: 1887507697664
. Object creation will fail if spilling is required.
 12%|███████▉                                                         | 660/5432 [00:26<01:55, 41.28it/s]
(raylet) [2024-08-10 15:52:22,745 E 250501 250531] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-51-51_574039_249508 is over 95% full, available space: 87885856768; capacity: 1887507697664
. Object creation will fail if spilling is required.
 15%|█████████▋                                                       | 814/5432 [00:33<01:15, 61.45it/s]
(raylet) [2024-08-10 15:52:32,752 E 250501 250531] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-51-51_574039_249508 is over 95% full, available space: 88604549120; capacity: 1887507697664
. Object creation will fail if spilling is required.
 16%|██████████▋                                                      | 895/5432 [00:41<02:07, 35.46it/s]
(raylet) [2024-08-10 15:52:42,759 E 250501 250531] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-51-51_574039_249508 is over 95% full, available space: 85732720640; capacity: 1887507697664
. Object creation will fail if spilling is required.
 19%|███████████▉                                                    | 1012/5432 [00:51<02:25, 30.30it/s]
(raylet) [2024-08-10 15:52:52,931 E 250501 250531] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-51-51_574039_249508 is over 95% full, available space: 86138023936; capacity: 1887507697664
. Object creation will fail if spilling is required.
 21%|█████████████▌                                                  | 1153/5432 [01:02<01:19, 54.16it/s]
(raylet) [2024-08-10 15:53:02,939 E 250501 250531] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-51-51_574039_249508 is over 95% full, available space: 85725331456; capacity: 1887507697664
. Object creation will fail if spilling is required.
 24%|███████████████                                                 | 1278/5432 [01:31<00:59, 70.08it/s]
(raylet) [2024-08-10 15:53:12,946 E 250501 250531] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-51-51_574039_249508 is over 95% full, available space: 85725257728; capacity: 1887507697664
. Object creation will fail if spilling is required.
 24%|███████████████                                                 | 1281/5432 [01:31<40:41,  1.70it/s]
(raylet) [2024-08-10 15:53:22,958 E 250501 250531] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-51-51_574039_249508 is over 95% full, available space: 85725245440; capacity: 1887507697664
. Object creation will fail if spilling is required.
 27%|█████████████████                                               | 1446/5432 [01:51<01:37, 40.91it/s]
(raylet) [2024-08-10 15:53:32,966 E 250501 250531] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-51-51_574039_249508 is over 95% full, available space: 85725224960; capacity: 1887507697664
. Object creation will fail if spilling is required.
 27%|████████████████▌                                             | 1449/5432 [01:51<1:02:53,  1.06it/s]
(raylet) [2024-08-10 15:53:42,974 E 250501 250531] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-51-51_574039_249508 is over 95% full, available space: 85725220864; capacity: 1887507697664
. Object creation will fail if spilling is required.
 31%|███████████████████▋                                            | 1675/5432 [02:16<00:48, 76.79it/s]
(raylet) [2024-08-10 15:53:52,981 E 250501 250531] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-51-51_574039_249508 is over 95% full, available space: 85725159424; capacity: 1887507697664
. Object creation will fail if spilling is required.
 31%|███████████████████▊                                            | 1681/5432 [02:16<48:40,  1.28it/s]
(raylet) [2024-08-10 15:54:02,988 E 250501 250531] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-51-51_574039_249508 is over 95% full, available space: 85724057600; capacity: 1887507697664
. Object creation will fail if spilling is required.
 36%|███████████████████████▏                                        | 1968/5432 [02:49<01:02, 55.14it/s]
(raylet) [2024-08-10 15:54:23,001 E 250501 250531] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-51-51_574039_249508 is over 95% full, available space: 85724024832; capacity: 1887507697664
. Object creation will fail if spilling is required. [repeated 2x across cluster] (Ray deduplicates logs
by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-o
bservability/user-guides/configure-logging.html#log-deduplication for more options.)
 42%|██████████████████████████▊                                     | 2278/5432 [02:55<01:43, 30.51it/s]
(raylet) [2024-08-10 15:54:53,024 E 250501 250531] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-51-51_574039_249508 is over 95% full, available space: 85723914240; capacity: 1887507697664
. Object creation will fail if spilling is required. [repeated 3x across cluster]
 48%|██████████████████████████████▉                                 | 2631/5432 [04:16<00:58, 48.22it/s]
(raylet) [2024-08-10 15:55:33,057 E 250501 250531] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-51-51_574039_249508 is over 95% full, available space: 85722353664; capacity: 1887507697664
. Object creation will fail if spilling is required. [repeated 4x across cluster]
 59%|█████████████████████████████████████▋                          | 3195/5432 [05:15<00:42, 52.13it/s]
(raylet) [2024-08-10 15:56:23,095 E 250501 250531] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-51-51_574039_249508 is over 95% full, available space: 85721481216; capacity: 1887507697664
. Object creation will fail if spilling is required. [repeated 5x across cluster]
 70%|████████████████████████████████████████████▋                   | 3789/5432 [05:27<00:24, 68.33it/s]
(raylet) [2024-08-10 15:57:23,135 E 250501 250531] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-51-51_574039_249508 is over 95% full, available space: 85720027136; capacity: 1887507697664
. Object creation will fail if spilling is required. [repeated 6x across cluster]
 71%|████████████████████████████████████████████▏                 | 3866/5432 [06:28<1:16:27,  2.93s/it]
(raylet) [2024-08-10 15:57:33,140 E 250501 250531] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-51-51_574039_249508 is over 95% full, available space: 85719998464; capacity: 1887507697664
. Object creation will fail if spilling is required.
(raylet) [2024-08-10 15:57:43,146 E 250501 250531] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-51-51_574039_249508 is over 95% full, available space: 85719973888; capacity: 1887507697664
. Object creation will fail if spilling is required.
 78%|█████████████████████████████████████████████████▋              | 4219/5432 [06:37<00:37, 32.78it/s]
(raylet) [2024-08-10 15:58:33,207 E 250501 250531] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-51-51_574039_249508 is over 95% full, available space: 85718994944; capacity: 1887507697664
. Object creation will fail if spilling is required. [repeated 5x across cluster]
 82%|████████████████████████████████████████████████████▋           | 4468/5432 [06:43<00:20, 46.23it/s]
(raylet) [2024-08-10 15:58:43,213 E 250501 250531] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-51-51_574039_249508 is over 95% full, available space: 85718962176; capacity: 1887507697664
. Object creation will fail if spilling is required.
 82%|███████████████████████████████████████████████████           | 4470/5432 [07:58<1:12:45,  4.54s/it]
(raylet) [2024-08-10 15:58:53,220 E 250501 250531] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-51-51_574039_249508 is over 95% full, available space: 85718937600; capacity: 1887507697664
. Object creation will fail if spilling is required.
 88%|████████████████████████████████████████████████████████▎       | 4781/5432 [08:07<00:11, 55.08it/s]
(raylet) [2024-08-10 16:00:03,278 E 250501 250531] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-51-51_574039_249508 is over 95% full, available space: 85716840448; capacity: 1887507697664
. Object creation will fail if spilling is required. [repeated 7x across cluster]
 99%|███████████████████████████████████████████████████████████████ | 5354/5432 [08:15<00:01, 74.49it/s]
(raylet) [2024-08-10 16:00:13,284 E 250501 250531] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-51-51_574039_249508 is over 95% full, available space: 85716815872; capacity: 1887507697664
. Object creation will fail if spilling is required.
 99%|███████████████████████████████████████████████████████████████▏| 5358/5432 [09:46<02:27,  2.00s/it]
(raylet) [2024-08-10 16:00:23,291 E 250501 250531] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-51-51_574039_249508 is over 95% full, available space: 85716803584; capacity: 1887507697664
. Object creation will fail if spilling is required.
100%|████████████████████████████████████████████████████████████████| 5432/5432 [09:46<00:00,  9.26it/s]
(raylet) [2024-08-10 16:01:33,341 E 250501 250531] (raylet) file_system_monitor.cc:111: /tmp/ray/session_
2024-08-10_15-51-51_574039_249508 is over 95% full, available space: 85715513344; capacity: 1887507697664
. Object creation will fail if spilling is required. [repeated 7x across cluster]
2024-08-10 16:01:44.609 | DEBUG    | lean_dojo.data_extraction.lean:get_dependencies:489 - Querying the d
ependencies of LeanGitRepo(url='https://github.com/Adarsh321123/new-version-test', commit='f465306be03ced
999caa157a85558a6c41b3e3f5')
2024-08-10 16:01:45.622 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the com
mit hash for lean4 v4.8.0-rc2
2024-08-10 16:01:46.908 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the com
mit hash for lean4 v4.6.0-rc1
2024-08-10 16:01:49.114 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the com
mit hash for lean4 v4.7.0
2024-08-10 16:01:50.057 | DEBUG    | lean_dojo.data_extraction.lean:url_to_repo:68 - url_to_repo("https:/
/github.com/leanprover-community/import-graph.git") failed. Retrying...
2024-08-10 16:01:51.182 | DEBUG    | lean_dojo.data_extraction.lean:url_to_repo:68 - url_to_repo("https:/
/github.com/leanprover-community/import-graph.git") failed. Retrying...
2024-08-10 16:01:54.485 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the com
mit hash for lean4 v4.11.0-rc1
2024-08-10 16:01:55.059 | DEBUG    | lean_dojo.data_extraction.lean:get_dependencies:489 - Querying the d
ependencies of LeanGitRepo(url='https://github.com/leanprover-community/mathlib4', commit='19796eea79b36c
81ba5b1de34f5500e0348c84a2')
2024-08-10 16:01:55.645 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the com
mit hash for lean4 v4.10.0
2024-08-10 16:01:56.748 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:88 - Querying the com
mit hash for lean4 v4.9.0
2024-08-10 16:02:25.412 | DEBUG    | lean_dojo.data_extraction.traced_data:check_sanity:1032 - Checking t
he sanity of TracedRepo(repo=LeanGitRepo(url='https://github.com/Adarsh321123/new-version-test', commit='
f465306be03ced999caa157a85558a6c41b3e3f5'), dependencies={'lean4': LeanGitRepo(url='https://github.com/le
anprover/lean4', commit='daa22187642d4cf6954c39a23eab20d8a8675416'), 'batteries': LeanGitRepo(url='https:
//github.com/leanprover-community/batteries', commit='dc167d260ff7ee9849b436037add06bed15104be'), 'Qq': L
eanGitRepo(url='https://github.com/leanprover-community/quote4', commit='71f54425e6fe0fa75f3aef33a2813a78
98392222'), 'aesop': LeanGitRepo(url='https://github.com/leanprover-community/aesop', commit='0444234b421
6e944d5be2ce42a25d7410c67876f'), 'proofwidgets': LeanGitRepo(url='https://github.com/leanprover-community
/ProofWidgets4', commit='a96aee5245720f588876021b6a0aa73efee49c76'), 'Cli': LeanGitRepo(url='https://gith
ub.com/leanprover/lean4-cli', commit='2cf1030dc2ae6b3632c84a09350b675ef3e347d0'), 'mathlib': LeanGitRepo(
url='https://github.com/leanprover-community/mathlib4', commit='19796eea79b36c81ba5b1de34f5500e0348c84a2'
), 'importGraph': LeanGitRepo(url='https://github.com/leanprover-community/import-graph', commit='57bd206
5f1dbea5e9235646fb836c7cea9ab03b6')}, root_dir=PosixPath('/raid/adarsh/.cache/lean_dojo/Adarsh321123-new-
version-test-f465306be03ced999caa157a85558a6c41b3e3f5/new-version-test'))
2024-08-10 16:02:39.858 | INFO     | generate_benchmark_lean4:main:344 - Successfully traced the repo
2024-08-10 16:02:51.060 | INFO     | generate_benchmark_lean4:split_data:132 - 119751 theorems in total
2024-08-10 16:02:51.061 | INFO     | generate_benchmark_lean4:split_data:137 - 119751 theorems in total,
with 2395 for validation and 2395 for testing
2024-08-10 16:02:51.061 | INFO     | generate_benchmark_lean4:split_randomly:80 - Splitting the theorems
randomly
2024-08-10 16:02:51.108 | INFO     | generate_benchmark_lean4:split_by_premise:94 - Splitting the theorem
s by premises
2024-08-10 16:03:01.573 | INFO     | generate_benchmark_lean4:main:352 - Successfully split the data
2024-08-10 16:03:29.561 | INFO     | generate_benchmark_lean4:export_proofs:194 - 114961 theorems and 243
249 tactics saved to /raid/adarsh/datasets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5
/random/train.json
2024-08-10 16:03:30.462 | INFO     | generate_benchmark_lean4:export_proofs:194 - 2395 theorems and 5027
tactics saved to /raid/adarsh/datasets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5/ran
dom/val.json
2024-08-10 16:03:31.043 | INFO     | generate_benchmark_lean4:export_proofs:194 - 2395 theorems and 4807
tactics saved to /raid/adarsh/datasets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5/ran
dom/test.json
2024-08-10 16:03:56.554 | INFO     | generate_benchmark_lean4:export_proofs:194 - 114961 theorems and 238
014 tactics saved to /raid/adarsh/datasets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5
/novel_premises/train.json
2024-08-10 16:03:57.633 | INFO     | generate_benchmark_lean4:export_proofs:194 - 2395 theorems and 7418
tactics saved to /raid/adarsh/datasets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5/nov
el_premises/val.json
2024-08-10 16:03:58.435 | INFO     | generate_benchmark_lean4:export_proofs:194 - 2395 theorems and 7651
tactics saved to /raid/adarsh/datasets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5/nov
el_premises/test.json
2024-08-10 16:03:58.442 | INFO     | generate_benchmark_lean4:export_data:270 - Successfully exported the
 proofs
2024-08-10 16:04:29.612 | INFO     | generate_benchmark_lean4:export_premises:218 - 175685 theorems/defin
itions from 5432 files saved to /raid/adarsh/datasets_test/new-version-test_f465306be03ced999caa157a85558
a6c41b3e3f5/corpus.jsonl
2024-08-10 16:04:29.612 | INFO     | generate_benchmark_lean4:export_data:274 - Successfully exported the
 premises
2024-08-10 16:04:29.613 | INFO     | generate_benchmark_lean4:export_data:282 - Successfully exported the
 metadata
2024-08-10 16:04:29.613 | INFO     | generate_benchmark_lean4:main:354 - Successfully exported the data
2024-08-10 16:04:29.754 | INFO     | __main__:generate_dataset:246 - Finished generating benchmark at /ra
id/adarsh/datasets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5
2024-08-10 16:04:29.754 | INFO     | __main__:generate_dataset:247 - Merging datasets
2024-08-10 16:04:29.754 | INFO     | __main__:merge_datasets:74 - Merging datasets for random
2024-08-10 16:04:29.755 | INFO     | __main__:merge_datasets:80 - Processing train split
2024-08-10 16:04:29.755 | INFO     | __main__:merge_datasets:86 - Processing new-version-test_f465306be03
ced999caa157a85558a6c41b3e3f5
2024-08-10 16:04:32.596 | INFO     | __main__:merge_datasets:98 - Deleted processed file: /raid/adarsh/da
tasets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5/random/train.json
2024-08-10 16:04:41.346 | INFO     | __main__:merge_datasets:104 - Finished processing train split
2024-08-10 16:04:41.346 | INFO     | __main__:merge_datasets:80 - Processing val split
2024-08-10 16:04:41.359 | INFO     | __main__:merge_datasets:86 - Processing new-version-test_f465306be03
ced999caa157a85558a6c41b3e3f5
2024-08-10 16:04:41.698 | INFO     | __main__:merge_datasets:98 - Deleted processed file: /raid/adarsh/da
tasets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5/random/val.json
2024-08-10 16:04:41.876 | INFO     | __main__:merge_datasets:104 - Finished processing val split
2024-08-10 16:04:41.876 | INFO     | __main__:merge_datasets:80 - Processing test split
2024-08-10 16:04:41.876 | INFO     | __main__:merge_datasets:86 - Processing new-version-test_f465306be03
ced999caa157a85558a6c41b3e3f5
2024-08-10 16:04:41.931 | INFO     | __main__:merge_datasets:98 - Deleted processed file: /raid/adarsh/da
tasets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5/random/test.json
2024-08-10 16:04:42.102 | INFO     | __main__:merge_datasets:104 - Finished processing test split
2024-08-10 16:04:42.102 | INFO     | __main__:merge_datasets:105 - Finished merging datasets for random
2024-08-10 16:04:42.102 | INFO     | __main__:merge_datasets:74 - Merging datasets for novel_premises
2024-08-10 16:04:42.102 | INFO     | __main__:merge_datasets:80 - Processing train split
2024-08-10 16:04:42.103 | INFO     | __main__:merge_datasets:86 - Processing new-version-test_f465306be03
ced999caa157a85558a6c41b3e3f5
2024-08-10 16:04:44.797 | INFO     | __main__:merge_datasets:98 - Deleted processed file: /raid/adarsh/da
tasets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5/novel_premises/train.json
2024-08-10 16:04:53.369 | INFO     | __main__:merge_datasets:104 - Finished processing train split
2024-08-10 16:04:53.369 | INFO     | __main__:merge_datasets:80 - Processing val split
2024-08-10 16:04:53.381 | INFO     | __main__:merge_datasets:86 - Processing new-version-test_f465306be03
ced999caa157a85558a6c41b3e3f5
2024-08-10 16:04:53.726 | INFO     | __main__:merge_datasets:98 - Deleted processed file: /raid/adarsh/da
tasets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5/novel_premises/val.json
2024-08-10 16:04:53.981 | INFO     | __main__:merge_datasets:104 - Finished processing val split
2024-08-10 16:04:53.981 | INFO     | __main__:merge_datasets:80 - Processing test split
2024-08-10 16:04:53.982 | INFO     | __main__:merge_datasets:86 - Processing new-version-test_f465306be03
ced999caa157a85558a6c41b3e3f5
2024-08-10 16:04:54.057 | INFO     | __main__:merge_datasets:98 - Deleted processed file: /raid/adarsh/da
tasets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5/novel_premises/test.json
2024-08-10 16:04:54.317 | INFO     | __main__:merge_datasets:104 - Finished processing test split
2024-08-10 16:04:54.318 | INFO     | __main__:merge_datasets:105 - Finished merging datasets for novel_pr
emises
2024-08-10 16:04:54.318 | INFO     | __main__:merge_datasets:107 - Merging corpus
2024-08-10 16:04:54.318 | INFO     | __main__:merge_datasets:112 - Processing new-version-test_f465306be0
3ced999caa157a85558a6c41b3e3f5
2024-08-10 16:04:54.767 | INFO     | __main__:merge_datasets:124 - Deleted processed corpus file: /raid/a
darsh/datasets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5/corpus.jsonl
2024-08-10 16:04:54.823 | INFO     | __main__:merge_datasets:130 - Finished merging corpus
2024-08-10 16:04:54.824 | INFO     | __main__:merge_datasets:132 - Adding metadata
2024-08-10 16:04:54.824 | INFO     | __main__:merge_datasets:137 - Checking for metadata in new-version-t
est_f465306be03ced999caa157a85558a6c41b3e3f5
2024-08-10 16:04:54.825 | INFO     | __main__:merge_datasets:145 - Deleted processed metadata file: /raid
/adarsh/datasets_test/new-version-test_f465306be03ced999caa157a85558a6c41b3e3f5/metadata.json
2024-08-10 16:04:54.825 | INFO     | __main__:merge_datasets:150 - Finished adding metadata
2024-08-10 16:04:54.825 | INFO     | __main__:merge_datasets:154 - Deleting individual datasets
2024-08-10 16:04:54.825 | INFO     | __main__:merge_datasets:158 - Deleting dataset: new-version-test_f46
5306be03ced999caa157a85558a6c41b3e3f5
2024-08-10 16:04:54.836 | INFO     | __main__:generate_dataset:249 - Finished merging datasets
2024-08-10 16:04:54.963 | INFO     | __main__:main:639 - Latest PL checkpoint found: AK123321/pl-leancopi
lot-2
2024-08-10 16:04:55.061 | INFO     | __main__:download_pl_checkpoint:274 - Checkpoint downloaded to: /hom
e/adarsh/.cache/huggingface/hub/models--AK123321--pl-leancopilot-2/snapshots/d4955ed1972ad84c03dce9189162
d03912f2d86f/model.ckpt
2024-08-10 16:04:55.061 | INFO     | __main__:main:642 - Checkpoint path: /home/adarsh/.cache/huggingface
/hub/models--AK123321--pl-leancopilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86f/model.ckpt
2024-08-10 16:04:55.061 | INFO     | __main__:train:464 - Training model with checkpoint: /home/adarsh/.c
ache/huggingface/hub/models--AK123321--pl-leancopilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86
f/model.ckpt
Seed set to 3407
Lightning automatically upgraded your loaded checkpoint from v0.0.0 to v2.4.0. To apply the upgrade to yo
ur files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../.cache/huggingface
/hub/models--AK123321--pl-leancopilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86f/model.ckpt`
2024-08-10 16:05:19.736 | INFO     | __main__:train:486 - Loaded premise retriever at /home/adarsh/.cache
/huggingface/hub/models--AK123321--pl-leancopilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86f/mo
del.ckpt
2024-08-10 16:05:19.736 | INFO     | __main__:train:490 - Data path: /raid/adarsh/datasets_test/merged/ra
ndom
/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/transformers/tokenization
_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True`
by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by def
ault. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
2024-08-10 16:05:19.834 | INFO     | common:__init__:200 - Building the corpus from /raid/adarsh/datasets
_test/merged/corpus.jsonl
2024-08-10 16:05:40.525 | INFO     | retrieval.datamodule:load_or_cache_data:52 - Loaded data from cache
/raid/adarsh/datasets_test/merged/random/cache_train/cached_data.pkl
Training dataset size: 353052
2024-08-10 16:05:40.556 | INFO     | retrieval.datamodule:load_or_cache_data:52 - Loaded data from cache
/raid/adarsh/datasets_test/merged/random/cache_val/cached_data.pkl
Validation dataset size: 5403
2024-08-10 16:05:40.588 | INFO     | retrieval.datamodule:load_or_cache_data:52 - Loaded data from cache
/raid/adarsh/datasets_test/merged/random/cache_pred/cached_data.pkl
Testing dataset size: 5321
2024-08-10 16:05:40.590 | INFO     | __main__:train:523 - Training dataset size after load: 353052
2024-08-10 16:05:40.590 | INFO     | __main__:train:524 - Validation dataset size after load: 5403
2024-08-10 16:05:40.590 | INFO     | __main__:train:525 - Testing dataset size after load: 5321
Using bfloat16 Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
2024-08-10 16:05:40.604 | INFO     | __main__:train:543 - Starting progressive training...
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 1 processes
----------------------------------------------------------------------------------------------------

2024-08-10 16:05:52.135 | INFO     | retrieval.datamodule:load_or_cache_data:52 - Loaded data from cache
/raid/adarsh/datasets_test/merged/random/cache_train/cached_data.pkl
Training dataset size: 353052
2024-08-10 16:05:52.328 | INFO     | retrieval.datamodule:load_or_cache_data:52 - Loaded data from cache
/raid/adarsh/datasets_test/merged/random/cache_val/cached_data.pkl
Validation dataset size: 5403
2024-08-10 16:05:52.365 | INFO     | retrieval.datamodule:load_or_cache_data:52 - Loaded data from cache
/raid/adarsh/datasets_test/merged/random/cache_pred/cached_data.pkl
Testing dataset size: 5321
/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning/callbac
ks/model_checkpoint.py:654: Checkpoint directory /raid/adarsh/checkpoints_test exists and is not empty.
Restoring states from the checkpoint path at /home/adarsh/.cache/huggingface/hub/models--AK123321--pl-lea
ncopilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86f/model.ckpt
Lightning automatically upgraded your loaded checkpoint from v0.0.0 to v2.4.0. To apply the upgrade to yo
ur files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../.cache/huggingface
/hub/models--AK123321--pl-leancopilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86f/model.ckpt`
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
2024-08-10 16:05:55.871 | INFO     | common:get_optimizers:433 - Optimizing with GaLoreAdamW
/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/galore_torch/adamw.py:48:
 FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use t
he PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this
 warning
  warnings.warn(

  | Name    | Type           | Params | Mode
---------------------------------------------------
0 | encoder | T5EncoderModel | 217 M  | train
---------------------------------------------------
217 M     Trainable params
0         Non-trainable params
217 M     Total params
870.630   Total estimated model params size (MB)
235       Modules in train mode
0         Modules in eval mode
Restored all states from the checkpoint at /home/adarsh/.cache/huggingface/hub/models--AK123321--pl-leanc
opilot-2/snapshots/d4955ed1972ad84c03dce9189162d03912f2d86f/model.ckpt
Epoch 0:   0%|                                                                | 0/176526 [00:00<?, ?it/s]
2024-08-10 16:06:50.939 | INFO     | __main__:main:651 - An error occurred: 'correct_bias'
Traceback (most recent call last):
  File "/home/adarsh/ReProver/compute_server.py", line 645, in main
    train(model_checkpoint_path, merged_data_path, next_suffix)
  File "/home/adarsh/ReProver/compute_server.py", line 545, in train
    trainer.fit(model, datamodule=data_module, ckpt_path=model_checkpoint_path)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/trainer/call.py", line 46, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/trainer/trainer.py", line 1025, in _run_stage
    self.fit_loop.run()
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/loops/training_epoch_loop.py", line 140, in run
    self.advance(data_fetcher)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/loops/training_epoch_loop.py", line 250, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/loops/optimization/automatic.py", line 190, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/loops/optimization/automatic.py", line 268, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/trainer/call.py", line 167, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/core/module.py", line 1306, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/core/optimizer.py", line 153, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/strategies/ddp.py", line 270, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/strategies/strategy.py", line 238, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/plugins/precision/amp.py", line 75, in optimizer_step
    return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/pytorch_lightning
/plugins/precision/precision.py", line 122, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/torch/optim/lr_sc
heduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/torch/optim/optim
izer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/torch/utils/_cont
extlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/adarsh/miniconda3/envs/ReProverComputeServer/lib/python3.10/site-packages/galore_torch/adam
w.py", line 117, in step
    if group["correct_bias"]:  # No bias correction for Bert
KeyError: 'correct_bias'
Epoch 0:   0%|                                                                | 0/176526 [02:25<?, ?it/s]
(ReProverComputeServer) adarsh@tensorlab-DGX-Station-A100-920-23487-2531-000:~/ReProver$
